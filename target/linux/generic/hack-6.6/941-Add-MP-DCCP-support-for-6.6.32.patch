From aadbbada31206585ff96fcd879b414195bca5785 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Ar=C4=B1n=C3=A7=20=C3=9CNAL?= <arinc.unal@arinc9.com>
Date: Tue, 28 May 2024 15:26:07 +0300
Subject: [PATCH 2/4] Add MP-DCCP support for 6.6.32
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Signed-off-by: Arınç ÜNAL <arinc.unal@arinc9.com>
---
 include/linux/dccp.h                 |   75 +
 include/linux/skbuff.h               |    2 +-
 include/net/mpdccp.h                 |  306 ++++
 include/net/mpdccp_link.h            |  130 ++
 include/net/mpdccp_link_info.h       |   90 +
 include/net/mpdccp_meta.h            |   35 +
 include/net/net_namespace.h          |    4 +
 include/net/netns/mpdccp.h           |   35 +
 include/uapi/linux/dccp.h            |   78 +-
 include/uapi/linux/if.h              |    2 +
 net/core/dev.c                       |    2 +-
 net/dccp/Kconfig                     |   43 +
 net/dccp/Makefile                    |   21 +
 net/dccp/ccid.c                      |   14 +-
 net/dccp/ccid.h                      |   91 +-
 net/dccp/ccids/Kconfig               |   49 +
 net/dccp/ccids/ccid2.c               |   52 +-
 net/dccp/ccids/ccid2.h               |   43 +-
 net/dccp/ccids/ccid5.c               | 1580 ++++++++++++++++
 net/dccp/ccids/ccid5.h               |  250 +++
 net/dccp/ccids/ccid6.c               | 2519 ++++++++++++++++++++++++++
 net/dccp/ccids/ccid6.h               |  345 ++++
 net/dccp/ccids/ccid7.c               | 1082 +++++++++++
 net/dccp/ccids/ccid7.h               |  188 ++
 net/dccp/dccp.h                      |   66 +-
 net/dccp/feat.c                      |   91 +-
 net/dccp/input.c                     |   74 +-
 net/dccp/ipv4.c                      |   61 +-
 net/dccp/ipv6.c                      |   43 +-
 net/dccp/minisocks.c                 |   44 +-
 net/dccp/mpdccp.h                    |  453 +++++
 net/dccp/mpdccp_ctrl.c               | 1376 ++++++++++++++
 net/dccp/mpdccp_link.c               | 1168 ++++++++++++
 net/dccp/mpdccp_link_sysfs.c         |  739 ++++++++
 net/dccp/mpdccp_link_sysfs.h         |   48 +
 net/dccp/mpdccp_mod.c                |  152 ++
 net/dccp/mpdccp_pm.c                 |  197 ++
 net/dccp/mpdccp_pm.h                 |  115 ++
 net/dccp/mpdccp_proto.c              | 1077 +++++++++++
 net/dccp/mpdccp_proto.h              |   57 +
 net/dccp/mpdccp_reordering.c         |  632 +++++++
 net/dccp/mpdccp_reordering.h         |  196 ++
 net/dccp/mpdccp_scheduler.c          |  286 +++
 net/dccp/mpdccp_scheduler.h          |   85 +
 net/dccp/mpdccp_subflow.c            |  149 ++
 net/dccp/mpdccp_sysctl.c             |  310 ++++
 net/dccp/mpdccp_version.h            |    3 +
 net/dccp/options.c                   |  714 +++++++-
 net/dccp/output.c                    |  154 +-
 net/dccp/output.h                    |   21 +
 net/dccp/pm/Kconfig                  |    5 +
 net/dccp/pm/Makefile                 |    2 +
 net/dccp/pm/pm_default.c             | 1682 +++++++++++++++++
 net/dccp/probe.c                     |  203 +++
 net/dccp/proto.c                     |  199 +-
 net/dccp/qpolicy.c                   |   58 +-
 net/dccp/reordering/Kconfig          |   26 +
 net/dccp/reordering/Makefile         |    3 +
 net/dccp/reordering/ro_default.c     |   92 +
 net/dccp/scheduler/Kconfig           |   82 +
 net/dccp/scheduler/Makefile          |   14 +
 net/dccp/scheduler/sched_default.c   |   66 +
 net/dccp/scheduler/sched_otias.c     |  170 ++
 net/dccp/scheduler/sched_redundant.c |  141 ++
 net/dccp/scheduler/sched_rr.c        |  184 ++
 net/dccp/scheduler/sched_srtt.c      |  215 +++
 net/dccp/sysctl.c                    |   23 +
 net/dccp/timer.c                     |   79 +-
 net/ipv6/addrconf.c                  |    1 +
 69 files changed, 18527 insertions(+), 65 deletions(-)
 create mode 100644 include/net/mpdccp.h
 create mode 100644 include/net/mpdccp_link.h
 create mode 100644 include/net/mpdccp_link_info.h
 create mode 100644 include/net/mpdccp_meta.h
 create mode 100644 include/net/netns/mpdccp.h
 create mode 100644 net/dccp/ccids/ccid5.c
 create mode 100644 net/dccp/ccids/ccid5.h
 create mode 100644 net/dccp/ccids/ccid6.c
 create mode 100644 net/dccp/ccids/ccid6.h
 create mode 100755 net/dccp/ccids/ccid7.c
 create mode 100755 net/dccp/ccids/ccid7.h
 create mode 100644 net/dccp/mpdccp.h
 create mode 100644 net/dccp/mpdccp_ctrl.c
 create mode 100644 net/dccp/mpdccp_link.c
 create mode 100644 net/dccp/mpdccp_link_sysfs.c
 create mode 100644 net/dccp/mpdccp_link_sysfs.h
 create mode 100644 net/dccp/mpdccp_mod.c
 create mode 100644 net/dccp/mpdccp_pm.c
 create mode 100644 net/dccp/mpdccp_pm.h
 create mode 100644 net/dccp/mpdccp_proto.c
 create mode 100644 net/dccp/mpdccp_proto.h
 create mode 100644 net/dccp/mpdccp_reordering.c
 create mode 100644 net/dccp/mpdccp_reordering.h
 create mode 100644 net/dccp/mpdccp_scheduler.c
 create mode 100644 net/dccp/mpdccp_scheduler.h
 create mode 100644 net/dccp/mpdccp_subflow.c
 create mode 100644 net/dccp/mpdccp_sysctl.c
 create mode 100644 net/dccp/mpdccp_version.h
 create mode 100644 net/dccp/output.h
 create mode 100644 net/dccp/pm/Kconfig
 create mode 100644 net/dccp/pm/Makefile
 create mode 100644 net/dccp/pm/pm_default.c
 create mode 100644 net/dccp/probe.c
 create mode 100644 net/dccp/reordering/Kconfig
 create mode 100644 net/dccp/reordering/Makefile
 create mode 100644 net/dccp/reordering/ro_default.c
 create mode 100644 net/dccp/scheduler/Kconfig
 create mode 100644 net/dccp/scheduler/Makefile
 create mode 100644 net/dccp/scheduler/sched_default.c
 create mode 100644 net/dccp/scheduler/sched_otias.c
 create mode 100644 net/dccp/scheduler/sched_redundant.c
 create mode 100644 net/dccp/scheduler/sched_rr.c
 create mode 100644 net/dccp/scheduler/sched_srtt.c

diff --git a/include/linux/dccp.h b/include/linux/dccp.h
index 325af611909f9..7cccbbec30029 100644
--- a/include/linux/dccp.h
+++ b/include/linux/dccp.h
@@ -15,6 +15,9 @@
 #include <net/inet_timewait_sock.h>
 #include <net/tcp_states.h>
 #include <uapi/linux/dccp.h>
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+#include <net/mpdccp_meta.h>
+#endif
 
 enum dccp_state {
 	DCCP_OPEN	     = TCP_ESTABLISHED,
@@ -157,6 +160,9 @@ static inline unsigned int dccp_hdr_len(const struct sk_buff *skb)
  * @dreq_timestamp_echo: last received timestamp to echo (13.1)
  * @dreq_timestamp_echo: the time of receiving the last @dreq_timestamp_echo
  */
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+struct mpdccp_link_info;
+#endif
 struct dccp_request_sock {
 	struct inet_request_sock dreq_inet_rsk;
 	__u64			 dreq_iss;
@@ -168,6 +174,20 @@ struct dccp_request_sock {
 	struct list_head	 dreq_featneg;
 	__u32			 dreq_timestamp_echo;
 	__u32			 dreq_timestamp_time;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	struct mpdccp_link_info  *link_info;
+	struct mpdccp_key	mpdccp_loc_key;
+	struct mpdccp_key	mpdccp_rem_key;
+	u32 		mpdccp_loc_cix;
+	u32 		mpdccp_rem_cix;
+	u32	 		mpdccp_loc_nonce;
+	u32 			mpdccp_rem_nonce;
+	u8 			mpdccp_loc_hmac[MPDCCP_HMAC_SIZE];
+	u8 			mpdccp_rem_hmac[MPDCCP_HMAC_SIZE];
+	struct sock*		meta_sk;
+#endif
+	struct list_head	dreq_featneg_mp;
+	enum mpdccp_version	multipath_ver;
 };
 
 static inline struct dccp_request_sock *dccp_rsk(const struct request_sock *req)
@@ -185,6 +205,30 @@ struct dccp_options_received {
 	u32	dccpor_timestamp;
 	u32	dccpor_timestamp_echo;
 	u32	dccpor_elapsed_time;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	u64 dccpor_oall_seq:48;		/* MPDCCP overall sequence number */
+	u8 dccpor_addaddr[MPDCCP_ADDADDR_SIZE];
+	u8 dccpor_addaddr_len;
+	u8 dccpor_removeaddr[4];
+	
+	u32 dccpor_join_ip_local;     /* MPDCCP mp_join received local ip */
+	u32 dccpor_join_ip_remote;     /* MPDCCP mp_join received remote ip */
+	u8 dccpor_join_id;          /* MPDCCP mp_join received address id */
+	u16 dccpor_join_port;
+
+
+	u8 dccpor_rtt_type;         /* MP_RTT type */
+	u32 dccpor_rtt_value;
+	u32 dccpor_rtt_age;
+	u8 dccpor_mp_suppkeys;			/* MPDCCP supported key types */
+	u32 dccpor_mp_token;			/* MPDCCP path token */
+	u32 dccpor_mp_cix;			/* MPDCCP Connection ID */
+	u32 dccpor_mp_nonce;			/* MPDCCP path nonce */
+	u8 dccpor_mp_hmac[MPDCCP_HMAC_SIZE];	/* MPDCCP HMAC */
+	struct mpdccp_key dccpor_mp_keys[MPDCCP_MAX_KEYS];	/* MPDCCP keys */
+	int saw_mpkey;
+	int saw_mpjoin;
+#endif
 };
 
 struct ccid;
@@ -260,6 +304,8 @@ struct dccp_ackvec;
  * @dccps_xmitlet - tasklet scheduled by the TX CCID to dequeue data packets
  * @dccps_xmit_timer - used by the TX CCID to delay sending (rate-based pacing)
  * @dccps_syn_rtt - RTT sample from Request/Response exchange (in usecs)
+ * @dccps_next - next MPDCCP subflow socket
+ * @dccps_priority - socket priority for cheapest pipe first scheduler
  */
 struct dccp_sock {
 	/* inet_connection_sock has to be the first member of dccp_sock */
@@ -303,6 +349,35 @@ struct dccp_sock {
 	__u8				dccps_sync_scheduled:1;
 	struct tasklet_struct		dccps_xmitlet;
 	struct timer_list		dccps_xmit_timer;
+	struct timer_list		dccps_rcv_timer;
+	
+	/* alerab: for ccid6 */
+	u32	data_segs_out;		/* total number of data segments sent. */
+	u64	dccps_wstamp_ns;	/* departure time for next sent data packet */
+	u64	dccps_clock_cache; 	/* cache last tcp_clock_ns() (see dccp_mstamp_refresh()) */
+	u64	dccps_mstamp;		/* most recent packet received/sent */
+	
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	unsigned int		dccps_keepalive_enable;	  
+	unsigned int		dccps_keepalive_snd_intvl;
+	unsigned int		dccps_keepalive_rcv_intvl;
+	__u32				dccps_lrcvtime;
+	__u32				dccps_lsndtime;
+#endif
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	struct mpdccp_meta_cb		mpdccp;
+#endif
+	int                 is_fast_close;
+	int 	multipath_active;
+	int 	is_kex_sk;
+	int 	auth_done;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	u32	mpdccp_loc_nonce;
+	u32 	mpdccp_rem_nonce;
+	u8 	mpdccp_loc_hmac[MPDCCP_HMAC_SIZE];
+	u8 	mpdccp_rem_hmac[MPDCCP_HMAC_SIZE];
+	enum mpdccp_version multipath_ver;
+#endif
 };
 
 #define dccp_sk(ptr)	container_of_const(ptr, struct dccp_sock, \
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 5f11f98733419..497cd5b1f68e4 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -870,7 +870,7 @@ struct sk_buff {
 	 * want to keep them across layers you have to do a skb_clone()
 	 * first. This is owned by whoever has the skb queued ATM.
 	 */
-	char			cb[48] __aligned(8);
+	char			cb[128] __aligned(8);
 
 	union {
 		struct {
diff --git a/include/net/mpdccp.h b/include/net/mpdccp.h
new file mode 100644
index 0000000000000..8f4ba7459ba56
--- /dev/null
+++ b/include/net/mpdccp.h
@@ -0,0 +1,306 @@
+/*
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ * 
+ * Copyright (C) 2020 by Frank Reker <frank@reker.net>
+ * Copyright (C) 2021 by Romeo Cane <rmcane@tiscali.it>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#ifndef MPDCCP_UI_H
+#define MPDCCP_UI_H
+
+
+#define MPDCCP_EV_SUBFLOW_CREATE	1
+#define MPDCCP_EV_SUBFLOW_DESTROY	2
+#define MPDCCP_EV_ALL_SUBFLOW_DOWN	3
+
+/* the following two are for backward compatibilitiy */
+#define MPDCCP_SUBFLOW_CREATE	MPDCCP_EV_SUBFLOW_CREATE
+#define MPDCCP_SUBFLOW_DESTROY	MPDCCP_EV_SUBFLOW_DESTROY
+
+#include <linux/dccp.h>
+#include <net/mpdccp_meta.h>
+
+/* List of supported version(s) (in ascending order of preference) */
+static u8 mpdccp_supported_versions[] __attribute__((unused)) = {
+	MPDCCP_VERS_0 << 4
+};
+
+struct sockaddr;
+struct sk_buff;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+struct mpdccp_link_info;
+#else
+struct mpdccp_link_info { int dummy; };
+#endif
+
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+struct mpdccp_funcs {
+	u32	magic;
+	/* call back functions */
+	int (*destroy_sock) (struct sock*);
+	int (*mk_meta_sk) (struct sock*);
+	int (*connect) (struct sock*, const struct sockaddr*, int addrlen);
+	int (*write_xmit) (struct sock*);
+	int (*xmit_skb) (struct sock*, struct sk_buff*);
+	int (*activate) (struct sock*, int);
+	int (*isactive) (const struct sock*);
+	int (*try_mpdccp) (struct sock*);
+	int (*conn_request) (struct sock *sk, struct dccp_request_sock *dreq);
+	int (*rcv_request_sent_state_process) (struct sock *sk, const struct sk_buff *skb);
+	int (*rcv_respond_partopen_state_process) (struct sock *sk, int type);
+	int (*rcv_established) (struct sock *sk);
+	int (*check_req) (struct sock *sk, struct sock *new, struct request_sock *req, struct sk_buff *skb, struct sock **master_sk);
+	int (*create_master) (struct sock *sk, struct sock *newsk, struct request_sock *req, struct sk_buff *skb);
+	int (*close_meta) (struct sock *sk);
+};
+extern struct mpdccp_funcs	mpdccp_funcs;
+#endif
+
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+# define MPDCCP_HAS_FUNC(func)  ((mpdccp_funcs.magic == MPDCCP_MAGIC) && \
+				 mpdccp_funcs.func)
+#else
+# define MPDCCP_HAS_FUNC(func)  (0)
+#endif
+
+
+static inline int is_mpdccp (const struct sock *sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	return sk ? dccp_sk(sk)->mpdccp.magic == MPDCCP_MAGIC : 0;
+#else
+	return 0;
+#endif
+}
+
+
+static inline int mpdccp_is_meta (const struct sock *sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	return is_mpdccp (sk) ? dccp_sk(sk)->mpdccp.is_meta : 0;
+#else
+	return 0;
+#endif
+}
+
+static inline bool mpdccp_is_validkey(struct mpdccp_key *key)
+{
+	return (key && (((key->type == DCCPK_PLAIN) && (key->size == MPDCCP_PLAIN_KEY_SIZE))
+			|| ((key->type == DCCPK_C25519_SHA256) && (key->size == MPDCCP_C25519_KEY_SIZE))
+			|| ((key->type == DCCPK_C25519_SHA512) && (key->size == MPDCCP_C25519_KEY_SIZE))));
+}
+
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+#define MPDCCP_CB(sk) (is_mpdccp(sk) ? dccp_sk(sk)->mpdccp.mpcb : NULL)
+#else
+#define MPDCCP_CB(sk) (NULL)
+#endif
+
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+# define MPDCCP_CHECK_SK(sk) do { \
+	   if (!is_mpdccp(sk)) { \
+		   printk ("%s: socket is not an mpdccp socket\n", __func__); \
+		   return -ENOTSUPP; \
+	   } \
+	} while (0)
+# define MPDCCP_CHECK_FUNC(func) do { \
+	   if (!MPDCCP_HAS_FUNC(func)) { \
+		   printk ("%s: mpdccp_funcs not initialized: .magic=%x, .%s=%p\n", \
+						__func__, mpdccp_funcs.magic, #func, mpdccp_funcs.func); \
+		   return -ENOTSUPP; \
+	   } \
+	} while (0)
+# define MPDCCP_CHECK_SKFUNC(sk,func) do { \
+		MPDCCP_CHECK_SK(sk); \
+		MPDCCP_CHECK_FUNC(func); \
+	} while (0)
+#else
+# define MPDCCP_CHECK_SK(sk) do { } while (0)
+# define MPDCCP_CHECK_FUNC(func) do { } while (0)
+# define MPDCCP_CHECK_SKFUNC(sk,func) do { } while (0)
+#endif
+
+
+static inline int mpdccp_destroy_sock (struct sock *sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC(sk,destroy_sock);
+	return mpdccp_funcs.destroy_sock (sk);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_mk_meta_sk (struct sock *sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_FUNC(mk_meta_sk);
+	return mpdccp_funcs.mk_meta_sk (sk);
+#else
+	return 0;
+#endif
+}
+
+
+static inline int mpdccp_connect (struct sock *sk, const struct sockaddr *addr, int addrlen)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC (sk,connect);
+	return mpdccp_funcs.connect (sk, addr, addrlen);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_write_xmit (struct sock *sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC(sk,write_xmit);
+	return mpdccp_funcs.write_xmit (sk);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_xmit_skb (struct sock *sk, struct sk_buff *skb)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC(sk,xmit_skb);
+	return mpdccp_funcs.xmit_skb (sk, skb);
+#else
+	return 0;
+#endif
+}
+
+
+#define MPDCCP_SUBFLOW_NOTIFIER
+struct mpdccp_subflow_notifier {
+	struct mpdccp_link_info	*link;
+	struct sock		*sk;
+	struct sock		*subsk;
+	int			role;
+};
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+int register_mpdccp_subflow_notifier (struct notifier_block *nb);
+int unregister_mpdccp_subflow_notifier (struct notifier_block *nb);
+#else
+static inline int register_mpdccp_subflow_notifier (struct notifier_block *nb)
+{
+	return 0;
+}
+static inline int unregister_mpdccp_subflow_notifier (struct notifier_block *nb)
+{
+	return 0;
+}
+#endif
+
+static inline int try_mpdccp (struct sock *sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_FUNC(try_mpdccp);
+	return mpdccp_funcs.try_mpdccp (sk);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_activate (struct sock *sk, int on)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_FUNC(activate);
+	return mpdccp_funcs.activate (sk, on);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_isactive (const struct sock *sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_FUNC(isactive);
+	return mpdccp_funcs.isactive (sk);
+#else
+	return 0;
+#endif
+}
+
+
+static inline int mpdccp_conn_request (struct sock *sk, struct dccp_request_sock *dreq)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC(sk, conn_request);
+	return mpdccp_funcs.conn_request (sk, dreq);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_rcv_request_sent_state_process (struct sock *sk, const struct sk_buff *skb)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC(sk, rcv_request_sent_state_process);
+	return mpdccp_funcs.rcv_request_sent_state_process (sk, skb);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_rcv_respond_partopen_state_process (struct sock *sk, int type)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC(sk, rcv_respond_partopen_state_process);
+	return mpdccp_funcs.rcv_respond_partopen_state_process (sk, type);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_rcv_established (struct sock *sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC(sk, rcv_established);
+	return mpdccp_funcs.rcv_established (sk);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_check_req (struct sock *sk, struct sock *newsk, struct request_sock *req, struct sk_buff *skb, struct sock **master_sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC(sk, check_req);
+	return mpdccp_funcs.check_req (sk, newsk, req, skb, master_sk);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_create_master (struct sock *sk, struct sock *newsk, struct request_sock *req, struct sk_buff *skb)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC (sk,create_master);
+	return mpdccp_funcs.create_master (sk, newsk, req, skb);
+#else
+	return 0;
+#endif
+}
+
+static inline int mpdccp_close_meta (struct sock *sk)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	MPDCCP_CHECK_SKFUNC(sk, close_meta);
+	return mpdccp_funcs.close_meta (sk);
+#else
+	return 0;
+#endif
+}
+
+#endif	/* MPDCCP_UI_H */
diff --git a/include/net/mpdccp_link.h b/include/net/mpdccp_link.h
new file mode 100644
index 0000000000000..b7ca5d8b74e51
--- /dev/null
+++ b/include/net/mpdccp_link.h
@@ -0,0 +1,130 @@
+#ifndef _LINUX_MPDCCP_LINK_H
+#define _LINUX_MPDCCP_LINK_H
+
+#include <generated/autoconf.h>
+#include <uapi/linux/in.h>
+#include <uapi/linux/in6.h>
+#include <net/net_namespace.h>
+#include <linux/netdevice.h>
+#include <linux/types.h>
+#include <net/mpdccp_link_info.h>
+
+#define MPDCCP_LINK_VER	3
+#define MPDCCP_HAS_DELAY	2
+#define MPDCCP_HAS_EXT_LPU
+
+#if 0
+#  define MPDCCP_LINK_TO_NET(link) ((link)->ndev ? read_pnet(&((link)->ndev->nd_net)) : \
+		(((link)->net ? (link)->net : &init_net)))
+#else
+#  define MPDCCP_LINK_TO_NET(link) ((link)->net)
+#endif
+
+struct mpdccp_link_info *mpdccp_link_find_by_dev(struct net_device*);
+#define MPDCCP_LINK_ISDEV(link) ((link)->is_devlink)
+#define MPDCCP_LINK_ISDEV_VALID(link) (MPDCCP_LINK_ISDEV(link) && ((link)->ndev))
+#define MPDCCP_LINK_FROM_DEV(ndev) mpdccp_link_find_by_dev(ndev)
+#define MPDCCP_LINK_TO_DEV(link) ((link)->ndev)
+#define MPDCCP_LINK_NAME(link) (((link)->ndev)?((link)->ndev_name):((link)->name))
+#ifdef CONFIG_SYSFS
+//#define MPDCCP_LINK_REFCOUNT(link) (atomic_read (&(link)->kobj.kref.refcount.refs))
+#define MPDCCP_LINK_REFCOUNT(link) (atomic_read (&(link)->kref.refcount.refs))
+#else
+#define MPDCCP_LINK_REFCOUNT(link) (atomic_read (&(link)->kref.refcount.refs))
+#endif
+
+
+int mpdccp_link_copy (struct mpdccp_link_info **new_link, struct mpdccp_link_info *old_link);
+int mpdccp_link_add (struct mpdccp_link_info**, struct net*, struct net_device*, const char *name);
+void mpdccp_link_get (struct mpdccp_link_info*);
+void mpdccp_link_put (struct mpdccp_link_info*);
+
+
+struct mpdccp_link_notifier_info {
+	struct mpdccp_link_info	*link_info;
+	struct net_device	*ndev;
+};
+
+
+int register_mpdccp_link_notifier (struct notifier_block*);
+int unregister_mpdccp_link_notifier (struct notifier_block*);
+int call_mpdccp_link_notifiers (unsigned long, struct mpdccp_link_info*);
+
+#define MPDCCP_LINK_CHANGE_PRIO				1
+#define MPDCCP_LINK_CHANGE_MAXBUF			2
+#define MPDCCP_LINK_CHANGE_DELAY				3
+#define MPDCCP_LINK_CHANGE_LPU				4
+#define MPDCCP_LINK_CHANGE_THROTTLE			5
+#define MPDCCP_LINK_CHANGE_MARK				6
+#define MPDCCP_LINK_CHANGE_CGSTCTRL			7
+#define MPDCCP_LINK_CHANGE_PATHTYPE			8
+#define MPDCCP_LINK_CHANGE_MATCH_PATHTYPE	9
+
+/*
+ * change functions, prototypes and dev_change_ inline wrappers
+ */
+
+
+#define MPDCCP_DEV_CHANGE(func,typ) \
+	int mpdccp_link_change_##func(struct mpdccp_link_info*, typ); \
+	static inline int dev_change_##func(struct net_device *dev, typ val) \
+	{ \
+		return mpdccp_link_change_##func (MPDCCP_LINK_FROM_DEV(dev), (val)); \
+	}
+MPDCCP_DEV_CHANGE(mpdccp_prio,u32)
+MPDCCP_DEV_CHANGE(mpdccp_maxbuf,u64)
+MPDCCP_DEV_CHANGE(mpdccp_T_delay,u32)
+MPDCCP_DEV_CHANGE(mpdccp_T_start_delay,u32)
+MPDCCP_DEV_CHANGE(mpdccp_T_lpu,u32)
+MPDCCP_DEV_CHANGE(mpdccp_T_lpu_min,u32)
+MPDCCP_DEV_CHANGE(mpdccp_lpu_cnt,u32)
+MPDCCP_DEV_CHANGE(mpdccp_ignthrottle,unsigned int)
+MPDCCP_DEV_CHANGE(mpdccp_match_mark,u32)
+MPDCCP_DEV_CHANGE(mpdccp_match_mask,u32)
+MPDCCP_DEV_CHANGE(mpdccp_send_mark,u32)
+MPDCCP_DEV_CHANGE(mpdccp_path_type,u32)
+MPDCCP_DEV_CHANGE(mpdccp_match_pathtype,u32)
+int mpdccp_link_change_name (struct mpdccp_link_info *link, struct sock *sk);
+int mpdccp_link_change_mpdccp_cgstalg(struct mpdccp_link_info *, const char *, size_t );
+static inline int dev_change_mpdccp_cgstalg(struct net_device *dev, const char *buf, size_t len)
+{
+	return mpdccp_link_change_mpdccp_cgstalg (MPDCCP_LINK_FROM_DEV(dev), buf, len);
+}
+int mpdccp_link_change_mpdccp_resetstat(struct mpdccp_link_info *);
+static inline int dev_change_mpdccp_resetstat(struct net_device *dev)
+{
+	return mpdccp_link_change_mpdccp_resetstat(MPDCCP_LINK_FROM_DEV(dev));
+}
+
+/* Connection ID functions  */
+u32 mpdccp_link_generate_cid(void);
+void mpdccp_link_free_cid(u32 cid);
+/* 
+ * find functions
+ */
+
+struct mpdccp_link_info* mpdccp_link_find_by_name (struct net *net, const char *name);
+struct mpdccp_link_info* mpdccp_link_find_mark (struct net *net, u32 mark);
+struct mpdccp_link_info* mpdccp_link_find_ip4 (struct net *net, struct in_addr *saddr, struct in_addr *daddr);
+#if IS_ENABLED(CONFIG_IPV6)
+struct mpdccp_link_info* mpdccp_link_find_ip6 (struct net *net, struct in6_addr *saddr, struct in6_addr *daddr);
+#endif /* IS_ENABLED(CONFIG_IPV6) */
+struct mpdccp_link_info *mpdccp_link_find_by_skb (struct net*, const struct sk_buff*);
+
+struct mpdccp_link_info *mpdccp_getfallbacklink(struct net *net);
+
+
+extern int mpdccp_link_net_id;
+struct mpdccp_link_net_data {
+	atomic_t		counter;
+	struct mpdccp_link_info	*fallback;
+	struct net		*net;
+#ifdef CONFIG_SYSFS
+	struct kobject		dev;
+	struct kobject		name;
+#endif
+};
+
+
+
+#endif	/* _LINUX_MPDCCP_LINK_H */
diff --git a/include/net/mpdccp_link_info.h b/include/net/mpdccp_link_info.h
new file mode 100644
index 0000000000000..743922fe02343
--- /dev/null
+++ b/include/net/mpdccp_link_info.h
@@ -0,0 +1,90 @@
+#ifndef _LINUX_MPDCCP_LINK_INFO_H
+#define _LINUX_MPDCCP_LINK_INFO_H
+
+#include <generated/autoconf.h>
+#include <net/net_namespace.h>
+#include <linux/types.h>
+#include <linux/kobject.h>
+#include <linux/netdevice.h>
+
+
+struct mpdccp_link_info {
+
+	struct list_head	link_list;
+//	char			name[IFNAMSIZ+1];
+	char			name[64];
+	int			id;
+	u32			is_linked:1,
+				is_devlink:1,
+				is_released:1,
+				sysfs_to_del:1;
+	struct net_device	*ndev;
+	struct net		*net;
+	char			ndev_name[IFNAMSIZ+1];
+#ifdef CONFIG_SYSFS
+	struct kobject		kobj;
+#endif
+	struct kref		kref;
+/* public: read-only - write thru mpdccp_link_change_... functions */
+
+	int	start_config;
+	u64	config_cnt;		/* config counter */
+	u32	mpdccp_prio;		/* prio for mpdccp connections */
+	u64	mpdccp_maxbuf;		/* max. buf for mpdccp subflows */
+	u32	mpdccp_ignthrottle;	/* ignore throttling */
+	char	mpdccp_cgstalg[64];	/* congestion algorithmn for subpath */
+	u32	mpdccp_match_mark;	/* fwmark to match incomming requests */
+	u32	mpdccp_match_mask;	/* fwmask to match incomming requests */
+	u32	mpdccp_send_mark;	/* mark outgoing packets */
+	u32	mpdccp_T_delay;		/* delay before peak start in ms */
+	u32	mpdccp_T_delay_j;	/* delay before peak start in jiffies */
+	u32	mpdccp_T_start_delay;	/* delay at stream start in ms */
+	u32	mpdccp_T_start_delay_j;	/* delay at stream start in jiffies */
+	u32	mpdccp_T_lpu;		/* threshhold for last path usage in ms */
+	u32	mpdccp_T_lpu_j;		/* threshhold for last path usage in jiffies */
+	u32	mpdccp_T_lpu_min;	/* threshhold for last path usage in ms */
+	u32	mpdccp_T_lpu_min_j;	/* threshhold for last path usage in jiffies */
+	u32	mpdccp_lpu_cnt;		/* max packets that can be sent between lpu_min and lpu */
+	u32	mpdccp_path_type;	/* path type (lte, wifi, ...) */
+	u32	mpdccp_match_pathtype;	/* match path type (e.g. match if path is lte */
+	int	end_config;
+
+	u64	mpdccp_rx_packets;	/* number packets received */
+	u64	mpdccp_rx_bytes;	/* number bytes received */
+	u64	mpdccp_tx_packets;	/* number packets send */
+	u64	mpdccp_tx_bytes;	/* number bytes send */
+
+#ifdef CONFIG_MPDCCP_STATS
+	/* mpdccp statistics */
+	int	start_stats;
+	u64	mpdccp_noavail_hard;
+	u64	mpdccp_noavail_hard_state;
+	u64	mpdccp_noavail_hard_pre;
+	u64	mpdccp_noavail_hard_pf;
+	u64	mpdccp_noavail_hard_loss;
+	u64	mpdccp_noavail_nocwnd;
+	u64	mpdccp_noavail_nospace_maxbuf;
+	u64	mpdccp_noavail_nospace;
+	u64	mpdccp_noavail_zerownd;
+	u64	mpdccp_noavail_nobuf;
+	u64	mpdccp_noavail_delay;
+	u64	mpdccp_noavail_start_delay;
+	u64	mpdccp_noavail_dontreinject;
+	u64	mpdccp_selected_delayed;
+	u64	mpdccp_selected_onlypath;
+	u64	mpdccp_selected_shutdown;
+	u64	mpdccp_selected_backup;
+	u64	mpdccp_selected_good;
+	u64	mpdccp_selected_best;
+	u64	mpdccp_selected_fallback;
+	u64	mpdccp_selected;
+	u64	allref;
+	int	end_stats;
+#endif
+};
+
+#define mpdccp_link_cnt(link) ((link)?((link)->config_cnt):0)
+
+
+
+#endif	/* _LINUX_MPDCCP_LINK_INFO_H */
diff --git a/include/net/mpdccp_meta.h b/include/net/mpdccp_meta.h
new file mode 100644
index 0000000000000..1bfbf73e83925
--- /dev/null
+++ b/include/net/mpdccp_meta.h
@@ -0,0 +1,35 @@
+/*
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ * 
+ * Copyright (C) 2020 by Frank Reker <frank@reker.net>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#ifndef MPDCCP_META_H
+#define MPDCCP_META_H
+
+
+
+#define MPDCCP_MAGIC	0xab386ff5
+struct mpdccp_cb;
+
+struct mpdccp_meta_cb {
+	u32			magic;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	int			is_meta;
+	struct mpdccp_cb	*mpcb;
+#endif
+};
+
+
+
+
+
+#endif	/* MPDCCP_META_H */
diff --git a/include/net/net_namespace.h b/include/net/net_namespace.h
index 704c6c6635561..9b145df1fcba7 100644
--- a/include/net/net_namespace.h
+++ b/include/net/net_namespace.h
@@ -23,6 +23,7 @@
 #include <net/netns/ieee802154_6lowpan.h>
 #include <net/netns/sctp.h>
 #include <net/netns/dccp.h>
+#include <net/netns/mpdccp.h>
 #include <net/netns/netfilter.h>
 #if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)
 #include <net/netns/conntrack.h>
@@ -145,6 +146,9 @@ struct net {
 #if defined(CONFIG_IP_DCCP) || defined(CONFIG_IP_DCCP_MODULE)
 	struct netns_dccp	dccp;
 #endif
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	struct netns_mpdccp	mpdccp;
+#endif
 #ifdef CONFIG_NETFILTER
 	struct netns_nf		nf;
 #if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)
diff --git a/include/net/netns/mpdccp.h b/include/net/netns/mpdccp.h
new file mode 100644
index 0000000000000..367965b6b4e58
--- /dev/null
+++ b/include/net/netns/mpdccp.h
@@ -0,0 +1,35 @@
+/*
+ * MPDCCP - MPDCCP namespace
+ *
+ * The namespace holds global information about interfaces and path
+ * managers.
+ *
+ * The code in this file is directly derived from the mptcp project's 
+ * include/net/netns/mptcp.h. All Copyright (C) the original authors
+ * Christoph Paasch et al.
+ *
+ * MPDCCP adjustments are Copyright (C) 2018 
+ * by Andreas Philipp Matz <info@andreasmatz.de>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#ifndef __NETNS_MPDCCP_H__
+#define __NETNS_MPDCCP_H__
+
+#include <linux/compiler.h>
+
+enum {
+	MPDCCP_PM_FULLMESH = 0,
+	MPDCCP_PM_MAX
+};
+
+struct netns_mpdccp {
+	/* TAG-0.8: Migrate to list implementation */
+	void *path_managers[MPDCCP_PM_MAX];
+};
+
+#endif /* __NETNS_MPDCCP_H__ */
diff --git a/include/uapi/linux/dccp.h b/include/uapi/linux/dccp.h
index 6e1978dbcf7c7..2302ea5854485 100644
--- a/include/uapi/linux/dccp.h
+++ b/include/uapi/linux/dccp.h
@@ -145,6 +145,7 @@ enum dccp_reset_codes {
 	DCCP_RESET_CODE_TOO_BUSY,
 	DCCP_RESET_CODE_BAD_INIT_COOKIE,
 	DCCP_RESET_CODE_AGGRESSION_PENALTY,
+	DCCP_RESET_CODE_MPDCCP_ABORTED = 13,
 
 	DCCP_MAX_RESET_CODES		/* Leave at the end!  */
 };
@@ -165,6 +166,7 @@ enum {
 	DCCPO_TIMESTAMP = 41,
 	DCCPO_TIMESTAMP_ECHO = 42,
 	DCCPO_ELAPSED_TIME = 43,
+	DCCPO_MULTIPATH = 46,
 	DCCPO_MAX = 45,
 	DCCPO_MIN_RX_CCID_SPECIFIC = 128,	/* from sender to receiver */
 	DCCPO_MAX_RX_CCID_SPECIFIC = 191,
@@ -174,10 +176,32 @@ enum {
 /* maximum size of a single TLV-encoded DCCP option (sans type/len bytes) */
 #define DCCP_SINGLE_OPT_MAXLEN	253
 
+/* MP-DCCP options */
+enum {
+	DCCPO_MP_CONFIRM = 0,				/* Confirm reception and processing of an MP_OPT option */
+	DCCPO_MP_JOIN = 1,					/* Join path to an existing MP-DCCP flow */
+	DCCPO_MP_FAST_CLOSE = 2,		/* Close MP-DCCP flow */
+	DCCPO_MP_KEY = 3,						/* Exchange key material for MP_HMAC */
+	DCCPO_MP_SEQ = 4,						/* MPDCCP overall sequence number */
+	DCCPO_MP_HMAC = 5,					/* HMA Code for authentication */
+	DCCPO_MP_RTT = 6,						/* Transmit RTT values */
+	DCCPO_MP_ADDADDR = 7,				/* Advertise additional Address */
+	DCCPO_MP_REMOVEADDR  = 8,			/* Remove Address */
+	DCCPO_MP_PRIO = 9,					/* path priorization */
+	DCCPO_MP_CLOSE = 10,				/* Close MPDCCP flow */
+};
+
 /* DCCP CCIDS */
 enum {
 	DCCPC_CCID2 = 2,
 	DCCPC_CCID3 = 3,
+	DCCPC_CCID5 = 5,
+	DCCPC_CCID6 = 6,
+	DCCPC_CCID7 = 7,
+
+#define DCCPC_TESTING_MIN	248
+#define DCCPC_TESTING_MAX	255
+	DCCPC_CCID_ZERO = DCCPC_TESTING_MIN,
 };
 
 /* DCCP features (RFC 4340 section 6.4) */
@@ -192,7 +216,8 @@ enum dccp_feature_numbers {
 	DCCPF_SEND_NDP_COUNT = 7,
 	DCCPF_MIN_CSUM_COVER = 8,
 	DCCPF_DATA_CHECKSUM = 9,
-	/* 10-127 reserved */
+	DCCPF_MULTIPATH = 10,	/* Used to negotiate MP support on connection establishment */
+	/* 11-127 reserved */
 	DCCPF_MIN_CCID_SPECIFIC = 128,
 	DCCPF_SEND_LEV_RATE = 192,	/* RFC 4342, sec. 8.4 */
 	DCCPF_MAX_CCID_SPECIFIC = 255,
@@ -210,8 +235,52 @@ enum dccp_cmsg_type {
 enum dccp_packet_dequeueing_policy {
 	DCCPQ_POLICY_SIMPLE,
 	DCCPQ_POLICY_PRIO,
+	DCCPQ_POLICY_DROP_OLDEST,
+	DCCPQ_POLICY_DROP_NEWEST,
 	DCCPQ_POLICY_MAX
 };
+/* for #ifdef's */
+#define DCCPQ_POLICY_DROP_OLDEST DCCPQ_POLICY_DROP_OLDEST
+#define DCCPQ_POLICY_DROP_NEWEST DCCPQ_POLICY_DROP_NEWEST
+
+
+
+/* Authentication data */
+#define MPDCCP_PLAIN_KEY_SIZE 8
+#define MPDCCP_C25519_KEY_SIZE 32
+#define MPDCCP_MAX_KEY_SIZE MPDCCP_C25519_KEY_SIZE
+#define MPDCCP_MAX_KEYS 3
+#define MPDCCP_HMAC_SIZE 20
+
+#define MPDCCP_ADDADDR_SIZE 22
+#define MPDCCP_CONFIRM_SIZE 31
+
+/* MPDCCP version type */
+enum mpdccp_version {
+	MPDCCP_VERS_0 = 0,
+	MPDCCP_VERS_MAX = 1,
+	MPDCCP_VERS_UNDEFINED = 0xF,
+};
+
+/* MPDCCP key types */
+enum mpdccp_key_type {
+	DCCPK_PLAIN = 0,
+	DCCPK_C25519_SHA256 = 1,
+	DCCPK_C25519_SHA512 = 2,
+	DCCPK_INVALID  = 255,
+};
+
+enum {
+	DCCPKF_PLAIN = (1 << DCCPK_PLAIN),
+	DCCPKF_C25519_SHA256 = (1 << DCCPK_C25519_SHA256),
+	DCCPKF_C25519_SHA512 = (1 << DCCPK_C25519_SHA512),
+};
+
+struct mpdccp_key {
+	enum mpdccp_key_type type;
+	__u32 size;
+	__u8 value[MPDCCP_MAX_KEY_SIZE];
+};
 
 /* DCCP socket options */
 #define DCCP_SOCKOPT_PACKET_SIZE	1 /* XXX deprecated, without effect */
@@ -228,8 +297,15 @@ enum dccp_packet_dequeueing_policy {
 #define DCCP_SOCKOPT_RX_CCID		15
 #define DCCP_SOCKOPT_QPOLICY_ID		16
 #define DCCP_SOCKOPT_QPOLICY_TXQLEN	17
+#define DCCP_SOCKOPT_MULTIPATH		18
+#define DCCP_SOCKOPT_KEEPALIVE		19
+#define DCCP_SOCKOPT_KA_TIMEOUT		20
+#define DCCP_SOCKOPT_MP_SCHEDULER	21
+#define DCCP_SOCKOPT_MP_REORDER		22
+#define DCCP_SOCKOPT_MP_FAST_CLOSE	23
 #define DCCP_SOCKOPT_CCID_RX_INFO	128
 #define DCCP_SOCKOPT_CCID_TX_INFO	192
+#define DCCP_SOCKOPT_CCID_LIM_RTO   193
 
 /* maximum number of services provided on the same listening port */
 #define DCCP_SERVICE_LIST_MAX_LEN      32
diff --git a/include/uapi/linux/if.h b/include/uapi/linux/if.h
index 797ba2c1562af..bf52d33502309 100644
--- a/include/uapi/linux/if.h
+++ b/include/uapi/linux/if.h
@@ -133,6 +133,8 @@ enum net_device_flags {
 #define IFF_ECHO			IFF_ECHO
 #endif /* __UAPI_DEF_IF_NET_DEVICE_FLAGS_LOWER_UP_DORMANT_ECHO */
 
+#define IFF_MPDCCPON	0x200000		/* Enable for MPTCP 		*/
+
 #define IFF_VOLATILE	(IFF_LOOPBACK|IFF_POINTOPOINT|IFF_BROADCAST|IFF_ECHO|\
 		IFF_MASTER|IFF_SLAVE|IFF_RUNNING|IFF_LOWER_UP|IFF_DORMANT)
 
diff --git a/net/core/dev.c b/net/core/dev.c
index 1f6c8945f2eca..426219a94a99b 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -8605,7 +8605,7 @@ int __dev_change_flags(struct net_device *dev, unsigned int flags,
 
 	dev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |
 			       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |
-			       IFF_AUTOMEDIA)) |
+			       IFF_AUTOMEDIA | IFF_MPDCCPON)) |
 		     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |
 				    IFF_ALLMULTI));
 
diff --git a/net/dccp/Kconfig b/net/dccp/Kconfig
index 0c7d2f66ba278..b13f8cf357ca4 100644
--- a/net/dccp/Kconfig
+++ b/net/dccp/Kconfig
@@ -25,6 +25,14 @@ config INET_DCCP_DIAG
 	def_tristate y if (IP_DCCP = y && INET_DIAG = y)
 	def_tristate m
 
+config DCCP_KEEPALIVE
+	bool "DCCP keepalive"
+	default y
+	help
+	  Enables Keepalive for DCCP
+
+	  If in doubt, say Y.
+
 source "net/dccp/ccids/Kconfig"
 
 menu "DCCP Kernel Hacking"
@@ -43,4 +51,39 @@ config IP_DCCP_DEBUG
 
 endmenu
 
+menuconfig IP_MPDCCP
+	tristate "Multipath DCCP"
+	help
+	   This module enables bundling of multiple physical connections over
+	   the DCCP protocol. If you don't understand what was just said, 
+	   you don't need it: say N.
+
+	   To compile this code as a module, choose M here: the
+	   module will be called dccp_mpdccp.
+
+if IP_MPDCCP
+config IP_MPDCCP_DEBUG
+	bool "MPDCCP debug messages"
+	help
+	  Only use this if you're hacking MPDCCP.
+
+	  When compiling MPDCCP as a module, this debugging output can be toggled
+	  by setting the parameter mpdccp_debug of the `dccp_mpdccp' module to 0 or 1.
+
+	  Just say N.
+
+
+source "net/dccp/pm/Kconfig"
+source "net/dccp/scheduler/Kconfig"
+source "net/dccp/reordering/Kconfig"
+
+
+config MPDCCP_STATS
+	bool "MPDCCP Statistics"
+	depends on IP_MPDCCP
+	help
+		Creates some statistics in /sys/class/net/<dev>
+
+
+endif # IP_MPDDCP
 endif # IP_DDCP
diff --git a/net/dccp/Makefile b/net/dccp/Makefile
index 5b4ff37bc8061..91909e3cd7b30 100644
--- a/net/dccp/Makefile
+++ b/net/dccp/Makefile
@@ -13,6 +13,9 @@ dccp-$(CONFIG_IP_DCCP_TFRC_LIB) += ccids/lib/tfrc.o		\
 				   ccids/lib/tfrc_equation.o	\
 				   ccids/lib/packet_history.o	\
 				   ccids/lib/loss_interval.o
+dccp-$(CONFIG_IP_DCCP_CCID5) += ccids/ccid5.o
+dccp-$(CONFIG_IP_DCCP_CCID6) += ccids/ccid6.o
+dccp-$(CONFIG_IP_DCCP_CCID7) += ccids/ccid7.o
 
 dccp_ipv4-y := ipv4.o
 
@@ -28,3 +31,21 @@ dccp_diag-y := diag.o
 
 # build with local directory for trace.h
 CFLAGS_proto.o := -I$(src)
+
+
+# mpdccp needs at least the default scheduler
+obj-$(CONFIG_IP_MPDCCP) += mpdccp.o
+mpdccp-$(CONFIG_IP_MPDCCP) := 	mpdccp_ctrl.o mpdccp_sysctl.o mpdccp_mod.o \
+				mpdccp_proto.o mpdccp_subflow.o \
+				mpdccp_pm.o pm/pm_default.o \
+				mpdccp_scheduler.o scheduler/sched_default.o \
+				mpdccp_reordering.o reordering/ro_default.o
+
+obj-$(CONFIG_IP_MPDCCP) += mpdccplink.o
+mpdccplink-y := mpdccp_link.o
+mpdccplink-$(CONFIG_SYSFS) += mpdccp_link_sysfs.o
+
+
+obj-$(CONFIG_IP_MPDCCP) += scheduler/
+obj-$(CONFIG_IP_MPDCCP) += reordering/
+obj-$(CONFIG_IP_MPDCCP) += pm/
diff --git a/net/dccp/ccid.c b/net/dccp/ccid.c
index 6beac5d348e2b..5ca332c2cde5f 100644
--- a/net/dccp/ccid.c
+++ b/net/dccp/ccid.c
@@ -18,8 +18,20 @@ static struct ccid_operations *ccids[] = {
 #ifdef CONFIG_IP_DCCP_CCID3
 	&ccid3_ops,
 #endif
+#ifdef CONFIG_IP_DCCP_CCID5
+	&ccid5_ops,
+#endif
+#ifdef CONFIG_IP_DCCP_CCID6
+	&ccid6_ops,
+#endif
+#ifdef CONFIG_IP_DCCP_CCID7
+	&ccid7_ops,
+#endif
 };
 
+u32 (*get_delay_valn)(struct sock *sk, struct tcp_info *info, u8 *type) = mrtt_as_delayn;
+EXPORT_SYMBOL_GPL(get_delay_valn);
+
 static struct ccid_operations *ccid_by_number(const u8 id)
 {
 	int i;
@@ -180,7 +192,7 @@ void ccid_hc_rx_delete(struct ccid *ccid, struct sock *sk)
 }
 
 void ccid_hc_tx_delete(struct ccid *ccid, struct sock *sk)
-{
+{	
 	if (ccid != NULL) {
 		if (ccid->ccid_ops->ccid_hc_tx_exit != NULL)
 			ccid->ccid_ops->ccid_hc_tx_exit(sk);
diff --git a/net/dccp/ccid.h b/net/dccp/ccid.h
index 105f3734dadbf..d44166b2b7d43 100644
--- a/net/dccp/ccid.h
+++ b/net/dccp/ccid.h
@@ -15,12 +15,12 @@
 #include <linux/dccp.h>
 #include <linux/list.h>
 #include <linux/module.h>
-
+#include <net/tcp.h> //added by nath
 /* maximum value for a CCID (RFC 4340, 19.5) */
 #define CCID_MAX		255
 #define CCID_SLAB_NAME_LENGTH	32
 
-struct tcp_info;
+//struct tcp_info;
 
 /**
  *  struct ccid_operations  -  Interface to Congestion-Control Infrastructure
@@ -89,6 +89,15 @@ extern struct ccid_operations ccid2_ops;
 #ifdef CONFIG_IP_DCCP_CCID3
 extern struct ccid_operations ccid3_ops;
 #endif
+#ifdef CONFIG_IP_DCCP_CCID5
+extern struct ccid_operations ccid5_ops;
+#endif
+#ifdef CONFIG_IP_DCCP_CCID6
+extern struct ccid_operations ccid6_ops;
+#endif
+#ifdef CONFIG_IP_DCCP_CCID7
+extern struct ccid_operations ccid7_ops;
+#endif
 
 int ccid_initialize_builtins(void);
 void ccid_cleanup_builtins(void);
@@ -259,4 +268,82 @@ static inline int ccid_hc_tx_getsockopt(struct ccid *ccid, struct sock *sk,
 						 optval, optlen);
 	return rc;
 }
+
+/* functions to get delay by natha :)*/
+
+/**
+ * Obtain SRTT value form CCID2 TX sock.
+ * NOTE: value is divided by 8 to match MRTT
+ */
+static inline u32 srtt_as_delayn(struct sock *sk, struct tcp_info *info, u8 *type){
+    if(dccp_sk(sk)->dccps_hc_tx_ccid != NULL) { 
+		*type = 3;
+    	ccid_hc_tx_get_info(dccp_sk(sk)->dccps_hc_tx_ccid, sk, info);
+    	return jiffies_to_msecs(info->tcpi_rtt >> 3); }		// divide by 2^3 (8)
+    else { return 0; }
+}
+
+/**
+ * Obtain MRTT value form CCID2 TX sock.
+ */
+static inline u32 mrtt_as_delayn(struct sock *sk, struct tcp_info *info, u8 *type){
+    if(dccp_sk(sk)->dccps_hc_tx_ccid != NULL) { 
+		*type = 0;
+    	ccid_hc_tx_get_info(dccp_sk(sk)->dccps_hc_tx_ccid, sk, info);
+    	return jiffies_to_msecs(info->tcpi_rttvar); }
+    else{ return 0; }
+}
+
+/**
+ * Obtain Min RTT value form CCID2 TX sock.
+ */
+static inline u32 min_rtt_as_delayn(struct sock *sk, struct tcp_info *info, u8 *type){
+    if(dccp_sk(sk)->dccps_hc_tx_ccid != NULL) { 
+		*type = 1;
+    	ccid_hc_tx_get_info(dccp_sk(sk)->dccps_hc_tx_ccid, sk, info);
+		// overwrite age field to display correct timestamp
+		info->tcpi_last_ack_recv = info->tcpi_last_ack_sent;
+    	return jiffies_to_msecs(info->tcpi_min_rtt); }
+    else{ return 0; }
+}
+
+/**
+ * Obtain Max RTT value form CCID2 TX sock.
+ */
+static inline u32 max_rtt_as_delayn(struct sock *sk, struct tcp_info *info, u8 *type){
+    if(dccp_sk(sk)->dccps_hc_tx_ccid != NULL) { 
+		*type = 2;
+    	ccid_hc_tx_get_info(dccp_sk(sk)->dccps_hc_tx_ccid, sk, info);
+		// overwrite age field to display correct timestamp
+		info->tcpi_last_ack_recv = info->tcpi_rcv_mss;
+    	return jiffies_to_msecs(info->tcpi_snd_mss); }
+    else{ return 0; }
+}
+
+extern u32 (*get_delay_valn)(struct sock *sk, struct tcp_info *info, u8 *type);
+
+static inline void set_srtt_as_delayn(void){
+    get_delay_valn = srtt_as_delayn;
+}
+
+static inline void set_mrtt_as_delayn(void){
+    get_delay_valn = mrtt_as_delayn;
+}
+
+static inline void set_min_rtt_as_delayn(void){
+    get_delay_valn = min_rtt_as_delayn;
+}
+
+static inline void set_max_rtt_as_delayn(void){
+    get_delay_valn = max_rtt_as_delayn;
+}
+
+/*
+ * Convert RFC 3390 larger initial window into an equivalent number of packets.
+ * This is based on the numbers specified in RFC 5681, 3.1.
+ */
+static inline u32 rfc3390_bytes_to_packets(const u32 smss)
+{
+        return smss <= 1095 ? 4 : (smss > 2190 ? 2 : 3);
+}
 #endif /* _CCID_H */
diff --git a/net/dccp/ccids/Kconfig b/net/dccp/ccids/Kconfig
index a3eeb84d16f9c..1004928b284e2 100644
--- a/net/dccp/ccids/Kconfig
+++ b/net/dccp/ccids/Kconfig
@@ -52,4 +52,53 @@ config IP_DCCP_TFRC_LIB
 
 config IP_DCCP_TFRC_DEBUG
 	def_bool y if IP_DCCP_CCID3_DEBUG
+
+config IP_DCCP_CCID5
+	bool "CCID 5 (BBR-Like)"
+	def_bool y if (IP_DCCP = y || IP_DCCP = m)
+	help
+	  CCID 5 - BBR implementation for DCCP.
+config IP_DCCP_CCID5_DEBUG
+	bool "CCID-5 debugging messages"
+	depends on IP_DCCP_CCID5
+	help
+	  Enable CCID-5 specific debugging messages.
+
+	  The debugging output can additionally be toggled by setting the
+	  ccid5_debug parameter to 0 or 1.
+
+	  If in doubt, say N.
+
+config IP_DCCP_CCID6
+	bool "CCID 6 (Cubic)"
+	def_bool y if (IP_DCCP = y || IP_DCCP = m)
+	help
+	  CCID 6 - BBRv2 implementation for DCCP.
+config IP_DCCP_CCID6_DEBUG
+	bool "CCID-6 debugging messages"
+	depends on IP_DCCP_CCID6
+	help
+	  Enable CCID-6 specific debugging messages.
+
+	  The debugging output can additionally be toggled by setting the
+	  ccid6_debug parameter to 0 or 1.
+
+	  If in doubt, say N.
+
+config IP_DCCP_CCID7
+	bool "CCID 7 (Cubic)"
+	def_bool y if (IP_DCCP = y || IP_DCCP = m)
+	help
+	  CCID 7 - Cubic implementation for DCCP.
+config IP_DCCP_CCID7_DEBUG
+	bool "CCID-7 debugging messages"
+	depends on IP_DCCP_CCID7
+	help
+	  Enable CCID-7 specific debugging messages.
+
+	  The debugging output can additionally be toggled by setting the
+	  ccid7_debug parameter to 0 or 1.
+
+	  If in doubt, say N.
+
 endmenu
diff --git a/net/dccp/ccids/ccid2.c b/net/dccp/ccids/ccid2.c
index 4d9823d6dcedf..7030ec3bedced 100644
--- a/net/dccp/ccids/ccid2.c
+++ b/net/dccp/ccids/ccid2.c
@@ -22,6 +22,10 @@ static bool ccid2_debug;
 #define ccid2_pr_debug(format, a...)
 #endif
 
+/* Function pointer to either get SRTT or MRTT ...*/
+u32 (*get_delay_val)(struct ccid2_hc_tx_sock *hc) = ccid2_mrtt_as_delay;
+EXPORT_SYMBOL_GPL(get_delay_val);
+
 static int ccid2_hc_tx_alloc_seq(struct ccid2_hc_tx_sock *hc)
 {
 	struct ccid2_seq *seqp;
@@ -206,6 +210,7 @@ static void ccid2_cwnd_application_limited(struct sock *sk, const u32 now)
 		hc->tx_ssthresh = max(hc->tx_ssthresh,
 				     (hc->tx_cwnd >> 1) + (hc->tx_cwnd >> 2));
 		hc->tx_cwnd = (hc->tx_cwnd + win_used) >> 1;
+		dccp_pr_debug("%s: tx_cwnd set to %d for sk %p", __func__, hc->tx_cwnd, sk);
 	}
 	hc->tx_cwnd_used  = 0;
 	hc->tx_cwnd_stamp = now;
@@ -366,6 +371,18 @@ static void ccid2_rtt_estimator(struct sock *sk, const long mrtt)
 	struct ccid2_hc_tx_sock *hc = ccid2_hc_tx_sk(sk);
 	long m = mrtt ? : 1;
 
+	hc->tx_mrtt = mrtt;
+	hc->tx_last_ack_recv = ccid2_jiffies32;
+
+	if(m > 0 && hc->tx_min_rtt >= m){
+		hc->tx_min_rtt = m;
+		hc->tx_min_rtt_stamp = ccid2_jiffies32;
+	}
+	if(m > 0 && hc->tx_max_rtt <= m){
+		hc->tx_max_rtt = m;
+		hc->tx_max_rtt_stamp = ccid2_jiffies32;
+	}
+
 	if (hc->tx_srtt == 0) {
 		/* First measurement m */
 		hc->tx_srtt = m << 3;
@@ -517,6 +534,7 @@ static void ccid2_hc_tx_packet_recv(struct sock *sk, struct sk_buff *skb)
 	u64 ackno, seqno;
 	struct ccid2_seq *seqp;
 	int done = 0;
+	bool not_rst = 0;
 	unsigned int maxincr = 0;
 
 	/* check reverse path congestion */
@@ -573,6 +591,7 @@ static void ccid2_hc_tx_packet_recv(struct sock *sk, struct sk_buff *skb)
 		seqp = seqp->ccid2s_next;
 		if (seqp == hc->tx_seqh) {
 			seqp = hc->tx_seqh->ccid2s_prev;
+			not_rst = 1;
 			break;
 		}
 	}
@@ -610,9 +629,11 @@ static void ccid2_hc_tx_packet_recv(struct sock *sk, struct sk_buff *skb)
 			if (done)
 				break;
 
+
 			/* check all seqnos in the range of the vector
 			 * run length
 			 */
+
 			while (between48(seqp->ccid2s_seq,ackno_end_rl,ackno)) {
 				const u8 state = dccp_ackvec_state(avp->vec);
 
@@ -654,6 +675,7 @@ static void ccid2_hc_tx_packet_recv(struct sock *sk, struct sk_buff *skb)
 		seqp = seqp->ccid2s_next;
 		if (seqp == hc->tx_seqh) {
 			seqp = hc->tx_seqh->ccid2s_prev;
+			not_rst = 1;
 			break;
 		}
 	}
@@ -706,7 +728,7 @@ static void ccid2_hc_tx_packet_recv(struct sock *sk, struct sk_buff *skb)
 	/* restart RTO timer if not all outstanding data has been acked */
 	if (hc->tx_pipe == 0)
 		sk_stop_timer(sk, &hc->tx_rtotimer);
-	else
+	else if(!not_rst)
 		sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
 done:
 	/* check if incoming Acks allow pending packets to be sent */
@@ -723,6 +745,11 @@ static int ccid2_hc_tx_init(struct ccid *ccid, struct sock *sk)
 
 	/* RFC 4341, 5: initialise ssthresh to arbitrarily high (max) value */
 	hc->tx_ssthresh = ~0U;
+	hc->tx_min_rtt = ~0U;
+	hc->tx_max_rtt = 0;
+
+	hc->tx_min_rtt_stamp = ccid2_jiffies32;
+	hc->tx_max_rtt_stamp = ccid2_jiffies32;
 
 	/* Use larger initial windows (RFC 4341, section 5). */
 	hc->tx_cwnd = rfc3390_bytes_to_packets(dp->dccps_mss_cache);
@@ -763,16 +790,36 @@ static void ccid2_hc_tx_exit(struct sock *sk)
 static void ccid2_hc_rx_packet_recv(struct sock *sk, struct sk_buff *skb)
 {
 	struct ccid2_hc_rx_sock *hc = ccid2_hc_rx_sk(sk);
+	//printk(KERN_INFO "natrm: enter ccid2_hc_rx_packet_recv %p", sk);
 
 	if (!dccp_data_packet(skb))
 		return;
-
 	if (++hc->rx_num_data_pkts >= dccp_sk(sk)->dccps_r_ack_ratio) {
 		dccp_send_ack(sk);
 		hc->rx_num_data_pkts = 0;
 	}
 }
 
+static void ccid2_hc_tx_get_info(struct sock *sk, struct tcp_info *info)
+{
+	info->tcpi_rto = ccid2_hc_tx_sk(sk)->tx_rto;
+	info->tcpi_rtt = ccid2_hc_tx_sk(sk)->tx_srtt;
+	info->tcpi_rttvar = ccid2_hc_tx_sk(sk)->tx_mrtt;
+	info->tcpi_segs_out = ccid2_hc_tx_sk(sk)->tx_pipe;
+	info->tcpi_snd_cwnd = ccid2_hc_tx_sk(sk)->tx_cwnd;
+	info->tcpi_last_data_sent = ccid2_hc_tx_sk(sk)->tx_lsndtime;
+	//calculate time since last rtt calculation.
+	info->tcpi_last_ack_recv = (ccid2_hc_tx_sk(sk)->tx_last_ack_recv > 0) ? ccid2_jiffies32 - ccid2_hc_tx_sk(sk)->tx_last_ack_recv : 0;
+	
+	info->tcpi_min_rtt = ccid2_hc_tx_sk(sk)->tx_min_rtt;
+	//calculate time since tx_min_rtt_stamp was set and store it in some unused var.
+	info->tcpi_last_ack_sent = (ccid2_hc_tx_sk(sk)->tx_min_rtt_stamp > 0) ? ccid2_jiffies32 - ccid2_hc_tx_sk(sk)->tx_min_rtt_stamp : 0;
+	
+	info->tcpi_snd_mss = ccid2_hc_tx_sk(sk)->tx_max_rtt;
+	//calculate time since tx_min_rtt_stamp was set and store it in some unused var.
+	info->tcpi_rcv_mss = (ccid2_hc_tx_sk(sk)->tx_max_rtt_stamp > 0) ? ccid2_jiffies32 - ccid2_hc_tx_sk(sk)->tx_max_rtt_stamp : 0;
+}
+
 struct ccid_operations ccid2_ops = {
 	.ccid_id		  = DCCPC_CCID2,
 	.ccid_name		  = "TCP-like",
@@ -783,6 +830,7 @@ struct ccid_operations ccid2_ops = {
 	.ccid_hc_tx_packet_sent	  = ccid2_hc_tx_packet_sent,
 	.ccid_hc_tx_parse_options = ccid2_hc_tx_parse_options,
 	.ccid_hc_tx_packet_recv	  = ccid2_hc_tx_packet_recv,
+	.ccid_hc_tx_get_info	  = ccid2_hc_tx_get_info,
 	.ccid_hc_rx_obj_size	  = sizeof(struct ccid2_hc_rx_sock),
 	.ccid_hc_rx_packet_recv	  = ccid2_hc_rx_packet_recv,
 };
diff --git a/net/dccp/ccids/ccid2.h b/net/dccp/ccids/ccid2.h
index 330c7b4ec0013..82121c9d18b5e 100644
--- a/net/dccp/ccids/ccid2.h
+++ b/net/dccp/ccids/ccid2.h
@@ -66,10 +66,16 @@ struct ccid2_hc_tx_sock {
 
 	/* RTT measurement: variables/principles are the same as in TCP */
 	u32			tx_srtt,
+				tx_mrtt,	/* Raw RTT value as measured by CCID */
 				tx_mdev,
 				tx_mdev_max,
 				tx_rttvar,
-				tx_rto;
+				tx_rto,
+				tx_min_rtt,
+				tx_max_rtt,
+				tx_min_rtt_stamp,	        /* timestamp of min_rtt_us */
+				tx_max_rtt_stamp,	        /* timestamp of max_rtt_us */
+				tx_last_ack_recv;
 	u64			tx_rtt_seq:48;
 	struct timer_list	tx_rtotimer;
 	struct sock		*sk;
@@ -87,6 +93,37 @@ struct ccid2_hc_tx_sock {
 	struct list_head	tx_av_chunks;
 };
 
+/**
+ * Obtain SRTT value form CCID2 TX sock.
+ */
+static inline u32 ccid2_srtt_as_delay(struct ccid2_hc_tx_sock *hc){
+	dccp_pr_debug("srtt value : %u", hc->tx_srtt);
+	if(hc){ return hc->tx_srtt;	}
+	else{ return 0; }
+}
+
+/**
+ * Obtain MRTT value form CCID2 TX sock.
+ * NOTE: value is scaled by 8 to match SRTT
+ */
+static inline u32 ccid2_mrtt_as_delay(struct ccid2_hc_tx_sock *hc){
+	dccp_pr_debug("mrtt value : %u", hc->tx_mrtt);
+	if(hc){ return (hc->tx_mrtt * 8); }
+	else{ return 0;	}
+}
+
+/* Function pointer to either get SRTT or MRTT ...*/
+extern u32 (*get_delay_val)(struct ccid2_hc_tx_sock *hc);
+
+static inline void set_srtt_as_delay(void){
+	get_delay_val = ccid2_srtt_as_delay;
+}
+
+static inline void set_mrtt_as_delay(void){
+	get_delay_val = ccid2_mrtt_as_delay;
+}
+
+
 static inline bool ccid2_cwnd_network_limited(struct ccid2_hc_tx_sock *hc)
 {
 	return hc->tx_pipe >= hc->tx_cwnd;
@@ -96,10 +133,10 @@ static inline bool ccid2_cwnd_network_limited(struct ccid2_hc_tx_sock *hc)
  * Convert RFC 3390 larger initial window into an equivalent number of packets.
  * This is based on the numbers specified in RFC 5681, 3.1.
  */
-static inline u32 rfc3390_bytes_to_packets(const u32 smss)
+/*static inline u32 rfc3390_bytes_to_packets(const u32 smss)
 {
 	return smss <= 1095 ? 4 : (smss > 2190 ? 2 : 3);
-}
+}*/
 
 /**
  * struct ccid2_hc_rx_sock  -  Receiving end of CCID-2 half-connection
diff --git a/net/dccp/ccids/ccid5.c b/net/dccp/ccids/ccid5.c
new file mode 100644
index 0000000000000..48900acf39e99
--- /dev/null
+++ b/net/dccp/ccids/ccid5.c
@@ -0,0 +1,1580 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2019 by Nathalie Romo, Deutsche Telekom AG
+ *
+ * BBR algorithm for the DCCP protocol.
+ *
+ * The code in this file is derived from net/ipv4/tcp_bbr.c,
+ * net/ipv4/tcp_rate.c and net/dccp/ccids/ccid2.c. ccid2 Derived code is
+ * Copyright (C) the original authors Andrea Bittau and Arnaldo Carvalho
+ * 
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+
+#include <linux/slab.h>
+#include "../feat.h"
+#include "ccid5.h"
+
+#define BW_SCALE 24
+#define BW_UNIT (1 << BW_SCALE)
+
+#define BBR_SCALE 8	/* scaling factor for fractions in BBR (e.g. gains)  ????*/
+#define BBR_UNIT (1 << BBR_SCALE)
+
+static const int bbr_high_gain  = BBR_UNIT * 2885 / 1000 + 1; // lo del logaritmo base 2
+static const int bbr_drain_gain = BBR_UNIT * 1000 / 2885;
+static const int bbr_cwnd_gain  = BBR_UNIT * 2;
+
+static const int bbr_pacing_gain[] = {
+	BBR_UNIT * 5 / 4,	/* probe for more available bw */
+	BBR_UNIT * 3 / 4,	/* drain queue and/or yield bw to other flows */
+	BBR_UNIT, BBR_UNIT, BBR_UNIT,	/* cruise at 1.0*bw to utilize pipe, */
+	BBR_UNIT, BBR_UNIT, BBR_UNIT	/* without creating excess queue... */
+};
+
+static const u32 bbr_cycle_rand = 7;
+
+#define CYCLE_LEN	8	/* number of phases in a pacing gain cycle */
+/* Window length of min_rtt filter (in sec): */
+static const u32 bbr_min_rtt_win_sec = 10;
+static const u32 bbr_probe_rtt_mode_ms = 200;
+static const int bbr_min_tso_rate = 1200000;
+static const u32 bbr_cwnd_min_target = 4;
+
+/* Window length of bw filter (in rounds): */
+static const int bbr_bw_rtts = CYCLE_LEN + 2;
+
+static const u32 bbr_full_bw_thresh = BBR_UNIT * 5 / 4;
+static const u32 bbr_full_bw_cnt = 3;
+
+/* "long-term" ("LT") bandwidth estimator parameters... */
+/* The minimum number of rounds in an LT bw sampling interval: */
+static const u32 bbr_lt_intvl_min_rtts = 4;
+/* If lost/delivered ratio > 20%, interval is "lossy" and we may be policed: */
+static const u32 bbr_lt_loss_thresh = 50;
+/* If 2 intervals have a bw ratio <= 1/8, their bw is "consistent": */
+static const u32 bbr_lt_bw_ratio = BBR_UNIT / 8;
+/* If 2 intervals have a bw diff <= 4 Kbit/sec their bw is "consistent": */
+static const u32 bbr_lt_bw_diff = 4000 / 8;
+/* If we estimate we're policed, use lt_bw for this many round trips: */
+static const u32 bbr_lt_bw_max_rtts = 48;
+
+enum bbr_mode {
+	BBR_STARTUP,	/* ramp up sending rate rapidly to fill pipe */
+	BBR_DRAIN,	/* drain any queue created during startup */
+	BBR_PROBE_BW,	/* discover, share bw: pace around estimated bw */
+	BBR_PROBE_RTT,	/* cut cwnd to min to probe min_rtt */
+};
+
+
+enum dccp_ca_state {
+	DCCP_CA_Open = 0,
+	DCCP_CA_Disorder = 1,
+	DCCP_CA_CWR = 2,
+	DCCP_CA_Recovery = 3,
+	DCCP_CA_Loss = 4
+};
+#ifdef CONFIG_IP_DCCP_CCID2_DEBUG
+static bool ccid5_debug;
+#define ccid5_pr_debug(format, a...)	DCCP_PR_DEBUG(ccid5_debug, format, ##a)
+#else
+#define ccid5_pr_debug(format, a...)
+#endif
+
+/* Function pointer to either get SRTT or MRTT ...*/
+//u32 (*get_delay_val)(struct ccid5_hc_tx_sock *hc) = mrtt_as_delay;
+//EXPORT_SYMBOL_GPL(get_delay_val);
+
+static int ccid5_hc_tx_alloc_seq(struct ccid5_hc_tx_sock *hc)
+{
+	//printk(KERN_INFO "natrm: enter ccid5_hc_tx_alloc_seq");
+	struct ccid5_seq *seqp;
+	int i;
+
+	/* check if we have space to preserve the pointer to the buffer */
+	if (hc->tx_seqbufc >= (sizeof(hc->tx_seqbuf) /
+			       sizeof(struct ccid5_seq *)))
+		return -ENOMEM;
+
+	/* allocate buffer and initialize linked list */
+	seqp = kmalloc(CCID5_SEQBUF_LEN * sizeof(struct ccid5_seq), gfp_any());
+	if (seqp == NULL)
+		return -ENOMEM;
+
+	for (i = 0; i < (CCID5_SEQBUF_LEN - 1); i++) {
+		seqp[i].ccid5s_next = &seqp[i + 1];
+		seqp[i + 1].ccid5s_prev = &seqp[i];
+	}
+	seqp[CCID5_SEQBUF_LEN - 1].ccid5s_next = seqp;
+	seqp->ccid5s_prev = &seqp[CCID5_SEQBUF_LEN - 1];
+
+	/* This is the first allocation.  Initiate the head and tail.  */
+	if (hc->tx_seqbufc == 0)
+		hc->tx_seqh = hc->tx_seqt = seqp;
+	else {
+		/* link the existing list with the one we just created */
+		hc->tx_seqh->ccid5s_next = seqp;
+		seqp->ccid5s_prev = hc->tx_seqh;
+
+		hc->tx_seqt->ccid5s_prev = &seqp[CCID5_SEQBUF_LEN - 1];
+		seqp[CCID5_SEQBUF_LEN - 1].ccid5s_next = hc->tx_seqt;
+	}
+
+	/* store the original pointer to the buffer so we can free it */
+	hc->tx_seqbuf[hc->tx_seqbufc] = seqp;
+	hc->tx_seqbufc++;
+
+	return 0;
+}
+
+
+static void ccid5_change_l_ack_ratio(struct sock *sk, u32 val)
+{
+	u32 max_ratio = DIV_ROUND_UP(ccid5_hc_tx_sk(sk)->tx_cwnd, 2);
+
+	/*
+	 * Ensure that Ack Ratio does not exceed ceil(cwnd/2), which is (2) from
+	 * RFC 4341, 6.1.2. We ignore the statement that Ack Ratio 2 is always
+	 * acceptable since this causes starvation/deadlock whenever cwnd < 2.
+	 * The same problem arises when Ack Ratio is 0 (ie. Ack Ratio disabled).
+	 */
+	if (val == 0 || val > max_ratio) {
+		DCCP_WARN("Limiting Ack Ratio (%u) to %u\n", val, max_ratio);
+		val = max_ratio;
+	}
+	//printk(KERN_INFO "natrm: ccid5 change ack_ratio %lu max %lu", val, max_ratio);
+	dccp_feat_signal_nn_change(sk, DCCPF_ACK_RATIO,
+				   min_t(u32, val, DCCPF_ACK_RATIO_MAX));
+}
+
+static void ccid5_check_l_ack_ratio(struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+
+	/*
+	 * After a loss, idle period, application limited period, or RTO we
+	 * need to check that the ack ratio is still less than the congestion
+	 * window. Otherwise, we will send an entire congestion window of
+	 * packets and got no response because we haven't sent ack ratio
+	 * packets yet.
+	 * If the ack ratio does need to be reduced, we reduce it to half of
+	 * the congestion window (or 1 if that's zero) instead of to the
+	 * congestion window. This prevents problems if one ack is lost.
+	 */
+
+	if (dccp_feat_nn_get(sk, DCCPF_ACK_RATIO) > hc->tx_cwnd)
+		ccid5_change_l_ack_ratio(sk, hc->tx_cwnd/2 ? : 1U);
+}
+
+static void ccid5_change_l_seq_window(struct sock *sk, u64 val)
+{
+	dccp_feat_signal_nn_change(sk, DCCPF_SEQUENCE_WINDOW,
+				   clamp_val(val, DCCPF_SEQ_WMIN,
+						  DCCPF_SEQ_WMAX));
+}
+static void dccp_tasklet_schedule(struct sock *sk)
+{
+	struct tasklet_struct *t = &dccp_sk(sk)->dccps_xmitlet;
+
+	if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
+		sock_hold(sk);
+		__tasklet_schedule(t);
+	}
+}
+
+/*
+ *	Congestion window validation (RFC 2861).
+ */
+
+static bool ccid5_do_cwv = true;
+module_param(ccid5_do_cwv, bool, 0644);
+MODULE_PARM_DESC(ccid5_do_cwv, "Perform RFC2861 Congestion Window Validation");
+
+/**
+ * ccid2_update_used_window  -  Track how much of cwnd is actually used
+ * This is done in addition to CWV. The sender needs to have an idea of how many
+ * packets may be in flight, to set the local Sequence Window value accordingly
+ * (RFC 4340, 7.5.2). The CWV mechanism is exploited to keep track of the
+ * maximum-used window. We use an EWMA low-pass filter to filter out noise.
+ */
+static void ccid5_update_used_window(struct ccid5_hc_tx_sock *hc, u32 new_wnd)
+{
+	hc->tx_expected_wnd = (3 * hc->tx_expected_wnd + new_wnd) / 4;
+}
+
+/* This borrows the code of tcp_cwnd_application_limited() */
+static void ccid5_cwnd_application_limited(struct sock *sk, const u32 now)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	/* don't reduce cwnd below the initial window (IW) */
+	u32 init_win = rfc3390_bytes_to_packets(dccp_sk(sk)->dccps_mss_cache),
+	    win_used = max(hc->tx_cwnd_used, init_win);
+
+	if (win_used < hc->tx_cwnd) {
+		hc->tx_ssthresh = max(hc->tx_ssthresh,
+				     (hc->tx_cwnd >> 1) + (hc->tx_cwnd >> 2));
+		//hc->tx_cwnd = (hc->tx_cwnd + win_used) >> 1;
+		//dccp_pr_debug("%s: tx_cwnd set to %d for sk %p", __func__, hc->tx_cwnd, sk);
+	}
+	hc->tx_cwnd_used  = 0;
+	hc->tx_cwnd_stamp = now;
+
+	ccid5_check_l_ack_ratio(sk);
+}
+
+/* This borrows the code of tcp_cwnd_restart() */
+static void ccid5_cwnd_restart(struct sock *sk, const u32 now)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	u32 cwnd = hc->tx_cwnd, restart_cwnd,
+	    iwnd = rfc3390_bytes_to_packets(dccp_sk(sk)->dccps_mss_cache);
+
+	hc->tx_ssthresh = max(hc->tx_ssthresh, (cwnd >> 1) + (cwnd >> 2));
+
+	/* don't reduce cwnd below the initial window (IW) */
+	restart_cwnd = min(cwnd, iwnd);
+	cwnd >>= (now - hc->tx_lsndtime) / hc->tx_rto;
+	//hc->tx_cwnd = max(cwnd, restart_cwnd);
+
+	hc->tx_cwnd_stamp = now;
+	hc->tx_cwnd_used  = 0;
+
+	ccid5_check_l_ack_ratio(sk);
+}
+
+static void ccid5_hc_tx_packet_sent(struct sock *sk, unsigned int len)
+{
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	const u32 now = ccid5_jiffies32;
+	struct ccid5_seq *next;
+	hc->bytes_sent += len;
+	if (hc->curr_ca_state == DCCP_CA_Loss)
+		hc->curr_ca_state = DCCP_CA_Open; 
+
+	/* slow-start after idle periods (RFC 2581, RFC 2861) */
+	if (ccid5_do_cwv && !hc->tx_pipe &&
+	    (s32)(now - hc->tx_lsndtime) >= hc->tx_rto)
+		ccid5_cwnd_restart(sk, now);
+
+	hc->tx_lsndtime = now;
+
+	hc->tx_seqh->sent_mstamp = tcp_clock_us();
+	if (!hc->tx_pipe) {
+		hc->first_tx_mstamp  = hc->tx_seqh->sent_mstamp;
+		hc->delivered_mstamp = hc->first_tx_mstamp;
+	}
+
+
+	hc->tx_seqh->delivered    = hc->delivered;
+	hc->tx_seqh->ccid5s_seq   = dp->dccps_gss;
+	hc->tx_seqh->ccid5s_acked = 0;
+	hc->tx_seqh->ccid5s_sent  = now;
+	hc->tx_seqh->first_tx_mstamp   = hc->first_tx_mstamp;
+	hc->tx_seqh->delivered_mstamp  = hc->delivered_mstamp;
+	hc->tx_seqh->is_app_limited = hc->app_limited ? 1 : 0;
+
+	next = hc->tx_seqh->ccid5s_next;
+	/* check if we need to alloc more space */
+	if (next == hc->tx_seqt) {
+		if (ccid5_hc_tx_alloc_seq(hc)) {
+			DCCP_CRIT("packet history - out of memory!");
+			/* FIXME: find a more graceful way to bail out */
+			return;
+		}
+		next = hc->tx_seqh->ccid5s_next;
+		BUG_ON(next == hc->tx_seqt);
+	}
+	hc->tx_seqh = next;
+
+	hc->tx_pipe  += 1;
+
+	/* see whether cwnd was fully used (RFC 2861), update expected window */
+	if (ccid5_cwnd_network_limited(hc)) {
+		ccid5_update_used_window(hc, hc->tx_cwnd);
+		hc->tx_cwnd_used  = 0;
+		hc->tx_cwnd_stamp = now;
+	} else {
+		if (hc->tx_pipe > hc->tx_cwnd_used)
+			hc->tx_cwnd_used = hc->tx_pipe;
+
+		ccid5_update_used_window(hc, hc->tx_cwnd_used);
+
+		if (ccid5_do_cwv && (s32)(now - hc->tx_cwnd_stamp) >= hc->tx_rto)
+			ccid5_cwnd_application_limited(sk, now);
+	}
+
+	ccid5_pr_debug("sk=%p cwnd=%d pipe=%d\n", sk, hc->tx_cwnd, hc->tx_pipe);
+
+	/*
+	 * FIXME: The code below is broken and the variables have been removed
+	 * from the socket struct. The `ackloss' variable was always set to 0,
+	 * and with arsent there are several problems:
+	 *  (i) it doesn't just count the number of Acks, but all sent packets;
+	 *  (ii) it is expressed in # of packets, not # of windows, so the
+	 *  comparison below uses the wrong formula: Appendix A of RFC 4341
+	 *  comes up with the number K = cwnd / (R^2 - R) of consecutive windows
+	 *  of data with no lost or marked Ack packets. If arsent were the # of
+	 *  consecutive Acks received without loss, then Ack Ratio needs to be
+	 *  decreased by 1 when
+	 *	      arsent >=  K * cwnd / R  =  cwnd^2 / (R^3 - R^2)
+	 *  where cwnd / R is the number of Acks received per window of data
+	 *  (cf. RFC 4341, App. A). The problems are that
+	 *  - arsent counts other packets as well;
+	 *  - the comparison uses a formula different from RFC 4341;
+	 *  - computing a cubic/quadratic equation each time is too complicated.
+	 *  Hence a different algorithm is needed.
+	 */
+#if 0
+	/* Ack Ratio.  Need to maintain a concept of how many windows we sent */
+	hc->tx_arsent++;
+	/* We had an ack loss in this window... */
+	if (hc->tx_ackloss) {
+		if (hc->tx_arsent >= hc->tx_cwnd) {
+			hc->tx_arsent  = 0;
+			hc->tx_ackloss = 0;
+		}
+	} else {
+		/* No acks lost up to now... */
+		/* decrease ack ratio if enough packets were sent */
+		if (dp->dccps_l_ack_ratio > 1) {
+			/* XXX don't calculate denominator each time */
+			int denom = dp->dccps_l_ack_ratio * dp->dccps_l_ack_ratio -
+				    dp->dccps_l_ack_ratio;
+
+			denom = hc->tx_cwnd * hc->tx_cwnd / denom;
+
+			if (hc->tx_arsent >= denom) {
+				ccid2_change_l_ack_ratio(sk, dp->dccps_l_ack_ratio - 1);
+				hc->tx_arsent = 0;
+			}
+		} else {
+			/* we can't increase ack ratio further [1] */
+			hc->tx_arsent = 0; /* or maybe set it to cwnd*/
+		}
+	}
+#endif
+
+	sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
+
+}
+
+/**
+ * uses the same code from ccid2_rtt_estimator
+ */
+static void ccid5_rtt_estimator(struct sock *sk, const long mrtt)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	long m = mrtt ? : 1;
+
+	hc->tx_mrtt = mrtt;
+	hc->tx_last_ack_recv = ccid5_jiffies32;
+
+	if (hc->tx_srtt == 0) {
+		/* First measurement m */
+		hc->tx_srtt = m << 3;
+		hc->tx_mdev = m << 1;
+
+		hc->tx_mdev_max = max(hc->tx_mdev, tcp_rto_min(sk));
+		hc->tx_rttvar   = hc->tx_mdev_max;
+
+		hc->tx_rtt_seq  = dccp_sk(sk)->dccps_gss;
+	} else {
+		/* Update scaled SRTT as SRTT += 1/8 * (m - SRTT) */
+		m -= (hc->tx_srtt >> 3);
+		hc->tx_srtt += m;
+
+		/* Similarly, update scaled mdev with regard to |m| */
+		if (m < 0) {
+			m = -m;
+			m -= (hc->tx_mdev >> 2);
+			/*
+			 * This neutralises RTO increase when RTT < SRTT - mdev
+			 * (see P. Sarolahti, A. Kuznetsov,"Congestion Control
+			 * in Linux TCP", USENIX 2002, pp. 49-62).
+			 */
+			if (m > 0)
+				m >>= 3;
+		} else {
+			m -= (hc->tx_mdev >> 2);
+		}
+		hc->tx_mdev += m;
+
+		if (hc->tx_mdev > hc->tx_mdev_max) {
+			hc->tx_mdev_max = hc->tx_mdev;
+			if (hc->tx_mdev_max > hc->tx_rttvar)
+				hc->tx_rttvar = hc->tx_mdev_max;
+		}
+
+		/*
+		 * Decay RTTVAR at most once per flight, exploiting that
+		 *  1) pipe <= cwnd <= Sequence_Window = W  (RFC 4340, 7.5.2)
+		 *  2) AWL = GSS-W+1 <= GAR <= GSS          (RFC 4340, 7.5.1)
+		 * GAR is a useful bound for FlightSize = pipe.
+		 * AWL is probably too low here, as it over-estimates pipe.
+		 */
+		if (after48(dccp_sk(sk)->dccps_gar, hc->tx_rtt_seq)) {
+			if (hc->tx_mdev_max < hc->tx_rttvar)
+				hc->tx_rttvar -= (hc->tx_rttvar -
+						  hc->tx_mdev_max) >> 2;
+			hc->tx_rtt_seq  = dccp_sk(sk)->dccps_gss;
+			hc->tx_mdev_max = tcp_rto_min(sk);
+		}
+	}
+
+	/*
+	 * Set RTO from SRTT and RTTVAR
+	 * As in TCP, 4 * RTTVAR >= TCP_RTO_MIN, giving a minimum RTO of 200 ms.
+	 * This agrees with RFC 4341, 5:
+	 *	"Because DCCP does not retransmit data, DCCP does not require
+	 *	 TCP's recommended minimum timeout of one second".
+	 */
+	hc->tx_rto = (hc->tx_srtt >> 3) + hc->tx_rttvar;
+
+	if (hc->tx_rto > DCCP_RTO_MAX)
+		hc->tx_rto = DCCP_RTO_MAX;
+}
+
+/************************************************************/
+/* BELLOW THE FUNCTIONS WHICH IN TCP ARE PART OF tcp_rate.c */
+/************************************************************/
+
+void dccp_rate_skb_delivered(struct sock *sk, struct ccid5_seq *acked,
+			    struct rate_sample_ccid5 *rs)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	if (!acked->delivered_mstamp)
+		return;
+
+	if (!rs->prior_delivered ||
+	    after(acked->delivered, rs->prior_delivered)) { 
+		rs->prior_delivered  = acked->delivered;
+		rs->prior_mstamp     = acked->delivered_mstamp;
+		rs->is_app_limited   = acked->is_app_limited;
+
+		/* Find the duration of the "send phase" of this window: */
+		rs->interval_us      = tcp_stamp_us_delta(
+						acked->sent_mstamp,
+						acked->first_tx_mstamp);
+		/* Record send time of most recently ACKed packet: */
+		hc->first_tx_mstamp  = acked->sent_mstamp;
+		}
+}
+
+void dccp_rate_gen(struct sock *sk, u32 delivered, u32 lost, u64 now, struct rate_sample_ccid5 *rs)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	s64 ack_us;
+	s64 snd_us;	
+	
+	/* Clear app limited if bubble is acked and gone. */
+	if (hc->app_limited && after(hc->delivered, hc->app_limited))
+		hc->app_limited = 0;
+
+	if (delivered) 
+		hc->delivered_mstamp = now; 
+	
+	rs->acked_sacked = delivered;	/* freshly ACKed or SACKed */
+	rs->losses = lost;		/* freshly marked lost */
+
+	if (!rs->prior_mstamp) {
+		rs->delivered = -1;
+		rs->interval_us = -1;
+		return;
+	}
+
+	rs->delivered   = hc->delivered - rs->prior_delivered;
+
+	//// takes maximum between send_us and ack_us
+	snd_us = rs->interval_us;				/* send phase */
+	ack_us = tcp_stamp_us_delta(now, rs->prior_mstamp);
+	rs->interval_us = max(snd_us, ack_us);
+
+}
+
+void dccp_rate_check_app_limited(struct sock *sk, int tsize)
+{
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	if (hc->bytes_att - hc->bytes_sent < dp->dccps_mss_cache &&
+		sk_wmem_alloc_get(sk) < tsize &&
+	    hc->tx_pipe < hc->tx_cwnd) 
+		hc->app_limited =
+			(hc->delivered + hc->tx_pipe) ? : 1;
+}
+
+/*****************************************************/
+/*       FUNCTIONS PART OF tcp_rate.c END HERE       */
+/*****************************************************/
+
+
+/*****************************************************/
+/*       FUNCTIONS PART OF tcp_bbr.c START HERE       */
+/*****************************************************/
+
+static bool bbr_full_bw_reached(const struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+
+	return hc->full_bw_reached;
+}
+
+static u32 bbr_max_bw(const struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	return minmax_get(&hc->bw);
+}
+
+/* Return the estimated bandwidth of the path, in pkts/uS << BW_SCALE. */
+static u32 bbr_bw(const struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	return hc->lt_use_bw ? hc->lt_bw : bbr_max_bw(sk);
+}
+
+
+static u64 bbr_rate_bytes_per_sec(struct sock *sk, u64 rate, int gain)
+{
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	rate *= icsk->icsk_pmtu_cookie;
+	rate *= gain;
+	rate >>= BBR_SCALE;
+	rate *= USEC_PER_SEC;
+	return rate >> BW_SCALE;
+}
+
+
+/* Convert a BBR bw and gain factor to a pacing rate in bytes per second. */
+static u32 bbr_bw_to_pacingrate(struct sock *sk, u32 bw, int gain)
+{
+	u64 rate = bw;
+
+	rate = bbr_rate_bytes_per_sec(sk, rate, gain);
+	rate = min_t(u64, rate, sk->sk_max_pacing_rate);
+	return rate;
+}
+
+static void bbr_init_pacingrate(struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	u64 bw;
+	u32 rtt_us;
+	if (hc->tx_srtt) {
+		rtt_us = max(hc->tx_srtt >> 3, 1U); 
+		hc->has_seen_rtt = 1; 
+	} else {			 /* no RTT sample yet */
+		rtt_us = USEC_PER_MSEC;	 /* use nominal default RTT */
+	}
+	bw = (u64)hc->tx_cwnd * BW_UNIT;
+	do_div(bw, rtt_us);
+	sk->sk_pacing_rate = bbr_bw_to_pacingrate(sk, bw, bbr_high_gain);
+	hc->pr_init = 1;
+}
+
+static void bbr_set_pacingrate(struct sock *sk, u32 bw, int gain)
+{
+	u32 rate = bbr_bw_to_pacingrate(sk, bw, gain);
+
+	if (bbr_full_bw_reached(sk) || rate > sk->sk_pacing_rate)
+		sk->sk_pacing_rate = rate;
+}
+
+u32 dccp_tso_autosize(const struct sock *sk, unsigned int mss_now,
+		     int min_tso_segs)
+{
+	u32 bytes, segs;
+
+	bytes = min(sk->sk_pacing_rate >> 10,
+		    (u32)(sk->sk_gso_max_size - 1 - MAX_DCCP_HEADER));
+	segs = max_t(u32, bytes / mss_now, min_tso_segs);
+
+	return segs;
+}
+
+static void bbr_set_tso_segs_goal(struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	struct dccp_sock *dp = dccp_sk(sk);
+	u32 min_segs;
+
+	min_segs = sk->sk_pacing_rate < (bbr_min_tso_rate >> 3) ? 1 : 2;
+	hc->tso_segs_goal = min(dccp_tso_autosize(sk, dp->dccps_mss_cache, min_segs),
+				 0x7FU);
+}
+
+static u32 bbr_target_cwnd(struct sock *sk, u32 bw, int gain)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	u32 cwnd;
+	u64 w;
+
+	/* If we've never had a valid RTT sample, cap cwnd at the initial
+	 * default. This should only happen when the connection is not using TCP
+	 * timestamps and has retransmitted all of the SYN/SYNACK/data packets
+	 * ACKed so far. In this case, an RTO can cut cwnd to 1, in which
+	 * case we need to slow-start up toward something safe: TCP_INIT_CWND.
+	 */
+	if (unlikely(hc->min_rtt_us == ~0U))	 /* no valid RTT samples yet? */
+		return 4;  /* be safe: cap at default initial cwnd*/
+
+	w = (u64)bw * hc->min_rtt_us;
+
+	/* Apply a gain to the given value, then remove the BW_SCALE shift. */
+	cwnd = (((w * gain) >> BBR_SCALE) + BW_UNIT - 1) / BW_UNIT;
+	
+
+	/* Allow enough full-sized skbs in flight to utilize end systems. */
+	//cwnd += 3 * bbr->tso_segs_goal; WHYYYY loggea en tcp
+	cwnd += 3 * hc->tso_segs_goal;
+	//cwnd += 3;
+
+	/* Reduce delayed ACKs by rounding up cwnd to the next even number. */
+	cwnd = (cwnd + 1) & ~1U;
+
+	return cwnd;
+}
+
+static bool bbr_set_cwnd_to_recover_or_restore(
+	struct sock *sk, const struct rate_sample_ccid5 *rs, u32 acked, u32 *new_cwnd)
+{
+
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	u8 prev_state = hc->prev_ca_state, state = hc->curr_ca_state;
+	u32 cwnd = hc->tx_cwnd;
+
+	/* An ACK for P pkts should release at most 2*P packets. We do this
+	 * in two steps. First, here we deduct the number of lost packets.
+	 * Then, in bbr_set_cwnd() we slow start up toward the target cwnd.
+	 */
+	if (rs->losses > 0)
+		cwnd = max_t(s32, cwnd - rs->losses, 1);
+
+	if (state == DCCP_CA_Recovery && prev_state != DCCP_CA_Recovery) {
+		/* Starting 1st round of Recovery, so do packet conservation. */
+		hc->packet_conservation = 1;
+		hc->next_rtt_delivered = hc->delivered;  /* start round now */
+		/* Cut unused cwnd from app behavior, TSQ, or TSO deferral: */
+		cwnd = hc->tx_pipe + acked;
+	} else if (prev_state >= DCCP_CA_Recovery && state < DCCP_CA_Recovery) {
+		/* Exiting loss recovery; restore cwnd saved before recovery. */
+		hc->restore_cwnd = 1;
+		hc->packet_conservation = 0;
+	}
+	hc->prev_ca_state = state;
+
+	/* Restore cwnd after seq_window */
+	if (hc->restore_cwnd && !hc->restore_seqwin) {
+		/* Restore cwnd after exiting loss recovery or PROBE_RTT. */
+		cwnd = max(cwnd, hc->prior_cwnd);
+		hc->restore_cwnd = 0;
+	}
+
+	if (hc->packet_conservation) {
+		*new_cwnd = max(cwnd, hc->tx_pipe + acked);
+		return true;	/* yes, using packet conservation */
+	}
+	*new_cwnd = cwnd;
+	return false;
+}
+
+
+static void bbr_set_cwnd(struct sock *sk, const struct rate_sample_ccid5 *rs,
+			 u32 acked, u32 bw, int gain)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	struct dccp_sock *dp = dccp_sk(sk);
+	int r_seq_used = hc->tx_cwnd / dp->dccps_l_ack_ratio;
+	u32 cwnd = 0, target_cwnd = 0;
+
+	if (!acked)
+		return;
+
+	if (bbr_set_cwnd_to_recover_or_restore(sk, rs, acked, &cwnd))
+		goto done;
+
+	/* If we're below target cwnd, slow start cwnd toward target cwnd. */
+
+	target_cwnd = bbr_target_cwnd(sk, bw, gain);
+	if (bbr_full_bw_reached(sk))  /* only cut cwnd if we filled the pipe */
+		cwnd = min(cwnd + acked, target_cwnd);
+	else if (cwnd < target_cwnd || hc->delivered < 10)
+		cwnd = cwnd + acked;
+	cwnd = max(cwnd, bbr_cwnd_min_target);
+
+done:
+	hc->tx_cwnd=cwnd;
+	if (hc->mode == BBR_PROBE_RTT) {  /* drain queue, refresh min_rtt */
+		hc->tx_cwnd = min(hc->tx_cwnd, bbr_cwnd_min_target);
+		ccid5_change_l_ack_ratio(sk, 1);
+		/* Allow extra packet(s) to let ack_ratio=1 option reaching the peer */
+		if (dccp_sk(sk)->dccps_l_ack_ratio != 1U) {
+			hc->tx_extrapkt = true;
+			dccp_tasklet_schedule(sk);
+		}
+	}
+
+	/* Do not adjust the ack_ratio if we are restoring it or we are in PROBE_RTT mode */
+	if (hc->restore_ackrt) {
+		ccid5_change_l_ack_ratio(sk, hc->prior_ackrt);
+		/* Restore should end when rx has sent confirmation */
+		if (hc->prior_ackrt == dp->dccps_l_ack_ratio) hc->restore_ackrt=0;
+	}
+	else if (hc->mode != BBR_PROBE_RTT) {
+		if (r_seq_used * CCID5_WIN_CHANGE_FACTOR >= dp->dccps_r_seq_win)
+			ccid5_change_l_ack_ratio(sk, dp->dccps_l_ack_ratio * 2);
+		else if (r_seq_used * CCID5_WIN_CHANGE_FACTOR < dp->dccps_r_seq_win/2)
+			ccid5_change_l_ack_ratio(sk, dp->dccps_l_ack_ratio / 2 ? : 1U);
+	}
+
+	/* Do not adjust the seq_window if we are restoring it */
+	if (hc->restore_seqwin) {
+		ccid5_change_l_seq_window(sk, hc->prior_seqwin);
+		/* HACK: force local seq_win to new value without waiting confirmation */
+		dp->dccps_l_seq_win = hc->prior_seqwin;
+		dccp_update_gss(sk, dp->dccps_gss);
+		hc->restore_seqwin=0;
+	}
+	else if (hc->tx_cwnd * CCID5_WIN_CHANGE_FACTOR >= dp->dccps_l_seq_win)
+		ccid5_change_l_seq_window(sk, dp->dccps_l_seq_win * 2);
+	else if (hc->tx_cwnd * CCID5_WIN_CHANGE_FACTOR < dp->dccps_l_seq_win/2)
+		ccid5_change_l_seq_window(sk, dp->dccps_l_seq_win / 2);
+}
+
+
+static bool bbr_is_next_cycle_phase(struct sock *sk,
+				    const struct rate_sample_ccid5 *rs)
+{
+
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	bool is_full_length =
+		tcp_stamp_us_delta(hc->delivered_mstamp, hc->cycle_mstamp) >
+		hc->min_rtt_us;
+	u32 inflight, bw, test;
+
+	/* The pacing_gain of 1.0 paces at the estimated bw to try to fully
+	 * use the pipe without increasing the queue.
+	 */
+	if (hc->pacing_gain == BBR_UNIT)
+		return is_full_length;		/* just use wall clock time */
+
+	inflight = rs->prior_in_flight;  /* what was in-flight before ACK? */
+	bw = bbr_max_bw(sk);
+
+	/* A pacing_gain > 1.0 probes for bw by trying to raise inflight to at
+	 * least pacing_gain*BDP; this may take more than min_rtt if min_rtt is
+	 * small (e.g. on a LAN). We do not persist if packets are lost, since
+	 * a path with small buffers may not hold that much.
+	 */
+	if (hc->pacing_gain > BBR_UNIT) {
+		test=bbr_target_cwnd(sk, bw, hc->pacing_gain);
+
+		return is_full_length && 
+			(rs->losses ||  /* perhaps pacing_gain*BDP won't fit */
+			 inflight >= test);
+	}
+			 
+
+	/* A pacing_gain < 1.0 tries to drain extra queue we added if bw
+	 * probing didn't find more bw. If inflight falls to match BDP then we
+	 * estimate queue is drained; persisting would underutilize the pipe.
+	 */
+	return is_full_length ||
+		inflight <= bbr_target_cwnd(sk, bw, BBR_UNIT);
+}
+
+static void bbr_advance_cycle_phase(struct sock *sk)
+{
+	
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+
+	hc->cycle_idx = (hc->cycle_idx + 1) & (CYCLE_LEN - 1);
+	hc->cycle_mstamp = hc->delivered_mstamp;
+	hc->pacing_gain = bbr_pacing_gain[hc->cycle_idx];
+}
+
+static void bbr_update_cycle_phase(struct sock *sk,
+				   const struct rate_sample_ccid5 *rs)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+
+	if (hc->mode == BBR_PROBE_BW && bbr_is_next_cycle_phase(sk, rs))
+		bbr_advance_cycle_phase(sk);
+}
+
+static void bbr_reset_startup_mode(struct ccid5_hc_tx_sock *hc)
+{
+	hc->mode = BBR_STARTUP;
+	hc->pacing_gain = bbr_high_gain;
+	hc->cwnd_gain	 = bbr_high_gain;
+}
+
+static void bbr_reset_probe_bw_mode(struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	hc->mode = BBR_PROBE_BW;
+	hc->pacing_gain = BBR_UNIT;
+	hc->cwnd_gain = bbr_cwnd_gain;
+	hc->cycle_idx = CYCLE_LEN - 1 - prandom_u32_max(bbr_cycle_rand);
+	bbr_advance_cycle_phase(sk);	/* flip to next phase of gain cycle */
+}
+
+static void bbr_reset_mode(struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	if (!bbr_full_bw_reached(sk))
+		bbr_reset_startup_mode(hc);
+	else
+		bbr_reset_probe_bw_mode(sk);
+}
+
+static void bbr_reset_lt_bw_sampling_interval(struct ccid5_hc_tx_sock *hc)
+{
+
+	hc->lt_last_stamp = div_u64(hc->delivered_mstamp, USEC_PER_MSEC);
+	hc->lt_last_delivered = hc->delivered;
+	hc->lt_last_lost = hc->lost;
+	hc->lt_rtt_cnt = 0;
+}
+
+/* Completely reset long-term bandwidth sampling. */
+static void bbr_reset_lt_bw_sampling(struct ccid5_hc_tx_sock *hc)
+{
+
+	hc->lt_bw = 0;
+	hc->lt_use_bw = 0;
+	hc->lt_is_sampling = false;
+	bbr_reset_lt_bw_sampling_interval(hc);
+}
+
+/* Long-term bw sampling interval is done. Estimate whether we're policed. */
+static void bbr_lt_bw_interval_done(struct sock *sk, u32 bw)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	u32 diff;
+
+	if (hc->lt_bw) {  /* do we have bw from a previous interval? */
+		/* Is new bw close to the lt_bw from the previous interval? */
+		diff = abs(bw - hc->lt_bw);
+		if ((diff * BBR_UNIT <= bbr_lt_bw_ratio * hc->lt_bw) ||
+		    (bbr_rate_bytes_per_sec(sk, diff, BBR_UNIT) <=
+		     bbr_lt_bw_diff)) {
+			/* All criteria are met; estimate we're policed. */
+			hc->lt_bw = (bw + hc->lt_bw) >> 1;  /* avg 2 intvls */
+			hc->lt_use_bw = 1;
+			hc->pacing_gain = BBR_UNIT;  /* try to avoid drops */
+			hc->lt_rtt_cnt = 0;
+			return;
+		}
+	}
+	hc->lt_bw = bw;
+	bbr_reset_lt_bw_sampling_interval(hc);
+}
+
+static void bbr_lt_bw_sampling(struct sock *sk, const struct rate_sample_ccid5 *rs)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	u32 lost, delivered;
+	u64 bw;
+	s32 t;
+
+	if (hc->lt_use_bw) {	/* already using long-term rate, lt_bw? */
+		if (hc->mode == BBR_PROBE_BW && hc->round_start &&
+		    ++hc->lt_rtt_cnt >= bbr_lt_bw_max_rtts) {
+			bbr_reset_lt_bw_sampling(hc);    /* stop using lt_bw */
+			bbr_reset_probe_bw_mode(sk);  /* restart gain cycling */
+		}
+		return;
+	}
+
+	/* Wait for the first loss before sampling, to let the policer exhaust
+	 * its tokens and estimate the steady-state rate allowed by the policer.
+	 * Starting samples earlier includes bursts that over-estimate the bw.
+	 */
+	if (!hc->lt_is_sampling) {
+		if (!rs->losses)
+			return;
+		bbr_reset_lt_bw_sampling_interval(hc);
+		hc->lt_is_sampling = true;
+	}
+
+	/* To avoid underestimates, reset sampling if we run out of data. */
+	if (rs->is_app_limited) {
+		bbr_reset_lt_bw_sampling(hc);
+		return;
+	}
+
+	if (hc->round_start)
+		hc->lt_rtt_cnt++;	/* count round trips in this interval */
+	if (hc->lt_rtt_cnt < bbr_lt_intvl_min_rtts)
+		return;		/* sampling interval needs to be longer */
+	if (hc->lt_rtt_cnt > 4 * bbr_lt_intvl_min_rtts) {
+		bbr_reset_lt_bw_sampling(hc);  /* interval is too long */
+		return;
+	}
+
+	/* End sampling interval when a packet is lost, so we estimate the
+	 * policer tokens were exhausted. Stopping the sampling before the
+	 * tokens are exhausted under-estimates the policed rate.
+	 */
+	if (!rs->losses)
+		return;
+
+	/* Calculate packets lost and delivered in sampling interval. */
+	lost = hc->lost - hc->lt_last_lost;
+	delivered = hc->delivered - hc->lt_last_delivered;
+	/* Is loss rate (lost/delivered) >= lt_loss_thresh? If not, wait. */
+	if (!delivered || (lost << BBR_SCALE) < bbr_lt_loss_thresh * delivered)
+		return;
+
+	/* Find average delivery rate in this sampling interval. */
+	t = (s32)(hc->delivered_mstamp - hc->lt_last_stamp);
+	if (t < 1)
+		return;		/* interval is less than one jiffy, so wait */
+	/* Interval long enough for jiffies_to_usecs() to return a bogus 0? */
+	if (t < 1) {
+		bbr_reset_lt_bw_sampling(hc);  /* interval too long; reset */
+		return;
+	}
+	bw = (u64)delivered * BW_UNIT;
+	do_div(bw, t);
+	bbr_lt_bw_interval_done(sk, bw);
+}
+
+static void bbr_update_btl_bw(struct sock *sk, const struct rate_sample_ccid5 *rs)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	u64 bw;
+
+	hc->round_start = 0;
+	if (rs->delivered < 0 || rs->interval_us <= 0)
+		return; /* Not a valid observation */
+
+	/* See if we've reached the next RTT */
+	if (!before(rs->prior_delivered, hc->next_rtt_delivered)) {
+		hc->next_rtt_delivered = hc->delivered;
+		hc->rtt_cnt++;
+		hc->round_start = 1;
+		hc->packet_conservation = 0;
+	}
+
+	bbr_lt_bw_sampling(sk, rs);
+
+
+	bw = (u64)rs->delivered * BW_UNIT;
+	do_div(bw, rs->interval_us);
+	/* Don't include bw samples during PROBE_RTT and cwnd/ackrt/seqwin recovery */
+	if ((!rs->is_app_limited && !hc->restore_cwnd && !hc->restore_seqwin && !hc->restore_ackrt)
+		|| bw >= bbr_max_bw(sk)) {
+		/* Incorporate new sample into our max bw filter. */
+		minmax_running_max(&hc->bw, bbr_bw_rtts, hc->rtt_cnt, bw); 
+	}
+}
+
+static void bbr_save_cwnd(struct sock *sk)
+{
+
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	struct dccp_sock *dp = dccp_sk(sk);
+
+	if (hc->prev_ca_state < DCCP_CA_Recovery && hc->mode != BBR_PROBE_RTT)
+		hc->prior_cwnd = hc->tx_cwnd;  /* this cwnd is good enough */
+	else  /* loss recovery or BBR_PROBE_RTT have temporarily cut cwnd */
+		hc->prior_cwnd = max(hc->prior_cwnd, hc->tx_cwnd);
+
+	/* Save ack_ratio and seq_window as well */
+	hc->prior_ackrt = dp->dccps_l_ack_ratio;
+	hc->prior_seqwin = dp->dccps_l_seq_win;
+}
+
+static void bbr_check_full_bw_reached(struct sock *sk,
+				      const struct rate_sample_ccid5 *rs)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	u32 bw_thresh;
+	if (bbr_full_bw_reached(sk) || !hc->round_start || rs->is_app_limited)
+		return;
+
+	bw_thresh = (u64)hc->full_bw * bbr_full_bw_thresh >> BBR_SCALE;
+	if (bbr_max_bw(sk) >= bw_thresh) {
+		hc->full_bw = bbr_max_bw(sk);
+		hc->full_bw_cnt = 0;
+		return;
+	}
+	// if there is no significant growth increment the count, after 3 counts, it asumes (estimate)
+	// the pipe is full
+	++hc->full_bw_cnt;
+	hc->full_bw_reached = hc->full_bw_cnt >= bbr_full_bw_cnt;
+}
+
+static void bbr_check_drain(struct sock *sk, const struct rate_sample_ccid5 *rs)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+
+	if (hc->mode == BBR_STARTUP && bbr_full_bw_reached(sk)) {
+		hc->mode = BBR_DRAIN;	/* drain queue we created */
+		hc->pacing_gain = bbr_drain_gain;	/* pace slow to drain */
+		hc->cwnd_gain = BBR_UNIT;	/* don't increase cwnd */
+	}	/* fall through to check if in-flight is already small: */
+	if (hc->mode == BBR_DRAIN &&
+	    hc->tx_pipe <=
+	    bbr_target_cwnd(sk, bbr_max_bw(sk), BBR_UNIT))
+		bbr_reset_probe_bw_mode(sk);  /* we estimate queue is drained */
+}
+
+static void bbr_update_rt_prop(struct sock *sk, const struct rate_sample_ccid5 *rs)
+{
+	
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	bool filter_expired;
+	if (rs->delivered < 0 || rs->interval_us <= 0)
+		return; /* Not a valid observation */
+
+	
+	filter_expired = false;
+
+	/* Track min RTT seen in the min_rtt_win_sec filter window: */
+	filter_expired = after(ccid5_jiffies32,
+			       hc->min_rtt_stamp + bbr_min_rtt_win_sec * HZ);
+
+	/* not sure if the following condition is necessary, maybe because my initializarion is wrong*/
+	/* it is necessary ut there's got to be a better way*/
+
+	if (hc->min_rtt_us==0 && rs->rtt_us > 0) {
+		hc->min_rtt_us = rs->rtt_us;
+		hc->min_rtt_stamp = ccid5_jiffies32;	
+		}
+
+
+	if (rs->rtt_us > 0 &&
+	    (rs->rtt_us <= hc->min_rtt_us || filter_expired)) {
+		hc->min_rtt_us = rs->rtt_us;
+		hc->min_rtt_stamp = ccid5_jiffies32;
+		}
+
+	if (rs->rtt_us > 0 &&
+		hc->max_rtt_us <= rs->rtt_us){
+		hc->max_rtt_us = rs->rtt_us;
+		hc->max_rtt_stamp = ccid5_jiffies32;
+	}
+
+	//Equivalent to check_probe_rtt
+	if (bbr_probe_rtt_mode_ms > 0 && filter_expired &&
+	    !hc->idle_restart && hc->mode != BBR_PROBE_RTT) {
+		hc->mode = BBR_PROBE_RTT;  /* dip, drain queue */
+		hc->pacing_gain = BBR_UNIT;
+		hc->cwnd_gain = BBR_UNIT;
+		bbr_save_cwnd(sk);  /* note cwnd so we can restore it */
+		hc->probe_rtt_done_stamp = 0;
+		hc->rtprop_fix=0;
+	}
+
+	//Equivalent to enter and handle? probe_rtt
+	if (hc->mode == BBR_PROBE_RTT) {
+		/* Ignore low rate samples during this mode. WHY ??*/
+		hc->app_limited =
+			(hc->delivered + hc->tx_pipe) ? : 1;
+		/* Maintain min packets in flight for max(200 ms, 1 round). */
+		if (!hc->probe_rtt_done_stamp &&
+		    hc->tx_pipe <= bbr_cwnd_min_target) {
+			hc->probe_rtt_done_stamp = ccid5_jiffies32 +
+				msecs_to_jiffies(bbr_probe_rtt_mode_ms);
+			hc->probe_rtt_round_done = 0;
+			hc->next_rtt_delivered = hc->delivered; 
+		} else if (hc->probe_rtt_done_stamp) {
+			if (hc->round_start)
+				hc->probe_rtt_round_done = 1;
+			if (hc->probe_rtt_round_done &&
+			    after(ccid5_jiffies32, hc->probe_rtt_done_stamp)) {
+				hc->min_rtt_stamp = ccid5_jiffies32;
+				hc->restore_cwnd = 1;  /* snap to prior_cwnd */
+				hc->restore_ackrt = 1;  /* snap to prior_ackrt */
+				hc->restore_seqwin = 1;  /* snap to prior_seqwin */
+				bbr_reset_mode(sk);
+			}
+		}
+	}
+	hc->idle_restart = 0;
+}
+
+static void bbr_set_state(struct sock *sk, u8 new_state)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+
+	if (new_state == DCCP_CA_Loss) {
+		hc->prev_ca_state = DCCP_CA_Loss;
+		hc->full_bw = 0;
+		hc->round_start = 1;	/* treat RTO like end of a round */
+		//bbr_lt_bw_sampling(sk, &rs);
+	}
+}
+
+/*****************************************************/
+/*       FUNCTIONS PART OF tcp_bbr.c END HERE       */
+/*****************************************************/
+
+
+//static void ccid5_hc_tx_rto_expire(unsigned long data)
+static void ccid5_hc_tx_rto_expire(struct timer_list *t)
+{
+	struct ccid5_hc_tx_sock *hc = from_timer(hc, t, tx_rtotimer);
+	struct sock *sk = hc->sk;
+	//struct sock *sk = (struct sock *)data;
+	//struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	const bool sender_was_blocked = ccid5_cwnd_network_limited(hc);
+
+	bh_lock_sock(sk);
+	if (sock_owned_by_user(sk)) {
+		sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + HZ / 5);
+		goto out;
+	}
+
+	if (sk->sk_state == DCCP_CLOSED)
+		goto out;
+
+	/* back-off timer */
+	hc->tx_rto <<= 1;
+	if (hc->tx_rto > DCCP_RTO_MAX)
+		hc->tx_rto = DCCP_RTO_MAX;
+
+	/* adjust pipe, cwnd etc */
+	hc->tx_ssthresh = hc->tx_cwnd / 2;
+	if (hc->tx_ssthresh < 2)
+		hc->tx_ssthresh = 2;
+	hc->lost += hc->tx_pipe; // not sure
+	hc->tx_cwnd	= 1; // not sure
+	hc->tx_pipe	= 0; // not sure
+	bbr_set_state(sk, DCCP_CA_Loss);
+
+	/* clear state about stuff we sent */
+	hc->tx_seqt = hc->tx_seqh;
+	hc->tx_packets_acked = 0;
+
+	/* clear ack ratio state. */
+	hc->tx_rpseq    = 0;
+	hc->tx_rpdupack = -1;
+	ccid5_change_l_ack_ratio(sk, 1);
+
+	/* if we were blocked before, we may now send cwnd=1 packet */
+	if (sender_was_blocked)
+		dccp_tasklet_schedule(sk);
+	/* restart backed-off timer */
+	sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
+out:
+	bh_unlock_sock(sk);
+	sock_put(sk);
+}
+
+static int ccid5_hc_tx_send_packet(struct sock *sk, struct sk_buff *skb)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	if (!hc->pr_init) {
+		bbr_init_pacingrate(sk);
+	}
+	
+	dccp_rate_check_app_limited(sk, skb->truesize); 
+	hc->bytes_att += skb->len;
+	if (hc->mode==BBR_PROBE_RTT && hc->probe_rtt_done_stamp &&
+		    hc->tx_pipe >= bbr_cwnd_min_target && !hc->rtprop_fix) {
+		hc->rtprop_fix=1;
+	}
+
+	/* Allow extra packet(s) to be sent during the drain phase */
+	if (hc->mode==BBR_PROBE_RTT && hc->tx_extrapkt) {
+		hc->tx_extrapkt = false;
+		return CCID_PACKET_SEND_AT_ONCE;
+	}
+
+	if (ccid5_cwnd_network_limited(hc))
+		return CCID_PACKET_WILL_DEQUEUE_LATER;
+	return CCID_PACKET_SEND_AT_ONCE;
+}
+
+
+static int ccid5_hc_tx_parse_options(struct sock *sk, u8 packet_type,
+				     u8 option, u8 *optval, u8 optlen)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+
+	switch (option) {
+	case DCCPO_ACK_VECTOR_0:
+	case DCCPO_ACK_VECTOR_1:
+		return dccp_ackvec_parsed_add(&hc->tx_av_chunks, optval, optlen,
+					      option - DCCPO_ACK_VECTOR_0);
+	}
+	return 0;
+}
+
+static void ccid5_hc_tx_packet_recv(struct sock *sk, struct sk_buff *skb)
+{
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	const bool sender_was_blocked = ccid5_cwnd_network_limited(hc);
+	struct dccp_ackvec_parsed *avp;
+	u64 ackno, seqno;
+	struct ccid5_seq *seqp;
+	int done = 0;
+	bool not_rst = 0;
+	unsigned int maxincr = 0;
+	struct rate_sample_ccid5 rs_i = { .prior_delivered = 0 };
+	struct rate_sample_ccid5 *rs = &rs_i;
+	u32 bw;
+	u32 delivered = hc->delivered;
+	u32 lost = hc->lost;
+	u64 now_mstamp;
+	now_mstamp = tcp_clock_us();
+	
+	rs->prior_in_flight = hc->tx_pipe;
+
+	/* check reverse path congestion */
+	seqno = DCCP_SKB_CB(skb)->dccpd_seq;
+
+	/* XXX this whole "algorithm" is broken.  Need to fix it to keep track
+	 * of the seqnos of the dupacks so that rpseq and rpdupack are correct
+	 * -sorbo.
+	 */
+	/* need to bootstrap */
+	if (hc->tx_rpdupack == -1) {
+		hc->tx_rpdupack = 0;
+		hc->tx_rpseq    = seqno;
+	} else {
+		/* check if packet is consecutive */
+		if (dccp_delta_seqno(hc->tx_rpseq, seqno) == 1)
+			hc->tx_rpseq = seqno;
+		/* it's a later packet */
+		else if (after48(seqno, hc->tx_rpseq)) {
+			hc->tx_rpdupack++;
+
+			/* check if we got enough dupacks */
+			if (hc->tx_rpdupack >= NUMDUPACK) {
+				hc->tx_rpdupack = -1; /* XXX lame */
+				hc->tx_rpseq    = 0;
+#ifdef __CCID5_COPES_GRACEFULLY_WITH_ACK_CONGESTION_CONTROL__
+				/*
+				 * FIXME: Ack Congestion Control is broken; in
+				 * the current state instabilities occurred with
+				 * Ack Ratios greater than 1; causing hang-ups
+				 * and long RTO timeouts. This needs to be fixed
+				 * before opening up dynamic changes. -- gerrit
+				 */
+				ccid5_change_l_ack_ratio(sk, 2 * dp->dccps_l_ack_ratio);
+#endif
+			}
+		}
+	}
+
+	/* check forward path congestion */
+	if (dccp_packet_without_ack(skb))
+		return;
+
+	/* still didn't send out new data packets */
+	if (hc->tx_seqh == hc->tx_seqt)
+		goto done;
+
+	ackno = DCCP_SKB_CB(skb)->dccpd_ack_seq;
+	if (after48(ackno, hc->tx_high_ack))
+		hc->tx_high_ack = ackno;
+
+	seqp = hc->tx_seqt;
+	while (before48(seqp->ccid5s_seq, ackno)) {
+		seqp = seqp->ccid5s_next;
+		if (seqp == hc->tx_seqh) {
+			seqp = hc->tx_seqh->ccid5s_prev;
+			not_rst = 1;
+			break;
+		}
+	}
+
+	/*
+	 * In slow-start, cwnd can increase up to a maximum of Ack Ratio/2
+	 * packets per acknowledgement. Rounding up avoids that cwnd is not
+	 * advanced when Ack Ratio is 1 and gives a slight edge otherwise.
+	 */
+	if (hc->tx_cwnd < hc->tx_ssthresh)
+		maxincr = DIV_ROUND_UP(dp->dccps_l_ack_ratio, 2);
+
+	/* go through all ack vectors */
+	list_for_each_entry(avp, &hc->tx_av_chunks, node) {
+		/* go through this ack vector */
+		for (; avp->len--; avp->vec++) {
+			//printk(KERN_INFO "natrm: en el for que no entiendo avp_len %d", avp->len);
+			u64 ackno_end_rl = SUB48(ackno,
+						 dccp_ackvec_runlen(avp->vec));
+			/* if the seqno we are analyzing is larger than the
+			 * current ackno, then move towards the tail of our
+			 * seqnos.
+			 */
+			while (after48(seqp->ccid5s_seq, ackno)) {
+				if (seqp == hc->tx_seqt) {
+					done = 1;
+					break;
+				}
+				seqp = seqp->ccid5s_prev;
+			}
+			if (done)
+				break;
+
+			/* check all seqnos in the range of the vector
+			 * run length
+			 */
+			while (between48(seqp->ccid5s_seq,ackno_end_rl,ackno)) {
+				const u8 state = dccp_ackvec_state(avp->vec);
+
+				/* new packet received or marked */
+				if (state != DCCPAV_NOT_RECEIVED &&
+				    !seqp->ccid5s_acked) {
+					if (state == DCCPAV_ECN_MARKED)
+						hc->lost++;
+					ccid5_rtt_estimator(sk, ccid5_jiffies32 - seqp->ccid5s_sent);
+					seqp->ccid5s_acked = 1;
+					hc->delivered++;
+					hc->tx_pipe--;
+					dccp_rate_skb_delivered(sk, seqp, rs);
+					if (seqp->ccid5s_seq == ackno)	{ 
+						rs->rtt_us = tcp_stamp_us_delta(now_mstamp, seqp->sent_mstamp);
+						hc->rtt_us = rs->rtt_us;
+					}
+				}
+				if (seqp == hc->tx_seqt) {
+					done = 1;
+					break;
+				}
+				seqp = seqp->ccid5s_prev;
+			}
+			if (done)
+				break;
+
+			ackno = SUB48(ackno_end_rl, 1);
+		}
+		if (done)
+			break;
+	}
+
+	/* The state about what is acked should be correct now
+	 * Check for NUMDUPACK
+	 */
+	seqp = hc->tx_seqt;
+	while (before48(seqp->ccid5s_seq, hc->tx_high_ack)) {
+		seqp = seqp->ccid5s_next;
+		if (seqp == hc->tx_seqh) {
+			seqp = hc->tx_seqh->ccid5s_prev;
+			break;
+		}
+	}
+	done = 0;
+	while (1) {
+		if (seqp->ccid5s_acked) {
+			done++;
+			if (done == NUMDUPACK)
+				break;
+		}
+		if (seqp == hc->tx_seqt)
+			break;
+		seqp = seqp->ccid5s_prev;
+	}
+
+	/* If there are at least 3 acknowledgements, anything unacknowledged
+	 * below the last sequence number is considered lost
+	 */
+	if (done == NUMDUPACK) {
+		struct ccid5_seq *last_acked = seqp;
+
+		/* check for lost packets */
+		while (1) {
+			if (!seqp->ccid5s_acked) {
+				ccid5_pr_debug("Packet lost: %llu\n",
+					       (unsigned long long)seqp->ccid5s_seq);
+				/* XXX need to traverse from tail -> head in
+				 * order to detect multiple congestion events in
+				 * one ack vector.
+				 */
+				hc->lost++;
+				hc->tx_pipe--;
+			}
+			if (seqp == hc->tx_seqt)
+				break;
+			seqp = seqp->ccid5s_prev;
+		}
+
+		hc->tx_seqt = last_acked;
+	}
+
+	/* trim acked packets in tail */
+	while (hc->tx_seqt != hc->tx_seqh) {
+		if (!hc->tx_seqt->ccid5s_acked)
+			break;
+
+		hc->tx_seqt = hc->tx_seqt->ccid5s_next;
+	}
+
+	/* restart RTO timer if not all outstanding data has been acked */
+	if (hc->tx_pipe == 0)
+		sk_stop_timer(sk, &hc->tx_rtotimer);
+	else if(!not_rst)
+		sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
+	delivered = hc->delivered - delivered;
+	lost = hc->lost - lost;	
+	dccp_rate_gen(sk, delivered, lost, now_mstamp, rs);
+
+	bbr_update_btl_bw(sk, rs);
+	bbr_update_cycle_phase(sk, rs);
+	bbr_check_full_bw_reached(sk, rs);
+	bbr_check_drain(sk, rs);
+	bbr_update_rt_prop(sk, rs);
+	bw = bbr_bw(sk);
+	bbr_set_pacingrate(sk, bw, hc->pacing_gain);
+	bbr_set_tso_segs_goal(sk);
+	bbr_set_cwnd(sk, rs, rs->acked_sacked, bbr_max_bw(sk), hc->cwnd_gain);
+	ccid5_pr_debug("sk=%p mode=%d min_rtt=%d bw=%d\n", sk, hc->mode, hc->min_rtt_us, bw);
+done:
+	/* check if incoming Acks allow pending packets to be sent */
+	if (sender_was_blocked && !ccid5_cwnd_network_limited(hc))
+		dccp_tasklet_schedule(sk);
+	dccp_ackvec_parsed_cleanup(&hc->tx_av_chunks);
+}
+
+
+static int ccid5_hc_tx_init(struct ccid *ccid, struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid_priv(ccid);
+	struct dccp_sock *dp = dccp_sk(sk);
+	u32 max_ratio;
+	ccid5_pr_debug("init ccid5 sk %p", sk);
+
+	/* RFC 4341, 5: initialise ssthresh to arbitrarily high (max) value */
+	hc->tx_ssthresh = ~0U;
+
+	/* Use larger initial windows (RFC 4341, section 5). */
+	hc->tx_cwnd = 10;
+	hc->tx_expected_wnd = hc->tx_cwnd;
+
+	/* Make sure that Ack Ratio is enabled and within bounds. */
+	max_ratio = DIV_ROUND_UP(hc->tx_cwnd, 2);
+	if (dp->dccps_l_ack_ratio == 0 || dp->dccps_l_ack_ratio > max_ratio)
+		dp->dccps_l_ack_ratio = max_ratio;
+
+	/* XXX init ~ to window size... */
+	if (ccid5_hc_tx_alloc_seq(hc))
+		return -ENOMEM;
+
+	hc->tx_rto	 = DCCP_TIMEOUT_INIT;
+	hc->tx_rpdupack  = -1;
+	hc->tx_last_cong = hc->tx_lsndtime = hc->tx_cwnd_stamp = ccid5_jiffies32;
+	hc->tx_cwnd_used = 0;
+	hc->tx_pipe = 0;
+	hc->min_rtt_us = 0;
+	hc->max_rtt_us = 0;
+
+	hc->prior_cwnd = 0;
+	hc->tso_segs_goal = 0;	 /* default segs per skb until first ACK */
+	hc->rtt_cnt = 0;
+	hc->next_rtt_delivered = 0;
+	hc->prev_ca_state = DCCP_CA_Open;
+	hc->curr_ca_state = DCCP_CA_Open;
+	hc->packet_conservation = 0;
+
+	hc->probe_rtt_done_stamp = 0;
+	hc->probe_rtt_round_done = 0;
+	hc->min_rtt_stamp = ccid5_jiffies32;
+	hc->max_rtt_stamp = ccid5_jiffies32;
+
+
+	hc->has_seen_rtt = 0;
+	hc->pr_init = 0;
+	hc->rtprop_fix=0;
+	hc->tx_extrapkt=false;
+	
+
+	hc->restore_cwnd = 0;
+	hc->restore_ackrt = 0;
+	hc->restore_seqwin = 0;
+	hc->round_start = 0;
+	hc->idle_restart = 0;
+	hc->full_bw_reached = 0;
+	hc->full_bw = 0;
+	hc->full_bw_cnt = 0;
+	hc->cycle_idx = 0;
+	bbr_reset_startup_mode(hc);
+	bbr_reset_lt_bw_sampling(hc);
+
+	timer_setup(&hc->tx_rtotimer, ccid5_hc_tx_rto_expire, 0);
+	INIT_LIST_HEAD(&hc->tx_av_chunks);
+
+	return 0;
+}
+
+static void ccid5_hc_tx_exit(struct sock *sk)
+{
+	struct ccid5_hc_tx_sock *hc = ccid5_hc_tx_sk(sk);
+	int i;
+
+	sk_stop_timer(sk, &hc->tx_rtotimer);
+
+	for (i = 0; i < hc->tx_seqbufc; i++)
+		kfree(hc->tx_seqbuf[i]);
+	hc->tx_seqbufc = 0;
+	dccp_ackvec_parsed_cleanup(&hc->tx_av_chunks);
+}
+
+static void ccid5_hc_rx_packet_recv(struct sock *sk, struct sk_buff *skb)
+{
+	struct ccid5_hc_rx_sock *hc = ccid5_hc_rx_sk(sk);
+
+	if (!dccp_data_packet(skb))
+		return;
+	if (++hc->rx_num_data_pkts >= dccp_sk(sk)->dccps_r_ack_ratio) {
+		dccp_send_ack(sk);
+		hc->rx_num_data_pkts = 0;
+	}
+}
+
+// Function to read h values and make them available for dccp
+static void ccid5_hc_tx_get_info(struct sock *sk, struct tcp_info *info)
+{
+	info->tcpi_rto = ccid5_hc_tx_sk(sk)->tx_rto;
+	info->tcpi_rtt = ccid5_hc_tx_sk(sk)->tx_srtt;
+	info->tcpi_rttvar = ccid5_hc_tx_sk(sk)->tx_mrtt;
+	info->tcpi_segs_out = ccid5_hc_tx_sk(sk)->tx_pipe;
+	info->tcpi_snd_cwnd = ccid5_hc_tx_sk(sk)->tx_cwnd;
+	info->tcpi_last_data_sent = ccid5_hc_tx_sk(sk)->tx_lsndtime;
+	info->tcpi_last_ack_recv = (ccid5_hc_tx_sk(sk)->tx_last_ack_recv > 0) ? ccid5_jiffies32 - ccid5_hc_tx_sk(sk)->tx_last_ack_recv : 0;
+	
+	info->tcpi_min_rtt = ccid5_hc_tx_sk(sk)->min_rtt_us;
+	//calculate time since tx_min_rtt_stamp was set and store it in some unused var.
+	info->tcpi_last_ack_sent = (ccid5_hc_tx_sk(sk)->min_rtt_stamp > 0) ? ccid5_jiffies32 - ccid5_hc_tx_sk(sk)->min_rtt_stamp : 0;
+	
+	info->tcpi_snd_mss = ccid5_hc_tx_sk(sk)->max_rtt_us;
+	//calculate time since tx_min_rtt_stamp was set and store it in some unused var.
+	info->tcpi_rcv_mss = (ccid5_hc_tx_sk(sk)->max_rtt_stamp > 0) ? ccid5_jiffies32 - ccid5_hc_tx_sk(sk)->max_rtt_stamp : 0;
+}
+
+struct ccid_operations ccid5_ops = {
+	.ccid_id		  = DCCPC_CCID5,
+	.ccid_name		  = "BBR-like",
+	.ccid_hc_tx_obj_size	  = sizeof(struct ccid5_hc_tx_sock),
+	.ccid_hc_tx_init	  = ccid5_hc_tx_init,
+	.ccid_hc_tx_exit	  = ccid5_hc_tx_exit,
+	.ccid_hc_tx_send_packet	  = ccid5_hc_tx_send_packet,
+	.ccid_hc_tx_packet_sent	  = ccid5_hc_tx_packet_sent,
+	.ccid_hc_tx_parse_options = ccid5_hc_tx_parse_options,
+	.ccid_hc_tx_packet_recv	  = ccid5_hc_tx_packet_recv,
+	.ccid_hc_tx_get_info	   = ccid5_hc_tx_get_info,
+	.ccid_hc_rx_obj_size	  = sizeof(struct ccid5_hc_rx_sock),
+	.ccid_hc_rx_packet_recv	  = ccid5_hc_rx_packet_recv,
+};
+
+#ifdef CONFIG_IP_DCCP_CCID5_DEBUG
+module_param(ccid5_debug, bool, 0644);
+MODULE_PARM_DESC(ccid5_debug, "Enable CCID-5 debug messages");
+#endif
diff --git a/net/dccp/ccids/ccid5.h b/net/dccp/ccids/ccid5.h
new file mode 100644
index 0000000000000..9d6d1c410b663
--- /dev/null
+++ b/net/dccp/ccids/ccid5.h
@@ -0,0 +1,250 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2019 by Nathalie Romo, Deutsche Telekom AG
+ *
+ * BBR algorithm for the DCCP protocol.
+ *
+ * The code in this file is derived from net/ipv4/tcp_bbr.c,
+ * net/ipv4/tcp_rate.c and net/dccp/ccids/ccid2.c. ccid2 Derived code is
+ * Copyright (C) the original authors Andrea Bittau and Arnaldo Carvalho
+ * 
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#ifndef _DCCP_CCID5_H_
+#define _DCCP_CCID5_H_
+
+#include <linux/timer.h>
+#include <linux/types.h>
+#include "../ccid.h"
+#include "../dccp.h"
+#include <linux/win_minmax.h>
+
+/*
+ * CCID-2 timestamping faces the same issues as TCP timestamping.
+ * Hence we reuse/share as much of the code as possible.
+ */
+//#define ccid5_time_stamp	tcp_time_stamp
+#define ccid5_jiffies32	((u32)jiffies)
+
+/* NUMDUPACK parameter from RFC 4341, p. 6 */
+#define NUMDUPACK	3
+
+struct ccid5_seq {
+	u64			ccid5s_seq;
+	u32			ccid5s_sent,
+				delivered;
+	int			ccid5s_acked;
+	bool 		is_app_limited;
+	//struct skb_mstamp	 sent_mstamp;
+	u64			sent_mstamp;
+	//struct skb_mstamp	 first_tx_mstamp;
+	u64			first_tx_mstamp;
+	//struct skb_mstamp	 delivered_mstamp;
+	u64			delivered_mstamp;
+	struct ccid5_seq	*ccid5s_prev;
+	struct ccid5_seq	*ccid5s_next;
+};
+
+#define CCID5_SEQBUF_LEN 1024
+#define CCID5_SEQBUF_MAX 128
+
+/*
+ * Multiple of congestion window to keep the sequence window at
+ * (RFC 4340 7.5.2)
+ */
+#define CCID5_WIN_CHANGE_FACTOR 5
+
+
+struct ccid5_hc_tx_sock {
+	u32			tx_cwnd;
+	u32			tx_ssthresh;
+	u32			tx_pipe;
+	u32			tx_packets_acked;
+	struct ccid5_seq	*tx_seqbuf[CCID5_SEQBUF_MAX];
+	int			tx_seqbufc;
+	struct ccid5_seq	*tx_seqh;
+	struct ccid5_seq	*tx_seqt;
+
+	/* RTT measurement: variables/principles are the same as in TCP */
+	u32			tx_srtt,
+				tx_mrtt,
+				tx_mdev,
+				tx_mdev_max,
+				tx_rttvar,
+				tx_rto,
+				tx_last_ack_recv;
+	u64			tx_rtt_seq:48;
+	struct timer_list	tx_rtotimer;
+	struct sock		*sk;
+
+	/* Congestion Window validation (optional, RFC 2861) */
+	u32			tx_cwnd_used,
+				tx_expected_wnd,
+				tx_cwnd_stamp,
+				tx_lsndtime;
+
+	u64			tx_rpseq;
+	int			tx_rpdupack;
+	u32			tx_last_cong;
+	u64			tx_high_ack;
+	struct list_head	tx_av_chunks;
+
+	u32                     rtt_us;
+
+	/* Rate sample population for BBR */
+	//struct skb_mstamp	 first_tx_mstamp;
+	u64			first_tx_mstamp;
+	//struct skb_mstamp	 delivered_mstamp;
+	u64			delivered_mstamp;
+	u32			delivered,
+				app_limited;
+
+	/* variables of BBR struct */
+	u32			min_rtt_us;
+	u32			max_rtt_us;
+	u32			min_rtt_stamp;	        /* timestamp of min_rtt_us */
+	u32			max_rtt_stamp;	        /* timestamp of min_rtt_us */
+	u32			probe_rtt_done_stamp;   /* end time for BBR_PROBE_RTT mode */
+	u32 		mode:3,		     /* current bbr_mode in state machine */
+				prev_ca_state:3,     /* CA state on previous ACK */
+				packet_conservation:1,  /* use packet conservation? */
+				restore_cwnd:1,	     /* decided to revert cwnd to old value */
+				restore_ackrt:1,     /* decided to revert ack_ratio to old value */
+				restore_seqwin:1,    /* decided to revert seq_window to old value */
+				round_start:1,	     /* start of packet-timed tx->ack round? */
+				tso_segs_goal:7,     /* segments we want in each skb we send */
+				idle_restart:1,	     /* restarting after idle? */
+				probe_rtt_round_done:1,  /* a BBR_PROBE_RTT round at 4 pkts? */
+				unused:3,
+				lt_is_sampling:1,    /* taking long-term ("LT") samples now? */
+				lt_rtt_cnt:7,	     /* round trips in long-term interval */
+				lt_use_bw:1;	     /* use lt_bw as our bw estimate? */
+	u32			lt_bw;
+	u32			lt_last_delivered;   /* LT intvl start: tp->delivered */
+	u32			lt_last_stamp;
+	u32			lt_last_lost;
+	u32			pacing_gain:10,	/* current gain for setting pacing rate */
+				cwnd_gain:10,	/* current gain for setting cwnd */
+				full_bw_reached:1,   /* reached full bw in Startup? */
+				full_bw_cnt:2,	/* number of rounds without large bw gains */
+				cycle_idx:3,	/* current index in pacing_gain cycle array */
+				has_seen_rtt:1, /* have we seen an RTT sample yet? */
+				unused_b:5;
+	u32			next_rtt_delivered;
+	//struct skb_mstamp cycle_mstamp;
+	u64			cycle_mstamp;
+	u32			rtt_cnt;
+	struct minmax bw;
+	u32			prior_cwnd;	/* prior cwnd upon entering loss recovery */
+	u32			full_bw;	/* recent bw, to estimate if pipe is full */
+	u32 		bytes_att;
+	u32 		bytes_sent;
+	u32			curr_ca_state; 
+	bool		pr_init;
+	bool		rtprop_fix;
+	u32			lost;
+	bool			tx_extrapkt;
+	u64			prior_ackrt;
+	u64			prior_seqwin;
+};
+
+struct rate_sample_ccid5 {
+	//u32	prior_mstamp;
+	//struct	skb_mstamp prior_mstamp; /* starting timestamp for interval */
+	u64  prior_mstamp;
+	u32  prior_delivered;	/* tp->delivered at "prior_mstamp" */
+	s32  delivered;		/* number of packets delivered over interval */
+	s64  interval_us;
+	//long interval_us;	/* time for tp->delivered to incr "delivered" */
+	//long rtt_us;		/* RTT of last (S)ACKed packet (or -1) */
+	u32 rtt_us;
+	int  losses;		/* number of packets marked lost upon ACK  */
+	u32  acked_sacked;	/* number of packets newly (S)ACKed upon ACK */
+	u32  prior_in_flight;	/* in flight before this ACK */
+	bool is_app_limited;	/* is sample from packet with bubble in pipe? */
+	bool is_retrans;	/* is sample from retransmission? */
+};
+
+// TODO reordering
+/**
+ * Obtain SRTT value form CCID5 TX sock.
+ */
+static inline u32 ccid5_srtt_as_delay(struct ccid5_hc_tx_sock *hc){
+	dccp_pr_debug("srtt value : %u", hc->tx_srtt);
+	if(hc){ return hc->tx_srtt;	}
+	else{ return 0; }
+}
+
+/**
+ * Obtain MRTT value form CCID2 TX sock.
+ * NOTE: value is scaled by 8 to match SRTT
+ */
+static inline u32 ccid5_mrtt_as_delay(struct ccid5_hc_tx_sock *hc){
+	dccp_pr_debug("mrtt value : %u", hc->tx_mrtt);
+	if(hc){ return (hc->tx_mrtt * 8); }
+	else{ return 0;	}
+}
+
+/* Function pointer to either get SRTT or MRTT ...*/
+//extern u32 (*get_delay_val)(struct ccid5_hc_tx_sock *hc);
+
+/**
+ * Set function pointer.
+ */
+//static inline void set_srtt_as_delay(void){
+//	get_delay_val = srtt_as_delay;
+//}
+
+/**
+ * Set function pointer.
+ */
+//static inline void set_mrtt_as_delay(void){
+//	get_delay_val = mrtt_as_delay;
+//}
+
+
+static inline bool ccid5_cwnd_network_limited(struct ccid5_hc_tx_sock *hc)
+{
+	//printk(KERN_INFO "natrm: net_lim pipe %lu cwnd %lu", hc->tx_pipe, hc->tx_cwnd);
+	return hc->tx_pipe >= hc->tx_cwnd;
+}
+
+/*
+ * Convert RFC 3390 larger initial window into an equivalent number of packets.
+ * This is based on the numbers specified in RFC 5681, 3.1.
+ */
+/*static inline u32 rfc3390_bytes_to_packets(const u32 smss)
+{
+	return smss <= 1095 ? 4 : (smss > 2190 ? 2 : 3);
+}*/
+
+/**
+ * struct ccid2_hc_rx_sock  -  Receiving end of CCID-2 half-connection
+ * @rx_num_data_pkts: number of data packets received since last feedback
+ */
+struct ccid5_hc_rx_sock {
+	u32	rx_num_data_pkts;
+};
+
+static inline struct ccid5_hc_tx_sock *ccid5_hc_tx_sk(const struct sock *sk)
+{
+	return ccid_priv(dccp_sk(sk)->dccps_hc_tx_ccid);
+}
+
+static inline struct ccid5_hc_rx_sock *ccid5_hc_rx_sk(const struct sock *sk)
+{
+	return ccid_priv(dccp_sk(sk)->dccps_hc_rx_ccid);
+}
+#endif /* _DCCP_CCID5_H_ */
diff --git a/net/dccp/ccids/ccid6.c b/net/dccp/ccids/ccid6.c
new file mode 100644
index 0000000000000..4d25f9e96201f
--- /dev/null
+++ b/net/dccp/ccids/ccid6.c
@@ -0,0 +1,2519 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ *  Copyright (C) 2022 by Alexander Rabitsch, Karlstad University for Deutsche Telekom AG
+ *
+ *  This code is a version of the BBR algorithm for the DCCP protocol.
+ *	Due to that, it copies and adapts as much code as possible from 
+ *	net/ipv4/tcp_bbr.c, net/ipv4/tcp_rate.c, net/dccp/ccids/ccid5.c, 
+ *	and net/dccp/ccids/ccid2.c
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+/*
+ * This implementation should follow RFC 4341
+ */
+#include <linux/slab.h>
+#include "../feat.h"
+#include "ccid6.h"
+
+struct bbr_context {
+    u32 sample_bw;
+    u32 target_cwnd;
+    u32 log:1;
+};
+
+enum bbr_mode {
+    BBR_STARTUP,
+    BBR_DRAIN,
+    BBR_PROBE_BW,
+    BBR_PROBE_RTT,
+};
+
+enum bbr_ack_phase {
+    BBR_ACKS_INIT,
+    BBR_ACKS_REFILLING,
+    BBR_ACKS_PROBE_STARTING,
+    BBR_ACKS_PROBE_FEEDBACK,
+    BBR_ACKS_PROBE_STOPPING,
+};
+
+enum tcp_bbr2_phase {
+    BBR2_PHASE_STARTUP,
+    BBR2_PHASE_DRAIN,
+    BBR2_PHASE_PROBE_RTT,
+    BBR2_PHASE_INVALID,
+    BBR2_PHASE_PROBE_BW_UP,
+    BBR2_PHASE_PROBE_BW_DOWN,
+    BBR2_PHASE_PROBE_BW_CRUISE,
+    BBR2_PHASE_PROBE_BW_REFILL,
+};
+
+enum dccp_ca_state {
+    DCCP_CA_Open = 0,
+    DCCP_CA_Disorder = 1,
+    DCCP_CA_CWR = 2,
+    DCCP_CA_Recovery = 3,
+    DCCP_CA_Loss = 4
+};
+
+/* Scale factor for rate in pkt/uSec unit to avoid truncation in bandwidth
+ * estimation. The rate unit ~= (1500 bytes / 1 usec / 2^24) ~= 715 bps.
+ * This handles bandwidths from 0.06pps (715bps) to 256Mpps (3Tbps) in a u32.
+ * Since the minimum window is >=4 packets, the lower bound isn't
+ * an issue. The upper bound isn't an issue with existing technologies.
+ */
+#define BW_SCALE 24
+#define BW_UNIT (1 << BW_SCALE)
+
+#define BBR_SCALE 8	/* scaling factor for fractions in BBR (e.g. gains) */
+#define BBR_UNIT (1 << BBR_SCALE)
+
+#define FLAG_DEBUG_VERBOSE	0x1	/* Verbose debugging messages */
+#define FLAG_DEBUG_LOOPBACK	0x2	/* Do NOT skip loopback addr */
+
+#define CYCLE_LEN	8 /* number of phases in a pacing gain cycle */
+
+#define FLAG_ECE 0x40
+
+// ???
+/* Window length of min_rtt filter (in sec). Max allowed value is 31 (0x1F) */
+static u32 bbr_min_rtt_win_sec = 10;
+/* Minimum time (in ms) spent at bbr_cwnd_min_target in BBR_PROBE_RTT mode.
+ * Max allowed value is 511 (0x1FF).
+ */
+static u32 bbr_probe_rtt_mode_ms = 200;
+/* Window length of probe_rtt_min_us filter (in ms), and consequently the
+ * typical interval between PROBE_RTT mode entries.
+ * Note that bbr_probe_rtt_win_ms must be <= bbr_min_rtt_win_sec * MSEC_PER_SEC
+ */
+static u32 bbr_probe_rtt_win_ms = 5000;
+/* Skip TSO below the following bandwidth (bits/sec): */
+static int bbr_min_tso_rate = 1200000;
+/* Use min_rtt to help adapt TSO burst size, with smaller min_rtt resulting
+ * in bigger TSO bursts. By default we cut the RTT-based allowance in half
+ * for every 2^9 usec (aka 512 us) of RTT, so that the RTT-based allowance
+ * is below 1500 bytes after 6 * ~500 usec = 3ms.
+ */
+static u32 bbr_tso_rtt_shift = 9;
+/* Select cwnd TSO budget approach:
+ *  0: padding
+ *  1: flooring
+ */
+static uint bbr_cwnd_tso_budget = 0; // alerab
+/* Pace at ~1% below estimated bw, on average, to reduce queue at bottleneck.
+ * In order to help drive the network toward lower queues and low latency while
+ * maintaining high utilization, the average pacing rate aims to be slightly
+ * lower than the estimated bandwidth. This is an important aspect of the
+ * design.
+ */
+static const int bbr_pacing_margin_percent = 1;
+/* We use a high_gain value of 2/ln(2) because it's the smallest pacing gain
+ * that will allow a smoothly increasing pacing rate that will double each RTT
+ * and send the same number of packets per RTT that an un-paced, slow-starting
+ * Reno or CUBIC flow would. Max allowed value is 2047 (0x7FF).
+ */
+static int bbr_high_gain  = BBR_UNIT * 2885 / 1000 + 1;
+/* The gain for deriving startup cwnd. Max allowed value is 2047 (0x7FF). */
+static int bbr_startup_cwnd_gain  = BBR_UNIT * 2885 / 1000 + 1;
+/* The pacing gain of 1/high_gain in BBR_DRAIN is calculated to typically drain
+ * the queue created in BBR_STARTUP in a single round. Max allowed value
+ * is 1023 (0x3FF).
+ */
+static int bbr_drain_gain = BBR_UNIT * 1000 / 2885;
+/* The gain for deriving steady-state cwnd tolerates delayed/stretched ACKs.
+ * Max allowed value is 2047 (0x7FF).
+ */
+static int bbr_cwnd_gain  = BBR_UNIT * 2;
+
+/* The pacing_gain values for the PROBE_BW gain cycle, to discover/share bw.
+ * Max allowed value for each element is 1023 (0x3FF).
+ */
+enum bbr_pacing_gain_phase {
+    BBR_BW_PROBE_UP		= 0,	/* push up inflight to probe for bw/vol */
+    BBR_BW_PROBE_DOWN	= 1, 	/* drain excess inflight from the queue */
+    BBR_BW_PROBE_CRUISE	= 2, 	/* use pipe, w/ headroom in queue/pipe */
+    BBR_BW_PROBE_REFILL	= 3, 	/* v2: refill the pipe again to 100% */
+};
+static int bbr_pacing_gain[] = {
+    BBR_UNIT * 5 / 4,	/* probe for more available bw */
+    BBR_UNIT * 3 / 4,	/* drain queue and/or yield bw to other flows */
+    BBR_UNIT, BBR_UNIT, BBR_UNIT,	/* cruise at 1.0*bw to utilize pipe, */
+    BBR_UNIT, BBR_UNIT, BBR_UNIT	/* without creating excess queue... */
+};
+
+/* Try to keep at least this many packets in flight, if things go smoothly. For
+ * smooth functioning, a sliding window protocol ACKing every other packet
+ * needs at least 4 packets in flight. Max allowed value is 15 (0xF).
+ */
+static u32 bbr_cwnd_min_target = 4;
+/* Cwnd to BDP proportion in PROBE_RTT mode scaled by BBR_UNIT. Default: 50%.
+ * Use 0 to disable. Max allowed value is 255.
+ */
+static u32 bbr_probe_rtt_cwnd_gain = BBR_UNIT * 1 / 2;
+/* To estimate if BBR_STARTUP mode (i.e. high_gain) has filled pipe... */
+/* If bw has increased significantly (1.25x), there may be more bw available.
+ * Max allowed value is 1023 (0x3FF).
+ */
+static u32 bbr_full_bw_thresh = BBR_UNIT * 5 / 4;
+/* But after 3 rounds w/o significant bw growth, estimate pipe is full.
+ * Max allowed value is 7 (0x7).
+ */
+static u32 bbr_full_bw_cnt = 3;
+
+//static u32 bbr_flags;	/* Debugging related stuff */
+//static bool bbr_debug_with_printk;
+//static bool bbr_debug_ftrace;
+
+/* Experiment: each cycle, try to hold sub-unity gain until inflight <= BDP. */
+static bool bbr_drain_to_target = false; // alerab
+/* Experiment: Flags to control BBR with ECN behavior.
+ */
+static bool bbr_precise_ece_ack = true;
+/* The max rwin scaling shift factor is 14 (RFC 1323), so the max sane rwin is
+ * (2^(16+14) B)/(1024 B/packet) = 1M packets.
+ */
+static u32 bbr_cwnd_warn_val	= 1U << 20;
+//static u16 bbr_debug_port_mask;
+
+
+/* BBR module parameters. These are module parameters only in Google prod.
+ * Upstream these are intentionally not module parameters.
+ */
+//static int bbr_pacing_gain_size = CYCLE_LEN;
+/* Gain factor for adding extra_acked to target cwnd: */
+static int bbr_extra_acked_gain = 256;
+/* Window length of extra_acked window. Max allowed val is 31. */
+static u32 bbr_extra_acked_win_rtts = 5;
+/* Max allowed val for ack_epoch_acked, after which sampling epoch is reset */
+static int bbr_extra_acked_in_startup = 1;
+/* Time period for clamping cwnd increment due to ack aggregation */
+static bool bbr_usage_based_cwnd = false;
+/* For lab testing, researchers can enable BBRv2 ECN support with this flag,
+ * when they know that any ECN marks that the connections experience will be
+ * DCTCP/L4S-style ECN marks, rather than RFC3168 ECN marks.
+ * TODO(ncardwell): Production use of the BBRv2 ECN functionality depends on
+ * negotiation or configuration that is outside the scope of the BBRv2
+ * alpha release.
+ */
+static bool bbr_ecn_enable = false;
+
+/* These are module parameters in bbrv2 */
+static u32 bbr_extra_acked_max_us = 100 * 1000;
+static u32 bbr_ack_epoch_acked_reset_thresh = 1U << 20;
+static u32 bbr_beta = BBR_UNIT * 30 / 100;
+static u32 bbr_ecn_alpha_gain = BBR_UNIT * 1 / 16; 
+static u32 bbr_ecn_alpha_init = BBR_UNIT;	
+static u32 bbr_ecn_factor = BBR_UNIT * 1 / 3;	
+static u32 bbr_ecn_thresh = BBR_UNIT * 1 / 2; 
+static u32 bbr_ecn_max_rtt_us = 5000;
+static u32 bbr_ecn_reprobe_gain;
+static u32 bbr_loss_thresh = BBR_UNIT * 2 / 100; 
+static u32 bbr_full_loss_cnt = 8;
+static u32 bbr_full_ecn_cnt = 2;
+static u32 bbr_inflight_headroom = BBR_UNIT * 15 / 100;
+static u32 bbr_bw_probe_pif_gain = BBR_UNIT * 5 / 4;
+static u32 bbr_bw_probe_reno_gain = BBR_UNIT;
+static u32 bbr_bw_probe_max_rounds = 63;
+static u32 bbr_bw_probe_rand_rounds = 2;
+static u32 bbr_bw_probe_base_us = 2 * USEC_PER_SEC; 
+static u32 bbr_bw_probe_rand_us = 1 * USEC_PER_SEC; 
+static bool bbr_undo = true;
+static bool bbr_fast_path = true;	
+//static int bbr_fast_ack_mode = 1;
+static u32 bbr_refill_add_inc;		
+
+static void bbr2_exit_probe_rtt (struct sock* sk);
+static void bbr2_reset_congestion_signals (struct ccid6_hc_tx_sock* hc);
+static void bbr_check_probe_rtt_done (struct sock* sk);
+static void bbr_cwnd_event(struct sock* sk, enum tcp_ca_event event);
+
+#ifdef CONFIG_IP_DCCP_CCID6_DEBUG
+static bool ccid6_debug;
+#define ccid6_pr_debug(format, a...)	DCCP_PR_DEBUG(ccid6_debug, format, ##a)
+#else
+#define ccid6_pr_debug(format, a...)
+#endif
+
+static int ccid6_hc_tx_alloc_seq(struct ccid6_hc_tx_sock *hc)
+{
+	//printk(KERN_INFO "natrm: enter ccid6_hc_tx_alloc_seq");
+	struct ccid6_seq *seqp;
+	int i;
+
+	/* check if we have space to preserve the pointer to the buffer */
+	if (hc->tx_seqbufc >= (sizeof(hc->tx_seqbuf) /
+				   sizeof(struct ccid6_seq *)))
+		return -ENOMEM;
+
+	/* allocate buffer and initialize linked list */
+	seqp = kmalloc(CCID6_SEQBUF_LEN * sizeof(struct ccid6_seq), gfp_any());
+	if (seqp == NULL)
+		return -ENOMEM;
+
+	for (i = 0; i < (CCID6_SEQBUF_LEN - 1); i++) {
+		seqp[i].ccid6s_next = &seqp[i + 1];
+		seqp[i + 1].ccid6s_prev = &seqp[i];
+	}
+	seqp[CCID6_SEQBUF_LEN - 1].ccid6s_next = seqp;
+	seqp->ccid6s_prev = &seqp[CCID6_SEQBUF_LEN - 1];
+
+	/* This is the first allocation.  Initiate the head and tail.  */
+	if (hc->tx_seqbufc == 0)
+		hc->tx_seqh = hc->tx_seqt = seqp;
+	else {
+		/* link the existing list with the one we just created */
+		hc->tx_seqh->ccid6s_next = seqp;
+		seqp->ccid6s_prev = hc->tx_seqh;
+
+		hc->tx_seqt->ccid6s_prev = &seqp[CCID6_SEQBUF_LEN - 1];
+		seqp[CCID6_SEQBUF_LEN - 1].ccid6s_next = hc->tx_seqt;
+	}
+
+	/* store the original pointer to the buffer so we can free it */
+	hc->tx_seqbuf[hc->tx_seqbufc] = seqp;
+	hc->tx_seqbufc++;
+
+	return 0;
+}
+
+
+static void ccid6_change_l_ack_ratio(struct sock *sk, u32 val)
+{
+	u32 max_ratio = DIV_ROUND_UP(ccid6_hc_tx_sk(sk)->tx_cwnd, 2);
+
+	/*
+	 * Ensure that Ack Ratio does not exceed ceil(cwnd/2), which is (2) from
+	 * RFC 4341, 6.1.2. We ignore the statement that Ack Ratio 2 is always
+	 * acceptable since this causes starvation/deadlock whenever cwnd < 2.
+	 * The same problem arises when Ack Ratio is 0 (ie. Ack Ratio disabled).
+	 */
+	if (val == 0 || val > max_ratio) {
+		DCCP_WARN("Limiting Ack Ratio (%u) to %u\n", val, max_ratio);
+		val = max_ratio;
+	}
+	//printk(KERN_INFO "natrm: ccid6 change ack_ratio %lu max %lu", val, max_ratio);
+	dccp_feat_signal_nn_change(sk, DCCPF_ACK_RATIO,
+				   min_t(u32, val, DCCPF_ACK_RATIO_MAX));
+}
+
+static void ccid6_check_l_ack_ratio(struct sock *sk)
+{
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	/*
+	 * After a loss, idle period, application limited period, or RTO we
+	 * need to check that the ack ratio is still less than the congestion
+	 * window. Otherwise, we will send an entire congestion window of
+	 * packets and got no response because we haven't sent ack ratio
+	 * packets yet.
+	 * If the ack ratio does need to be reduced, we reduce it to half of
+	 * the congestion window (or 1 if that's zero) instead of to the
+	 * congestion window. This prevents problems if one ack is lost.
+	 */
+
+	if (dccp_feat_nn_get(sk, DCCPF_ACK_RATIO) > hc->tx_cwnd)
+		ccid6_change_l_ack_ratio(sk, hc->tx_cwnd/2 ? : 1U);
+}
+
+static void ccid6_change_l_seq_window(struct sock *sk, u64 val)
+{
+	dccp_feat_signal_nn_change(sk, DCCPF_SEQUENCE_WINDOW,
+				   clamp_val(val, DCCPF_SEQ_WMIN,
+						  DCCPF_SEQ_WMAX));
+}
+static void dccp_tasklet_schedule(struct sock *sk)
+{
+	struct tasklet_struct *t = &dccp_sk(sk)->dccps_xmitlet;
+
+	if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
+		sock_hold(sk);
+		__tasklet_schedule(t);
+	}
+}
+
+/*
+ *	Congestion window validation (RFC 2861).
+ */
+
+static bool ccid6_do_cwv = true;
+module_param(ccid6_do_cwv, bool, 0644);
+MODULE_PARM_DESC(ccid6_do_cwv, "Perform RFC2861 Congestion Window Validation");
+
+/**
+ * ccid2_update_used_window  -  Track how much of cwnd is actually used
+ * This is done in addition to CWV. The sender needs to have an idea of how many
+ * packets may be in flight, to set the local Sequence Window value accordingly
+ * (RFC 4340, 7.5.2). The CWV mechanism is exploited to keep track of the
+ * maximum-used window. We use an EWMA low-pass filter to filter out noise.
+ */
+static void ccid6_update_used_window(struct ccid6_hc_tx_sock *hc, u32 new_wnd)
+{
+	hc->tx_expected_wnd = (3 * hc->tx_expected_wnd + new_wnd) / 4;
+}
+
+/* This borrows the code of tcp_cwnd_application_limited() */
+static void ccid6_cwnd_application_limited(struct sock *sk, const u32 now)
+{
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	/* don't reduce cwnd below the initial window (IW) */
+	u32 init_win = ccid6_rfc3390_bytes_to_pkts(dccp_sk(sk)->dccps_mss_cache),
+		win_used = max(hc->tx_cwnd_used, init_win);
+
+	if (win_used < hc->tx_cwnd) {
+		hc->tx_ssthresh = max(hc->tx_ssthresh,
+					 (hc->tx_cwnd >> 1) + (hc->tx_cwnd >> 2));
+		//hc->tx_cwnd = (hc->tx_cwnd + win_used) >> 1;
+		//ccid6_pr_debug("%s: tx_cwnd set to %d for sk %p", __func__, hc->tx_cwnd, sk);
+	}
+	hc->tx_cwnd_used  = 0;
+	hc->tx_cwnd_stamp = now;
+
+	ccid6_check_l_ack_ratio(sk);
+}
+
+/* This borrows the code of tcp_cwnd_restart() */
+static void ccid6_cwnd_restart(struct sock *sk, const u32 now)
+{
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	u32 cwnd = hc->tx_cwnd, restart_cwnd,
+		iwnd = ccid6_rfc3390_bytes_to_pkts(dccp_sk(sk)->dccps_mss_cache);
+
+	hc->tx_ssthresh = max(hc->tx_ssthresh, (cwnd >> 1) + (cwnd >> 2));
+
+	/* don't reduce cwnd below the initial window (IW) */
+	restart_cwnd = min(cwnd, iwnd);
+	cwnd >>= (now - hc->tx_lsndtime) / hc->tx_rto;
+	//hc->tx_cwnd = max(cwnd, restart_cwnd);
+
+	hc->tx_cwnd_stamp = now;
+	hc->tx_cwnd_used  = 0;
+
+	ccid6_check_l_ack_ratio(sk);
+}
+
+static void ccid6_hc_tx_packet_sent(struct sock *sk, unsigned int len)
+{
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	const u32 now = ccid6_jiffies32;
+	struct ccid6_seq *next;
+
+	//u64 prior_wstamp;
+	hc->bytes_sent += len;
+	if (hc->curr_ca_state == DCCP_CA_Loss)
+		hc->curr_ca_state = DCCP_CA_Open;
+
+	/* slow-start after idle periods (RFC 2581, RFC 2861) */
+	if (ccid6_do_cwv && !hc->tx_pipe && (s32)(now - hc->tx_lsndtime) >= hc->tx_rto) {
+		// CWND EVENT: TX_START, i.e. return from idle
+		bbr_cwnd_event(sk, CA_EVENT_TX_START); //<------------------ 
+
+		ccid6_cwnd_restart(sk, now);
+		// This is returning from idle: 
+	}
+
+	hc->tx_lsndtime = now;
+
+	hc->tx_seqh->sent_mstamp = tcp_clock_us();
+	if (!hc->tx_pipe) {
+		hc->first_tx_mstamp  = hc->tx_seqh->sent_mstamp;
+		hc->delivered_mstamp = hc->first_tx_mstamp;
+	}
+
+	hc->tx_seqh->delivered_ce = hc->delivered_ce;
+	hc->tx_seqh->delivered    = hc->delivered;
+	hc->tx_seqh->ccid6s_seq   = dp->dccps_gss;
+	hc->tx_seqh->ccid6s_acked = 0;
+	hc->tx_seqh->ccid6s_sent  = now;
+	hc->tx_seqh->first_tx_mstamp   = hc->first_tx_mstamp;
+	hc->tx_seqh->delivered_mstamp  = hc->delivered_mstamp;
+	hc->tx_seqh->is_app_limited = hc->app_limited ? 1 : 0; // what? this is what tcp does, but I dont get it
+
+	hc->tx_seqh->lost = hc->lost; // <---------
+
+	next = hc->tx_seqh->ccid6s_next;
+	/* check if we need to alloc more space */
+	if (next == hc->tx_seqt) {
+		if (ccid6_hc_tx_alloc_seq(hc)) {
+			DCCP_CRIT("packet history - out of memory!");
+			/* FIXME: find a more graceful way to bail out */
+			return;
+		}
+		next = hc->tx_seqh->ccid6s_next;
+		BUG_ON(next == hc->tx_seqt);
+	}
+	hc->tx_seqh = next;
+
+	hc->tx_pipe  += 1;
+
+	/* see whether cwnd was fully used (RFC 2861), update expected window */
+	if (ccid6_cwnd_network_limited(hc)) {
+		ccid6_update_used_window(hc, hc->tx_cwnd);
+		hc->tx_cwnd_used  = 0;
+		hc->tx_cwnd_stamp = now;
+	} else {
+		if (hc->tx_pipe > hc->tx_cwnd_used)
+			hc->tx_cwnd_used = hc->tx_pipe;
+
+		ccid6_update_used_window(hc, hc->tx_cwnd_used);
+
+		if (ccid6_do_cwv && (s32)(now - hc->tx_cwnd_stamp) >= hc->tx_rto)
+			ccid6_cwnd_application_limited(sk, now);
+	}
+
+	//ccid6_pr_debug("sk=%p cwnd=%d pipe=%d\n", sk, hc->tx_cwnd, hc->tx_pipe);
+
+	/*
+	 * FIXME: The code below is broken and the variables have been removed
+	 * from the socket struct. The `ackloss' variable was always set to 0,
+	 * and with arsent there are several problems:
+	 *  (i) it doesn't just count the number of Acks, but all sent packets;
+	 *  (ii) it is expressed in # of packets, not # of windows, so the
+	 *  comparison below uses the wrong formula: Appendix A of RFC 4341
+	 *  comes up with the number K = cwnd / (R^2 - R) of consecutive windows
+	 *  of data with no lost or marked Ack packets. If arsent were the # of
+	 *  consecutive Acks received without loss, then Ack Ratio needs to be
+	 *  decreased by 1 when
+	 *	      arsent >=  K * cwnd / R  =  cwnd^2 / (R^3 - R^2)
+	 *  where cwnd / R is the number of Acks received per window of data
+	 *  (cf. RFC 4341, App. A). The problems are that
+	 *  - arsent counts other packets as well;
+	 *  - the comparison uses a formula different from RFC 4341;
+	 *  - computing a cubic/quadratic equation each time is too complicated.
+	 *  Hence a different algorithm is needed.
+	 */
+#if 0
+	/* Ack Ratio.  Need to maintain a concept of how many windows we sent */
+	hc->tx_arsent++;
+	/* We had an ack loss in this window... */
+	if (hc->tx_ackloss) {
+		if (hc->tx_arsent >= hc->tx_cwnd) {
+			hc->tx_arsent  = 0;
+			hc->tx_ackloss = 0;
+		}
+	} else {
+		/* No acks lost up to now... */
+		/* decrease ack ratio if enough packets were sent */
+		if (dp->dccps_l_ack_ratio > 1) {
+			/* XXX don't calculate denominator each time */
+			int denom = dp->dccps_l_ack_ratio * dp->dccps_l_ack_ratio -
+					dp->dccps_l_ack_ratio;
+
+			denom = hc->tx_cwnd * hc->tx_cwnd / denom;
+
+			if (hc->tx_arsent >= denom) {
+				ccid2_change_l_ack_ratio(sk, dp->dccps_l_ack_ratio - 1);
+				hc->tx_arsent = 0;
+			}
+		} else {
+			/* we can't increase ack ratio further [1] */
+			hc->tx_arsent = 0; /* or maybe set it to cwnd*/
+		}
+	}
+#endif
+
+	sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
+
+}
+
+/**
+ * uses the same code from ccid2_rtt_estimator
+ */
+static void ccid6_rtt_estimator(struct sock *sk, const long mrtt)
+{
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	long m = mrtt ? : 1;
+
+	hc->tx_mrtt = mrtt;
+
+	if (hc->tx_srtt == 0) {
+		/* First measurement m */
+		hc->tx_srtt = m << 3;
+		hc->tx_mdev = m << 1;
+
+		hc->tx_mdev_max = max(hc->tx_mdev, tcp_rto_min(sk));
+		hc->tx_rttvar   = hc->tx_mdev_max;
+
+		hc->tx_rtt_seq  = dccp_sk(sk)->dccps_gss;
+	} else {
+		/* Update scaled SRTT as SRTT += 1/8 * (m - SRTT) */
+		m -= (hc->tx_srtt >> 3);
+		hc->tx_srtt += m;
+
+		/* Similarly, update scaled mdev with regard to |m| */
+		if (m < 0) {
+			m = -m;
+			m -= (hc->tx_mdev >> 2);
+			/*
+			 * This neutralises RTO increase when RTT < SRTT - mdev
+			 * (see P. Sarolahti, A. Kuznetsov,"Congestion Control
+			 * in Linux TCP", USENIX 2002, pp. 49-62).
+			 */
+			if (m > 0)
+				m >>= 3;
+		} else {
+			m -= (hc->tx_mdev >> 2);
+		}
+		hc->tx_mdev += m;
+
+		if (hc->tx_mdev > hc->tx_mdev_max) {
+			hc->tx_mdev_max = hc->tx_mdev;
+			if (hc->tx_mdev_max > hc->tx_rttvar)
+				hc->tx_rttvar = hc->tx_mdev_max;
+		}
+
+		/*
+		 * Decay RTTVAR at most once per flight, exploiting that
+		 *  1) pipe <= cwnd <= Sequence_Window = W  (RFC 4340, 7.5.2)
+		 *  2) AWL = GSS-W+1 <= GAR <= GSS          (RFC 4340, 7.5.1)
+		 * GAR is a useful bound for FlightSize = pipe.
+		 * AWL is probably too low here, as it over-estimates pipe.
+		 */
+		if (after48(dccp_sk(sk)->dccps_gar, hc->tx_rtt_seq)) {
+			if (hc->tx_mdev_max < hc->tx_rttvar)
+				hc->tx_rttvar -= (hc->tx_rttvar -
+						  hc->tx_mdev_max) >> 2;
+			hc->tx_rtt_seq  = dccp_sk(sk)->dccps_gss;
+			hc->tx_mdev_max = tcp_rto_min(sk);
+		}
+	}
+
+	/*
+	 * Set RTO from SRTT and RTTVAR
+	 * As in TCP, 4 * RTTVAR >= TCP_RTO_MIN, giving a minimum RTO of 200 ms.
+	 * This agrees with RFC 4341, 5:
+	 *	"Because DCCP does not retransmit data, DCCP does not require
+	 *	 TCP's recommended minimum timeout of one second".
+	 */
+	hc->tx_rto = (hc->tx_srtt >> 3) + hc->tx_rttvar;
+
+	if (hc->tx_rto > DCCP_RTO_MAX)
+		hc->tx_rto = DCCP_RTO_MAX;
+}
+
+/************************************************************/
+/* BELLOW THE FUNCTIONS WHICH IN TCP ARE PART OF tcp_rate.c */
+/************************************************************/
+
+void ccid6_rate_skb_delivered(struct sock *sk, struct ccid6_seq *acked,
+				struct rate_sample_ccid6 *rs)
+{
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (!acked->delivered_mstamp)
+		return;
+
+	if (!rs->prior_delivered || after(acked->delivered, rs->prior_delivered)) {
+		rs->prior_delivered_ce = acked->delivered_ce;
+		rs->prior_delivered  = acked->delivered;
+		rs->prior_mstamp     = acked->delivered_mstamp;
+		rs->is_app_limited   = acked->is_app_limited;
+
+		/* Find the duration of the "send phase" of this window: */
+		rs->interval_us      = tcp_stamp_us_delta(
+						acked->sent_mstamp,
+						acked->first_tx_mstamp);
+		/* Record send time of most recently ACKed packet: */
+		hc->first_tx_mstamp  = acked->sent_mstamp;
+	}
+}
+
+void ccid6_rate_gen(struct sock *sk, u32 delivered, u32 lost, u64 now, struct rate_sample_ccid6 *rs)
+{
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	s64 ack_us;
+	s64 snd_us;	
+	
+	/* Clear app limited if bubble is acked and gone. */
+	if (hc->app_limited && after(hc->delivered, hc->app_limited))
+		hc->app_limited = 0;
+
+	if (delivered) 
+		hc->delivered_mstamp = now; 
+	
+	rs->acked_sacked = delivered;	/* freshly ACKed or SACKed */
+	rs->losses = lost;		/* freshly marked lost */
+
+	if (!rs->prior_mstamp) {
+		rs->delivered = -1;
+		rs->interval_us = -1;
+		return;
+	}
+
+	rs->delivered    = hc->delivered - rs->prior_delivered;
+	rs->delivered_ce = hc->delivered_ce - rs->prior_delivered_ce;
+	/* delivered_ce occupies less than 32 bits in the skb control block */
+	//rs->delivered_ce &= TCPCB_DELIVERED_CE_MASK; // ?
+
+	//// takes maximum between send_us and ack_us
+	snd_us = rs->interval_us;				/* send phase */
+	ack_us = tcp_stamp_us_delta(now, rs->prior_mstamp);
+	rs->interval_us = max(snd_us, ack_us);
+}
+
+void ccid6_rate_check_app_limited(struct sock *sk, int tsize)
+{
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (hc->bytes_att - hc->bytes_sent < dp->dccps_mss_cache &&
+			sk_wmem_alloc_get(sk) < tsize && hc->tx_pipe < hc->tx_cwnd) {
+		hc->app_limited = (hc->delivered + hc->tx_pipe) ? : 1;
+	}
+
+}
+
+/*****************************************************/
+/*       FUNCTIONS PART OF tcp_rate.c END HERE       */
+/*****************************************************/
+
+
+/*****************************************************/
+/*       FUNCTIONS PART OF tcp_bbrv2.c START HERE       */
+/*****************************************************/
+
+static bool bbr_full_bw_reached (const struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	return hc->full_bw_reached;
+}
+
+static u32 bbr_max_bw (const struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	return max (hc->bw_hi[0], hc->bw_hi[1]);
+} 
+
+static u32 bbr_bw (const struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	return min (bbr_max_bw (sk), hc->bw_lo);
+} 
+
+static u16 bbr_extra_acked (const struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	return max (hc->extra_acked[0], hc->extra_acked[1]);
+}
+
+static u64 bbr_rate_bytes_per_sec (struct sock* sk, u64 rate, int gain, int margin) {
+	struct inet_connection_sock *icsk = inet_csk(sk);
+	rate *= icsk->icsk_pmtu_cookie;
+	rate *= gain;
+	rate >>= BBR_SCALE;
+	rate *= USEC_PER_SEC / 100 * (100 - margin);
+	rate >>= BW_SCALE;
+	rate = max (rate, 1ULL);
+	return rate;
+}
+
+static u64 bbr_bw_bytes_per_sec (struct sock* sk, u64 rate) {
+	return bbr_rate_bytes_per_sec (sk, rate, BBR_UNIT, 0);
+}
+
+// Enable for debugging
+static u64 bbr_rate_kbps (struct sock* sk, u64 rate) {
+	rate = bbr_bw_bytes_per_sec (sk, rate);
+	rate *= 8;
+	do_div (rate, 1000);
+	return rate;
+}
+
+static unsigned long bbr_bw_to_pacing_rate (struct sock* sk, u32 bw, int gain) {
+	u64 rate = bw;
+	rate = bbr_rate_bytes_per_sec (sk, rate, gain, bbr_pacing_margin_percent);
+	rate = min_t (u64, rate, sk->sk_max_pacing_rate);
+	return rate;
+} 
+
+static void bbr_init_pacing_rate_from_rtt (struct sock* sk, struct ccid6_hc_tx_sock *hc) {
+	u64 bw;
+	u32 rtt_us;
+	if (hc->tx_srtt) {
+		rtt_us = max (hc->tx_srtt >> 3, 1U);
+		hc->has_seen_rtt = 1;
+	} else {
+		rtt_us = USEC_PER_MSEC;
+	}
+	bw = (u64)hc->tx_cwnd * BW_UNIT;
+	do_div (bw, rtt_us);
+	sk->sk_pacing_rate = bbr_bw_to_pacing_rate (sk, bw, hc->params.high_gain);
+	hc->pr_init = 1;
+}
+
+static void bbr_set_pacing_rate (struct sock* sk, u32 bw, int gain) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 rate = bbr_bw_to_pacing_rate(sk, bw, gain);
+
+	if (unlikely(!hc->has_seen_rtt && hc->tx_srtt))
+		bbr_init_pacing_rate_from_rtt(sk, hc);
+	if (bbr_full_bw_reached(sk) || rate > sk->sk_pacing_rate)
+		sk->sk_pacing_rate = rate;
+} 
+
+/*static u32 bbr_min_tso_segs (struct sock* sk) {
+	return sk->sk_pacing_rate < (bbr_min_tso_rate >> 3) ? 1 : 2;
+}*/
+
+/*static u32 bbr_tso_segs_generic (struct sock* sk, unsigned int mss_now, u32 gso_max_size) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 segs, r;
+	u64 bytes = sk->sk_pacing_rate >> sk->sk_pacing_shift;
+	if (hc->params.tso_rtt_shift) {
+		r = hc->min_rtt_us >> hc->params.tso_rtt_shift;
+		if (r < BITS_PER_TYPE(u32))  
+			bytes += GSO_MAX_SIZE >> r;
+	}
+	bytes = min_t (u32, bytes, gso_max_size - 1 - MAX_TCP_HEADER);
+	segs = max_t (u32, bytes / mss_now, bbr_min_tso_segs (sk));
+	return segs;
+}
+
+static u32  bbr_tso_segs (struct sock* sk, unsigned int mss_now) {
+	return bbr_tso_segs_generic (sk, mss_now, sk->sk_gso_max_size);
+}*/
+
+u32 ccid6_tso_autosize(const struct sock *sk, unsigned int mss_now, int min_tso_segs)
+{
+	u32 bytes, segs;
+
+	bytes = min(sk->sk_pacing_rate >> 10, (u32)(sk->sk_gso_max_size - 1 - MAX_DCCP_HEADER));
+	segs = max_t(u32, bytes / mss_now, min_tso_segs);
+
+	return segs;
+}
+
+static u32 bbr_tso_segs_goal (struct sock* sk) {
+	struct dccp_sock *dp = dccp_sk(sk);
+	u32 min_segs;
+
+	min_segs = sk->sk_pacing_rate < (bbr_min_tso_rate >> 3) ? 1 : 2;
+	return min(ccid6_tso_autosize(sk, dp->dccps_mss_cache, min_segs), 0x7FU);
+}
+
+static void bbr_save_cwnd (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	struct dccp_sock *dp = dccp_sk(sk);
+
+	if (hc->prev_ca_state < DCCP_CA_Recovery && hc->mode != BBR_PROBE_RTT)
+		hc->prior_cwnd = hc->tx_cwnd;  /* this cwnd is good enough */
+	else  /* loss recovery or BBR_PROBE_RTT have temporarily cut cwnd */
+		hc->prior_cwnd = max(hc->prior_cwnd, hc->tx_cwnd);
+
+	/* From ccid6: Save ack_ratio and seq_window as well (keep?) */
+	hc->prior_ackrt = dp->dccps_l_ack_ratio;
+	hc->prior_seqwin = dp->dccps_l_seq_win;
+
+} 
+
+static void bbr_cwnd_event (struct sock* sk, enum tcp_ca_event event) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	if (event == CA_EVENT_TX_START && hc->app_limited) {
+			hc->idle_restart = 1;
+			hc->ack_epoch_mstamp = dccp_sk(sk)->dccps_mstamp;
+			hc->ack_epoch_acked = 0;
+
+			if (hc->mode == BBR_PROBE_BW)
+				bbr_set_pacing_rate(sk, bbr_bw(sk), BBR_UNIT);
+			else if (hc->mode == BBR_PROBE_RTT)
+				bbr_check_probe_rtt_done(sk);
+	} /* Important for ECN handling! */ 
+	/*else if ((event == CA_EVENT_ECN_IS_CE || event == CA_EVENT_ECN_NO_CE) && bbr_ecn_enable && hc->params.precise_ece_ack) {
+		u32 state = hc->ce_state;
+		dctcp_ece_ack_update(sk, event, &hc->prior_rcv_nxt, &state);
+		hc->ce_state = state;
+	}*/
+}
+
+static u32 bbr_bdp (struct sock* sk, u32 bw, int gain) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	u32 bdp;
+	u64 w;
+	if (unlikely(hc->min_rtt_us == ~0U)) {
+		return hc->init_cwnd; 
+	}
+	w = (u64)bw * hc->min_rtt_us;
+	bdp = (((w * gain) >> BBR_SCALE) + BW_UNIT - 1) / BW_UNIT;
+
+	ccid6_pr_debug("sk=%p bw=%d min_rtt_us=%d gain=%d bdp=%d\n", sk, bw, hc->min_rtt_us, gain, bdp);
+
+	return bdp;
+}
+
+static u32 bbr_quantization_budget (struct sock* sk, u32 cwnd) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	u32 tso_segs_goal;
+	tso_segs_goal = 3 * bbr_tso_segs_goal (sk);
+	/* Allow enough full-sized skbs in flight to utilize end systems. */
+	if (hc->params.cwnd_tso_budget == 1) {
+		cwnd = max_t(u32, cwnd, tso_segs_goal);
+		cwnd = max_t(u32, cwnd, hc->params.cwnd_min_target);
+	} else {
+		cwnd += tso_segs_goal;
+
+		/* Reduce delayed ACKs by rounding up cwnd to the next even number. */
+		cwnd = (cwnd + 1) & ~1U;
+	}
+	/* Ensure gain cycling gets inflight above BDP even for small BDPs. */
+	if (hc->mode == BBR_PROBE_BW && hc->cycle_idx == BBR_BW_PROBE_UP) {
+		cwnd += 2;
+	}
+	return cwnd;
+}
+
+static u32 bbr_inflight (struct sock* sk, u32 bw, int gain) {
+	u32 inflight;
+	inflight = bbr_bdp (sk, bw, gain);
+	inflight = bbr_quantization_budget (sk, inflight);
+	
+	ccid6_pr_debug("sk %p quantization_budget %d", sk, inflight);
+
+
+	return inflight;
+}
+
+static u32 bbr_packets_in_net_at_edt (struct sock* sk, u32 inflight_now) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	struct dccp_sock *dp = dccp_sk(sk);
+
+	u64 now_ns, edt_ns, interval_us;
+	u32 interval_delivered, inflight_at_edt;
+
+	now_ns = dp->dccps_clock_cache;
+	edt_ns = max (dp->dccps_wstamp_ns, now_ns);
+	
+	interval_us = div_u64 (edt_ns - now_ns, NSEC_PER_USEC);
+	interval_delivered = (u64)bbr_bw (sk) * interval_us >> BW_SCALE;
+	inflight_at_edt = inflight_now;
+	if (hc->pacing_gain > BBR_UNIT) {
+		inflight_at_edt += bbr_tso_segs_goal (sk); 
+	}
+	if (interval_delivered >= inflight_at_edt) {
+		return 0;
+	}
+	return inflight_at_edt - interval_delivered;
+} 
+
+static u32 bbr_ack_aggregation_cwnd (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 max_aggr_cwnd, aggr_cwnd = 0;
+	if (hc->params.extra_acked_gain && (bbr_full_bw_reached (sk) || hc->params.extra_acked_in_startup)) {
+		max_aggr_cwnd = ((u64)bbr_bw (sk) * bbr_extra_acked_max_us) / BW_UNIT;
+		aggr_cwnd = (hc->params.extra_acked_gain * bbr_extra_acked (sk)) >> BBR_SCALE;
+		aggr_cwnd = min (aggr_cwnd, max_aggr_cwnd);
+	}
+	return aggr_cwnd;
+} 
+
+static u32 bbr_probe_rtt_cwnd (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (hc->params.probe_rtt_cwnd_gain == 0) {
+		return hc->params.cwnd_min_target;
+	}
+	return max_t (u32, hc->params.cwnd_min_target, bbr_bdp (sk, bbr_bw (sk), hc->params.probe_rtt_cwnd_gain));
+} 
+
+static void bbr_set_cwnd (struct sock* sk, const struct rate_sample_ccid6* rs, u32 acked, u32 bw, int gain, u32 cwnd, struct bbr_context* ctx) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	struct dccp_sock *dp = dccp_sk(sk);
+	int r_seq_used = hc->tx_cwnd / dp->dccps_l_ack_ratio;
+	u32 target_cwnd = 0, prev_cwnd = hc->tx_cwnd; //, max_probe;
+
+	if (!acked)
+		goto done;
+
+	target_cwnd = bbr_bdp (sk, bw, gain);
+	target_cwnd += bbr_ack_aggregation_cwnd (sk);
+	target_cwnd = bbr_quantization_budget (sk, target_cwnd);
+
+	hc->debug.target_cwnd = target_cwnd;
+	hc->try_fast_path = 0;
+	if (bbr_full_bw_reached (sk)) { 
+		cwnd += acked;
+		if (cwnd >= target_cwnd) {
+			cwnd = target_cwnd;
+			hc->try_fast_path = 1;
+		}
+	} else if (cwnd < target_cwnd || cwnd  < 2 * hc->init_cwnd) {
+		cwnd += acked;
+	} else {
+		hc->try_fast_path = 1;
+	}
+
+	cwnd = max_t (u32, cwnd, hc->params.cwnd_min_target);
+
+done:
+	hc->tx_cwnd = cwnd;
+	if (hc->mode == BBR_PROBE_RTT) {  /* drain queue, refresh min_rtt */
+		hc->tx_cwnd = min_t(u32, hc->tx_cwnd, bbr_probe_rtt_cwnd(sk));
+		ccid6_change_l_ack_ratio(sk, 1);
+		/* Allow extra packet(s) to let ack_ratio=1 option reaching the peer */
+		if (dccp_sk(sk)->dccps_l_ack_ratio != 1U) {
+			hc->tx_extrapkt = true;
+			dccp_tasklet_schedule(sk);
+		}
+	}
+
+	/* Do not adjust the ack_ratio if we are restoring it or we are in PROBE_RTT mode */
+	if (hc->restore_ackrt) {
+		ccid6_change_l_ack_ratio(sk, hc->prior_ackrt);
+		/* Restore should end when rx has sent confirmation */
+		if (hc->prior_ackrt == dp->dccps_l_ack_ratio) hc->restore_ackrt=0;
+	}
+	else if (hc->mode != BBR_PROBE_RTT) {
+		if (r_seq_used * CCID6_WIN_CHANGE_FACTOR >= dp->dccps_r_seq_win)
+			ccid6_change_l_ack_ratio(sk, dp->dccps_l_ack_ratio * 2);
+		else if (r_seq_used * CCID6_WIN_CHANGE_FACTOR < dp->dccps_r_seq_win/2)
+			ccid6_change_l_ack_ratio(sk, dp->dccps_l_ack_ratio / 2 ? : 1U);
+	}
+
+	/* Do not adjust the seq_window if we are restoring it */
+	if (hc->restore_seqwin) {
+		ccid6_change_l_seq_window(sk, hc->prior_seqwin);
+		/* HACK: force local seq_win to new value without waiting confirmation */
+		dp->dccps_l_seq_win = hc->prior_seqwin;
+		dccp_update_gss(sk, dp->dccps_gss);
+		hc->restore_seqwin=0;
+	}
+	else if (hc->tx_cwnd * CCID6_WIN_CHANGE_FACTOR >= dp->dccps_l_seq_win)
+		ccid6_change_l_seq_window(sk, dp->dccps_l_seq_win * 2);
+	else if (hc->tx_cwnd * CCID6_WIN_CHANGE_FACTOR < dp->dccps_l_seq_win/2)
+		ccid6_change_l_seq_window(sk, dp->dccps_l_seq_win / 2);
+
+	ctx->target_cwnd = target_cwnd;
+	ctx->log = (hc->tx_cwnd != prev_cwnd);
+} 
+
+static void bbr_update_round_start (struct sock* sk, const struct rate_sample_ccid6* rs, struct bbr_context* ctx) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	hc->round_start = 0;
+
+	if (rs->interval_us > 0 && !before (rs->prior_delivered, hc->next_rtt_delivered)) {
+		hc->next_rtt_delivered = hc->delivered;
+		hc->round_start = 1;
+	}
+}
+
+static void bbr_calculate_bw_sample (struct sock* sk, const struct rate_sample_ccid6* rs, struct bbr_context* ctx) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	u64 bw = 0;
+	if (rs->interval_us > 0) {
+		if (rs->delivered < 0) {
+			return;
+		}
+		bw = DIV_ROUND_UP_ULL((u64)rs->delivered * BW_UNIT, rs->interval_us);
+	}
+	ctx->sample_bw = bw;
+	hc->debug.rs_bw = bw;
+} 
+
+static void bbr_update_ack_aggregation (struct sock* sk,const struct rate_sample_ccid6* rs) {
+	u32 epoch_us, expected_acked, extra_acked;
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	u32 extra_acked_win_rtts_thresh = hc->params.extra_acked_win_rtts;
+	if (!hc->params.extra_acked_gain || rs->acked_sacked <= 0 || rs->delivered < 0 || rs->interval_us <= 0) {
+		return;
+	}
+	if (hc->round_start) {
+		hc->extra_acked_win_rtts = min (0x1F,	hc->extra_acked_win_rtts + 1);
+		if (hc->params.extra_acked_in_startup && !bbr_full_bw_reached (sk)) {
+			extra_acked_win_rtts_thresh = 1;
+		}
+		if (hc->extra_acked_win_rtts >= extra_acked_win_rtts_thresh) {
+			hc->extra_acked_win_rtts = 0;
+			hc->extra_acked_win_idx = hc->extra_acked_win_idx ? 0 : 1;
+			hc->extra_acked[hc->extra_acked_win_idx] = 0;
+		}
+	}
+	epoch_us = tcp_stamp_us_delta (hc->delivered_mstamp, hc->ack_epoch_mstamp);
+	expected_acked = ((u64)bbr_bw(sk) * epoch_us) / BW_UNIT;
+	if (hc->ack_epoch_acked <= expected_acked || (hc->ack_epoch_acked + rs->acked_sacked >= bbr_ack_epoch_acked_reset_thresh)) {
+		hc->ack_epoch_acked = 0;
+		hc->ack_epoch_mstamp = hc->delivered_mstamp;
+		expected_acked = 0;
+	}
+	hc->ack_epoch_acked = min_t (u32, 0xFFFFF, hc->ack_epoch_acked + rs->acked_sacked);
+	extra_acked = hc->ack_epoch_acked - expected_acked;
+	extra_acked = min (extra_acked, hc->tx_cwnd);
+	if (extra_acked > hc->extra_acked[hc->extra_acked_win_idx]) {
+		hc->extra_acked[hc->extra_acked_win_idx] = extra_acked;
+	} 
+}
+
+static void bbr_check_full_bw_reached (struct sock* sk, const struct rate_sample_ccid6* rs) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	u32 bw_thresh;
+	if (bbr_full_bw_reached(sk) || !hc->round_start || rs->is_app_limited)
+		return;
+
+	bw_thresh = (u64)hc->full_bw * hc->params.full_bw_thresh >> BBR_SCALE;
+	if (bbr_max_bw(sk) >= bw_thresh) {
+		hc->full_bw = bbr_max_bw (sk);
+		hc->full_bw_cnt = 0;
+		return;
+	}
+	++hc->full_bw_cnt;
+
+	hc->full_bw_reached = hc->full_bw_cnt >= hc->params.full_bw_cnt;
+} 
+
+static bool bbr_check_drain (struct sock* sk, const struct rate_sample_ccid6* rs, struct bbr_context* ctx) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (hc->mode == BBR_STARTUP && bbr_full_bw_reached (sk)) {
+		hc->mode = BBR_DRAIN;
+		hc->tx_ssthresh = bbr_inflight (sk, bbr_max_bw (sk), BBR_UNIT);
+
+		bbr2_reset_congestion_signals (hc);
+	}	
+	if (hc->mode == BBR_DRAIN && bbr_packets_in_net_at_edt (sk, hc->tx_pipe) <= bbr_inflight (sk, bbr_max_bw(sk), BBR_UNIT)) {
+		return true; 
+	}
+	return false;
+} 
+
+static void bbr_check_probe_rtt_done (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	ccid6_pr_debug("sk %p hc->probe_rtt_done_stamp %u ccid6_jiffies32 %u", sk, hc->probe_rtt_done_stamp, ccid6_jiffies32);
+
+	if (!(hc->probe_rtt_done_stamp && after (ccid6_jiffies32, hc->probe_rtt_done_stamp))) {
+		return;
+	}
+	hc->probe_rtt_min_stamp = ccid6_jiffies32;
+
+	hc->tx_cwnd = max (hc->tx_cwnd, hc->prior_cwnd);
+
+	bbr2_exit_probe_rtt (sk);
+}
+
+static void bbr_update_min_rtt (struct sock* sk, const struct rate_sample_ccid6* rs) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	bool probe_rtt_expired, min_rtt_expired;
+	u32 expire;
+
+	if (hc->min_rtt_us == ~0U && rs->rtt_us > 0) {
+		hc->min_rtt_us = rs->rtt_us;
+		hc->min_rtt_stamp = ccid6_jiffies32;
+	}
+
+	expire = hc->probe_rtt_min_stamp + msecs_to_jiffies(hc->params.probe_rtt_win_ms);
+	probe_rtt_expired = after (ccid6_jiffies32, expire);
+	if (rs->rtt_us >= 0 && (rs->rtt_us <= hc->probe_rtt_min_us || (probe_rtt_expired && !rs->is_ack_delayed))) {
+		hc->probe_rtt_min_us = rs->rtt_us;
+		hc->probe_rtt_min_stamp = ccid6_jiffies32;
+	}
+	expire = hc->min_rtt_stamp + hc->params.min_rtt_win_sec * HZ;
+	min_rtt_expired = after (ccid6_jiffies32, expire);
+	if (hc->probe_rtt_min_us <= hc->min_rtt_us || min_rtt_expired) {
+		if (hc->probe_rtt_min_us > 0) /* FIXME */{
+			hc->min_rtt_us = hc->probe_rtt_min_us;
+			hc->min_rtt_stamp = hc->probe_rtt_min_stamp;
+		}
+	}
+	if (hc->params.probe_rtt_mode_ms > 0 && probe_rtt_expired && !hc->idle_restart && hc->mode != BBR_PROBE_RTT) {
+		hc->mode = BBR_PROBE_RTT; 
+		bbr_save_cwnd (sk);
+		hc->probe_rtt_done_stamp = 0;
+		hc->ack_phase = BBR_ACKS_PROBE_STOPPING;
+		hc->next_rtt_delivered = hc->delivered;
+	}
+	if (hc->mode == BBR_PROBE_RTT) {
+		hc->app_limited = (hc->delivered + hc->tx_pipe) ? : 1;
+		if (!hc->probe_rtt_done_stamp && hc->tx_pipe <= bbr_probe_rtt_cwnd (sk)) {
+			hc->probe_rtt_done_stamp = ccid6_jiffies32 + msecs_to_jiffies (hc->params.probe_rtt_mode_ms);
+			hc->probe_rtt_round_done = 0;
+			hc->next_rtt_delivered = hc->delivered;
+		} else if (hc->probe_rtt_done_stamp) {
+			if (hc->round_start) {
+				hc->probe_rtt_round_done = 1;
+			} 
+			if (hc->probe_rtt_round_done) {
+				bbr_check_probe_rtt_done(sk);
+			} 
+		}
+	}
+	if (rs->delivered > 0) {
+		hc->idle_restart = 0;
+	} 
+} 
+
+static void bbr_update_gains (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	switch (hc->mode) {
+		case BBR_STARTUP:
+			hc->pacing_gain = hc->params.high_gain;
+			hc->cwnd_gain = hc->params.startup_cwnd_gain;
+			break;
+		case BBR_DRAIN:
+			hc->pacing_gain = hc->params.drain_gain; 
+			hc->cwnd_gain = hc->params.startup_cwnd_gain; 
+			break;
+		case BBR_PROBE_BW:
+			hc->pacing_gain = hc->params.pacing_gain[hc->cycle_idx];
+			hc->cwnd_gain = hc->params.cwnd_gain;
+			break;
+		case BBR_PROBE_RTT:
+			hc->pacing_gain = BBR_UNIT;
+			hc->cwnd_gain = BBR_UNIT;
+			break;
+		default:
+			break;
+	}
+} 
+
+/* Double check this, needs to be properly merged with ccid6_hc_tx_init */
+static void bbr_init (struct sock* sk, struct ccid6_hc_tx_sock *hc) {
+	int i;
+
+	WARN_ON_ONCE(hc->tx_cwnd >= bbr_cwnd_warn_val);
+
+	hc->initialized = 1;
+	hc->params.high_gain = min(0x7FF, bbr_high_gain);
+	hc->params.drain_gain = min(0x3FF, bbr_drain_gain);
+	hc->params.startup_cwnd_gain = min(0x7FF, bbr_startup_cwnd_gain);
+	hc->params.cwnd_gain = min(0x7FF, bbr_cwnd_gain);
+	hc->params.cwnd_tso_budget = min(0x1U, bbr_cwnd_tso_budget);
+	hc->params.cwnd_min_target = min(0xFU, bbr_cwnd_min_target);
+	hc->params.min_rtt_win_sec = min(0x1FU, bbr_min_rtt_win_sec);
+	hc->params.probe_rtt_mode_ms = min(0x1FFU, bbr_probe_rtt_mode_ms);
+	hc->params.full_bw_cnt = min(0x7U, bbr_full_bw_cnt);
+	//hc->params.bw_rtts = min(0x1F, bbr_bw_rtts); <----- never used
+	hc->params.full_bw_thresh = min(0x3FFU, bbr_full_bw_thresh);
+	hc->params.extra_acked_gain = min(0x7FF, bbr_extra_acked_gain);
+	hc->params.extra_acked_win_rtts = min(0x1FU, bbr_extra_acked_win_rtts);
+	hc->params.drain_to_target = bbr_drain_to_target ? 1 : 0;
+	hc->params.precise_ece_ack = bbr_precise_ece_ack ? 1 : 0;
+	hc->params.extra_acked_in_startup = bbr_extra_acked_in_startup ? 1 : 0;
+	hc->params.probe_rtt_cwnd_gain = min(0xFFU, bbr_probe_rtt_cwnd_gain);
+	hc->params.probe_rtt_win_ms = 
+		min(0x3FFFU, 
+			min_t(u32, bbr_probe_rtt_win_ms,
+				  hc->params.min_rtt_win_sec * MSEC_PER_SEC));
+	for (i = 0; i < CYCLE_LEN; i++)
+		hc->params.pacing_gain[i] = min(0x3FF, bbr_pacing_gain[i]);
+	hc->params.usage_based_cwnd = bbr_usage_based_cwnd ? 1 : 0;
+	hc->params.tso_rtt_shift =  min(0xFU, bbr_tso_rtt_shift);
+
+	//hc->debug.snd_isn = tp->snd_una; // only used in debug messages
+	hc->debug.target_cwnd = 0;
+	hc->debug.undo = 0;
+
+	hc->init_cwnd = min(0x7FU, hc->tx_cwnd);
+	hc->prior_cwnd = 0;
+	// ssthresh (is set in ccid6)
+	hc->next_rtt_delivered = 0;
+	hc->prev_ca_state = DCCP_CA_Open;
+	hc->packet_conservation = 0;
+
+	hc->probe_rtt_done_stamp = 0;
+	hc->probe_rtt_round_done = 0;
+	hc->probe_rtt_min_us = ~0U; // <-----
+	hc->probe_rtt_min_stamp = ccid6_jiffies32;
+	hc->min_rtt_us = ~0U; // ?
+	hc->min_rtt_stamp = ccid6_jiffies32;
+
+	hc->has_seen_rtt = 0;
+	/* We do not call bbr_init_pacing_rate_from_rtt from here, intentionally.
+	 * This is done only when a data packet has been sent, 
+	 * i.e. in ccid6_hc_tx_send_packet.
+	 */
+
+	hc->round_start = 0;
+	hc->idle_restart = 0;
+	hc->full_bw_reached = 0;
+	hc->full_bw = 0;
+	hc->full_bw_cnt = 0;
+	hc->cycle_mstamp = 0;
+	hc->cycle_idx = 0;
+	hc->mode = BBR_STARTUP;
+	hc->debug.rs_bw = 0;
+
+	hc->ack_epoch_mstamp = dccp_sk(sk)->dccps_mstamp;
+	hc->ack_epoch_acked = 0;
+	hc->extra_acked_win_rtts = 0;
+	hc->extra_acked_win_idx = 0;
+	hc->extra_acked[0] = 0;
+	hc->extra_acked[1] = 0;
+
+	hc->ce_state = 0;
+	//hc->prior_rcv_nxt = dp->dccps_gsr; //or dccps_gsr + 1 ???
+	hc->try_fast_path = 0;
+
+	cmpxchg (&sk->sk_pacing_status, SK_PACING_NONE, SK_PACING_NEEDED);
+} 
+
+/* static u32 bbr_sndbuf_expand(struct sock *sk) {
+   return 0;
+   } */
+
+// ** BBRv2 ******************************************************************************
+
+static void bbr2_take_bw_hi_sample (struct sock* sk, u32 bw) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	hc->bw_hi[1] = max (bw, hc->bw_hi[1]);
+} 
+
+static void bbr2_advance_bw_hi_filter (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (!hc->bw_hi[1]) {
+		return; 
+	}
+	hc->bw_hi[0] = hc->bw_hi[1];
+	hc->bw_hi[1] = 0;
+} 
+
+static u32 bbr2_target_inflight (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	u32 bdp = bbr_inflight (sk, bbr_bw (sk), BBR_UNIT);
+	return min (bdp, hc->tx_cwnd);
+} 
+
+static bool bbr2_is_probing_bandwidth (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	return (hc->mode == BBR_STARTUP) || (hc->mode == BBR_PROBE_BW && (hc->cycle_idx == BBR_BW_PROBE_REFILL || hc->cycle_idx == BBR_BW_PROBE_UP));
+}
+
+static bool bbr2_has_elapsed_in_phase (const struct sock* sk, u32 interval_us) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	return tcp_stamp_us_delta (dccp_sk(sk)->dccps_mstamp, hc->cycle_mstamp + interval_us) > 0;
+} 
+
+static void bbr2_handle_queue_too_high_in_startup (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	hc->full_bw_reached = 1;
+	hc->inflight_hi = bbr_inflight (sk, bbr_max_bw (sk), BBR_UNIT);
+}
+
+static void bbr2_check_ecn_too_high_in_startup (struct sock* sk, u32 ce_ratio) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (bbr_full_bw_reached (sk) || !hc->ecn_eligible || !hc->params.full_ecn_cnt || !hc->params.ecn_thresh) {
+		return;
+	}
+	if (ce_ratio >= hc->params.ecn_thresh) {
+		hc->startup_ecn_rounds++;
+	} else {
+		hc->startup_ecn_rounds = 0;
+	}
+	if (hc->startup_ecn_rounds >= hc->params.full_ecn_cnt) {
+		bbr2_handle_queue_too_high_in_startup (sk);
+		return;
+	} 
+}
+
+static void bbr2_update_ecn_alpha (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	s32 delivered, delivered_ce;
+	u64 alpha, ce_ratio;
+	u32 gain;
+	if (hc->params.ecn_factor == 0) {
+		return;
+	}
+	delivered = hc->delivered - hc->alpha_last_delivered;
+	delivered_ce = hc->delivered_ce - hc->alpha_last_delivered_ce;
+	if (delivered == 0) {
+		return;
+	}
+	if (!hc->ecn_eligible && bbr_ecn_enable && (hc->min_rtt_us <= hc->params.ecn_max_rtt_us || !hc->params.ecn_max_rtt_us)) {
+		hc->ecn_eligible = 1;
+	}
+	ce_ratio = (u64)delivered_ce << BBR_SCALE;
+	do_div (ce_ratio, delivered);
+	gain = hc->params.ecn_alpha_gain;
+	alpha = ((BBR_UNIT - gain) * hc->ecn_alpha) >> BBR_SCALE;
+	alpha += (gain * ce_ratio) >> BBR_SCALE;
+	hc->ecn_alpha = min_t (u32, alpha, BBR_UNIT);
+	hc->alpha_last_delivered = hc->delivered;
+	hc->alpha_last_delivered_ce = hc->delivered_ce;
+	bbr2_check_ecn_too_high_in_startup (sk, ce_ratio);
+}
+
+static void bbr2_raise_inflight_hi_slope (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 growth_this_round, cnt;
+	growth_this_round = 1 << hc->bw_probe_up_rounds;
+	hc->bw_probe_up_rounds = min (hc->bw_probe_up_rounds + 1, 30);
+	cnt = hc->tx_cwnd / growth_this_round;
+	cnt = max (cnt, 1U);
+	hc->bw_probe_up_cnt = cnt;
+} 
+
+static void bbr2_probe_inflight_hi_upward (struct sock* sk, const struct rate_sample_ccid6* rs) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 delta;
+	// not sure about the below line
+	if (!ccid6_cwnd_network_limited(hc) || hc->tx_cwnd < hc->inflight_hi) {	
+		hc->bw_probe_up_acks = 0;  
+		return;  
+	}
+	hc->bw_probe_up_acks += rs->acked_sacked;
+	if (hc->bw_probe_up_acks >=  hc->bw_probe_up_cnt) {
+		delta = hc->bw_probe_up_acks / hc->bw_probe_up_cnt;
+		hc->bw_probe_up_acks -= delta * hc->bw_probe_up_cnt;
+		hc->inflight_hi += delta;
+	}
+	if (hc->round_start) {
+		bbr2_raise_inflight_hi_slope (sk);
+	} 
+} 
+
+static bool bbr2_is_inflight_too_high (const struct sock* sk, const struct rate_sample_ccid6 * rs) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 loss_thresh, ecn_thresh;
+	if (rs->lost > 0 && rs->tx_in_flight) {
+		loss_thresh = (u64)rs->tx_in_flight * hc->params.loss_thresh >> BBR_SCALE;
+		if (rs->lost > loss_thresh) {
+			return true;
+		}
+	}
+	if (rs->delivered_ce > 0 && rs->delivered > 0 && hc->ecn_eligible && hc->params.ecn_thresh) {
+		ecn_thresh = (u64)rs->delivered * hc->params.ecn_thresh >> BBR_SCALE;
+		if (rs->delivered_ce >= ecn_thresh) {
+			return true;
+		}
+	}
+	return false;
+} 
+
+static u32 bbr2_inflight_hi_from_lost_skb (const struct sock* sk, const struct rate_sample_ccid6* rs, u32 pcount) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);		
+	u32 loss_thresh = hc->params.loss_thresh;
+	u32 divisor, inflight_hi;
+	s32 inflight_prev, lost_prev;
+	u64 loss_budget, lost_prefix;
+	//pcount = tcp_skb_pcount (skb); // This is supposed to keep track of how many actual packets we have, in case of TSO (but no TSO here). Can we assume that this is always equal to 1?
+
+	inflight_prev = rs->tx_in_flight - pcount;
+	if (inflight_prev < 0) {
+		return ~0U;
+	}
+	lost_prev = rs->lost - pcount;
+	if (lost_prev < 0) {
+		return ~0U;
+	}
+	loss_budget = (u64)inflight_prev * loss_thresh + BBR_UNIT - 1;
+	loss_budget >>= BBR_SCALE;
+	if (lost_prev >= loss_budget) {
+		lost_prefix = 0; 
+	} else {
+		lost_prefix = loss_budget - lost_prev;
+		lost_prefix <<= BBR_SCALE;
+		divisor = BBR_UNIT - loss_thresh;
+		if (!divisor) { 
+			return ~0U;
+		}
+		do_div (lost_prefix, divisor);
+	}
+	inflight_hi = inflight_prev + lost_prefix;
+	return inflight_hi;
+} 
+
+static u32 bbr2_inflight_with_headroom (const struct sock *sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 headroom, headroom_fraction;
+	if (hc->inflight_hi == ~0U) {
+		return ~0U;
+	}
+	headroom_fraction = hc->params.inflight_headroom;
+	headroom = ((u64)hc->inflight_hi * headroom_fraction) >> BBR_SCALE;
+	headroom = max (headroom, 1U);
+	return max_t(s32, hc->inflight_hi - headroom, hc->params.cwnd_min_target);
+} 
+
+static void bbr2_bound_cwnd_for_inflight_model (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	u32 cap;
+	if (!hc->initialized) {
+		return;
+	}
+	cap = ~0U;
+	if (hc->mode == BBR_PROBE_BW && hc->cycle_idx != BBR_BW_PROBE_CRUISE) {
+		cap = hc->inflight_hi;
+	} else {
+		if (hc->mode == BBR_PROBE_RTT || (hc->mode == BBR_PROBE_BW && hc->cycle_idx == BBR_BW_PROBE_CRUISE)) {
+			cap = bbr2_inflight_with_headroom (sk);
+		}
+	}
+	cap = min (cap, hc->inflight_lo);
+	cap = max_t (u32, cap, hc->params.cwnd_min_target);
+
+	hc->tx_cwnd = min (cap, hc->tx_cwnd);
+} 
+
+static void bbr2_adapt_lower_bounds (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 ecn_cut, ecn_inflight_lo, beta;
+
+	/* We only use lower-bound estimates when not probing bw.
+	 * When probing we need to push inflight higher to probe bw.
+	 */
+	if (bbr2_is_probing_bandwidth (sk)) {
+		return;
+	}
+
+	/* ECN response. */
+	if (hc->ecn_in_round && hc->ecn_eligible && hc->params.ecn_factor) {
+		ecn_cut = (BBR_UNIT - ((hc->ecn_alpha * hc->params.ecn_factor) >> BBR_SCALE));
+		if (hc->inflight_lo == ~0U) {
+			hc->inflight_lo = hc->tx_cwnd;
+		}
+		ecn_inflight_lo = (u64)hc->inflight_lo * ecn_cut >> BBR_SCALE;
+	} else {
+		ecn_inflight_lo = ~0U;
+	}
+
+	/* Loss response. */
+	if (hc->loss_in_round) {
+		ccid6_pr_debug("sk=%p loss_in_round", sk);
+		/* Reduce bw and inflight to (1 - beta). */
+		if (hc->bw_lo == ~0U) {
+			hc->bw_lo = bbr_max_bw(sk);
+		}
+		if (hc->inflight_lo == ~0U) {
+			hc->inflight_lo = hc->tx_cwnd;
+		}
+		beta = hc->params.beta;
+		hc->bw_lo = max_t(u32, hc->bw_latest, (u64)hc->bw_lo * (BBR_UNIT - beta) >> BBR_SCALE);
+		hc->inflight_lo = max_t(u32, hc->inflight_latest, (u64)hc->inflight_lo * (BBR_UNIT - beta) >> BBR_SCALE);
+		
+	}
+
+	/* Adjust to the lower of the levels implied by loss or ECN. */
+	hc->inflight_lo = min (hc->inflight_lo, ecn_inflight_lo);
+} 
+
+static void bbr2_reset_lower_bounds (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	hc->bw_lo = ~0U;
+	hc->inflight_lo = ~0U;
+} 
+
+static void bbr2_reset_congestion_signals (struct ccid6_hc_tx_sock *hc) {
+	hc->loss_in_round = 0;
+	hc->ecn_in_round = 0;
+	hc->loss_in_cycle = 0;
+	hc->ecn_in_cycle = 0;
+	hc->bw_latest = 0;
+	hc->inflight_latest = 0;
+} 
+
+static void bbr2_update_congestion_signals (struct sock* sk, const struct rate_sample_ccid6* rs, struct bbr_context* ctx) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u64 bw;
+	hc->loss_round_start = 0;
+	if (rs->interval_us <= 0 || !rs->acked_sacked) {
+		return; 
+	}
+	bw = ctx->sample_bw;
+	if (!rs->is_app_limited || bw >= bbr_max_bw (sk)) {
+		bbr2_take_bw_hi_sample (sk, bw);
+	}
+	hc->loss_in_round |= (rs->losses > 0);
+	hc->bw_latest = max_t (u32, hc->bw_latest, ctx->sample_bw);
+	hc->inflight_latest = max_t (u32, hc->inflight_latest, rs->delivered);
+	if (before (rs->prior_delivered, hc->loss_round_delivered)) {
+		return;	
+	}
+	hc->loss_round_delivered = hc->delivered; 
+	hc->loss_round_start = 1;
+	bbr2_adapt_lower_bounds (sk);
+	hc->loss_in_round = 0;
+	hc->ecn_in_round  = 0;
+	hc->bw_latest = ctx->sample_bw;
+	hc->inflight_latest = rs->delivered;
+} 
+
+static bool bbr2_is_reno_coexistence_probe_time (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 inflight, rounds, reno_gain, reno_rounds;
+	rounds = hc->params.bw_probe_max_rounds;
+	reno_gain = hc->params.bw_probe_reno_gain;
+	if (reno_gain) {
+		inflight = bbr2_target_inflight (sk);
+		reno_rounds = ((u64)inflight * reno_gain) >> BBR_SCALE;
+		rounds = min (rounds, reno_rounds);
+	}
+	return hc->rounds_since_probe >= rounds;
+} 
+
+static void bbr2_pick_probe_wait (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	hc->rounds_since_probe = prandom_u32_max (hc->params.bw_probe_rand_rounds);
+	hc->probe_wait_us = hc->params.bw_probe_base_us + prandom_u32_max (hc->params.bw_probe_rand_us);
+} 
+
+static void bbr2_set_cycle_idx (struct sock* sk, int cycle_idx) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	hc->cycle_idx = cycle_idx;
+	hc->try_fast_path = 0;
+} 
+
+static void bbr2_start_bw_probe_refill (struct sock* sk, u32 bw_probe_up_rounds) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	bbr2_reset_lower_bounds (sk);
+	if (hc->inflight_hi != ~0U) {
+		hc->inflight_hi += hc->params.refill_add_inc;
+	}
+	hc->bw_probe_up_rounds = bw_probe_up_rounds;
+	hc->bw_probe_up_acks = 0;
+	hc->stopped_risky_probe = 0;
+	hc->ack_phase = BBR_ACKS_REFILLING;
+	hc->next_rtt_delivered = hc->delivered;
+	bbr2_set_cycle_idx(sk, BBR_BW_PROBE_REFILL);
+} 
+
+static void bbr2_start_bw_probe_up (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	hc->ack_phase = BBR_ACKS_PROBE_STARTING;
+	hc->next_rtt_delivered = hc->delivered;
+	hc->cycle_mstamp = dccp_sk(sk)->dccps_mstamp;
+	bbr2_set_cycle_idx (sk, BBR_BW_PROBE_UP);
+	bbr2_raise_inflight_hi_slope (sk);
+} 
+
+static void bbr2_start_bw_probe_down (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	bbr2_reset_congestion_signals (hc);
+	hc->bw_probe_up_cnt = ~0U; 
+	bbr2_pick_probe_wait (sk);
+	hc->cycle_mstamp = dccp_sk(sk)->dccps_mstamp;
+	hc->ack_phase = BBR_ACKS_PROBE_STOPPING;
+	hc->next_rtt_delivered = hc->delivered;
+	bbr2_set_cycle_idx (sk, BBR_BW_PROBE_DOWN);
+}
+
+static void bbr2_start_bw_probe_cruise (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	if (hc->inflight_lo != ~0U) {
+		hc->inflight_lo = min (hc->inflight_lo, hc->inflight_hi);
+	}
+	bbr2_set_cycle_idx (sk, BBR_BW_PROBE_CRUISE); 
+} 
+
+static void bbr2_handle_inflight_too_high (struct sock* sk, const struct rate_sample_ccid6* rs) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	const u32 beta = hc->params.beta;
+	hc->prev_probe_too_high = 1;
+	hc->bw_probe_samples = 0;  
+	if (!rs->is_app_limited) {
+		hc->inflight_hi = max_t(u32, rs->tx_in_flight, (u64)bbr2_target_inflight (sk) * (BBR_UNIT - beta) >> BBR_SCALE);
+	}
+	if (hc->mode == BBR_PROBE_BW && hc->cycle_idx == BBR_BW_PROBE_UP) {
+		bbr2_start_bw_probe_down (sk);
+	} 
+} 
+
+static bool bbr2_adapt_upper_bounds (struct sock* sk, const struct rate_sample_ccid6* rs) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (hc->ack_phase == BBR_ACKS_PROBE_STARTING && hc->round_start) {
+		hc->ack_phase = BBR_ACKS_PROBE_FEEDBACK;
+	}
+	if (hc->ack_phase == BBR_ACKS_PROBE_STOPPING && hc->round_start) {
+		hc->bw_probe_samples = 0;
+		hc->ack_phase = BBR_ACKS_INIT;
+		if (hc->mode == BBR_PROBE_BW && !rs->is_app_limited) {
+			bbr2_advance_bw_hi_filter (sk);
+		}
+		if (hc->mode == BBR_PROBE_BW && hc->stopped_risky_probe && !hc->prev_probe_too_high) {
+			bbr2_start_bw_probe_refill (sk, 0);
+			return true;  
+		}
+	}
+	if (bbr2_is_inflight_too_high(sk, rs)) {
+		if (hc->bw_probe_samples) {
+			bbr2_handle_inflight_too_high(sk, rs);
+		}
+	} else {
+		if (hc->inflight_hi == ~0U) {
+			return false;
+		}
+		if (rs->tx_in_flight > hc->inflight_hi) {
+			hc->inflight_hi = rs->tx_in_flight;
+		}
+
+		if (hc->mode == BBR_PROBE_BW && hc->cycle_idx == BBR_BW_PROBE_UP) {
+			bbr2_probe_inflight_hi_upward (sk, rs);
+		}
+	}
+
+	return false;
+} 
+
+static bool bbr2_check_time_to_probe_bw (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 n;
+	if (hc->params.ecn_reprobe_gain && hc->ecn_eligible && hc->ecn_in_cycle && !hc->loss_in_cycle && hc->curr_ca_state == DCCP_CA_Open) {
+		n = ilog2((((u64)hc->inflight_hi * hc->params.ecn_reprobe_gain) >> BBR_SCALE));
+		bbr2_start_bw_probe_refill (sk, n);
+		return true;
+	}
+	if (bbr2_has_elapsed_in_phase (sk, hc->probe_wait_us) || bbr2_is_reno_coexistence_probe_time (sk)) {
+		bbr2_start_bw_probe_refill (sk, 0);
+		return true;
+	}
+	return false;
+} 
+
+static bool bbr2_check_time_to_cruise (struct sock* sk, u32 inflight, u32 bw) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	bool is_under_bdp, is_long_enough;
+	if (inflight > bbr2_inflight_with_headroom (sk)) {
+		return false;
+	}
+	is_under_bdp = inflight <= bbr_inflight (sk, bw, BBR_UNIT);
+	if (hc->params.drain_to_target) {
+		return is_under_bdp;
+	}
+	is_long_enough = bbr2_has_elapsed_in_phase (sk, hc->min_rtt_us);
+
+	return is_under_bdp || is_long_enough;
+} 
+
+static void bbr2_update_cycle_phase (struct sock* sk, const struct rate_sample_ccid6* rs) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	bool is_risky = false, is_queuing = false;
+	u32 inflight, bw;
+	if (!bbr_full_bw_reached (sk)) {
+		return;
+	}
+	if (bbr2_adapt_upper_bounds (sk, rs)) {
+		return;	
+	}
+	if (hc->mode != BBR_PROBE_BW) {
+		return;
+	}
+	inflight = bbr_packets_in_net_at_edt (sk, rs->prior_in_flight);
+	bw = bbr_max_bw (sk);
+	switch (hc->cycle_idx) {
+		case BBR_BW_PROBE_CRUISE:
+			if (bbr2_check_time_to_probe_bw (sk))
+				return;	
+			break;
+		case BBR_BW_PROBE_REFILL:
+			if (hc->round_start) {
+				hc->bw_probe_samples = 1;
+				bbr2_start_bw_probe_up(sk);
+			}
+			break;
+		case BBR_BW_PROBE_UP:
+			if (hc->prev_probe_too_high &&
+					inflight >= hc->inflight_hi) {
+				hc->stopped_risky_probe = 1;
+				is_risky = true;
+			} else if (bbr2_has_elapsed_in_phase (sk, hc->min_rtt_us) && inflight >= bbr_inflight (sk, bw, hc->params.bw_probe_pif_gain)) {
+				is_queuing = true;
+			}
+			if (is_risky || is_queuing) {
+				hc->prev_probe_too_high = 0;
+				bbr2_start_bw_probe_down(sk);
+			}
+			break;
+		case BBR_BW_PROBE_DOWN:
+			if (bbr2_check_time_to_probe_bw (sk))
+				return;
+			if (bbr2_check_time_to_cruise (sk, inflight, bw))
+				bbr2_start_bw_probe_cruise (sk);
+			break;
+		default:
+			break;
+	}
+} 
+
+static void bbr2_exit_probe_rtt (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	bbr2_reset_lower_bounds (sk);
+	if (bbr_full_bw_reached (sk)) {
+		hc->mode = BBR_PROBE_BW;
+		bbr2_start_bw_probe_down (sk);
+		bbr2_start_bw_probe_cruise (sk);
+	} else {
+		hc->mode = BBR_STARTUP;
+	}
+} 
+
+static void bbr2_check_loss_too_high_in_startup (struct sock* sk, const struct rate_sample_ccid6* rs) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (bbr_full_bw_reached (sk)) {
+		return;
+	}
+	if (rs->losses && hc->loss_events_in_round < 0xf) {
+		hc->loss_events_in_round++;
+	}
+	if (hc->params.full_loss_cnt && hc->loss_round_start && hc->curr_ca_state == DCCP_CA_Recovery && hc->loss_events_in_round >= hc->params.full_loss_cnt && bbr2_is_inflight_too_high(sk, rs)) {
+		bbr2_handle_queue_too_high_in_startup (sk);
+		return;
+	}
+	if (hc->loss_round_start) {
+		hc->loss_events_in_round = 0;
+	} 
+} 
+
+static void bbr2_check_drain (struct sock* sk, const struct rate_sample_ccid6* rs, struct bbr_context* ctx) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (bbr_check_drain (sk, rs, ctx)) {
+		hc->mode = BBR_PROBE_BW;
+		bbr2_start_bw_probe_down (sk);
+	}
+} 
+
+static void bbr2_update_model (struct sock* sk, const struct rate_sample_ccid6* rs, struct bbr_context* ctx) {
+	bbr2_update_congestion_signals (sk, rs, ctx);
+	bbr_update_ack_aggregation (sk, rs);
+	bbr2_check_loss_too_high_in_startup (sk, rs);
+	bbr_check_full_bw_reached (sk, rs);
+	bbr2_check_drain (sk, rs, ctx);
+	bbr2_update_cycle_phase (sk, rs);
+	bbr_update_min_rtt (sk, rs);
+} 
+
+static bool bbr2_fast_path (struct sock* sk, bool* update_model, const struct rate_sample_ccid6* rs, struct bbr_context* ctx) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	u32 prev_min_rtt_us, prev_mode;
+	if (hc->params.fast_path && hc->try_fast_path && rs->is_app_limited && ctx->sample_bw < bbr_max_bw (sk) && !hc->loss_in_round && !hc->ecn_in_round) {
+		prev_mode = hc->mode;
+		prev_min_rtt_us = hc->min_rtt_us;
+		bbr2_check_drain (sk, rs, ctx);
+		bbr2_update_cycle_phase (sk, rs);
+		bbr_update_min_rtt (sk, rs);
+		if (hc->mode == prev_mode && hc->min_rtt_us == prev_min_rtt_us && hc->try_fast_path) {
+			return true;
+		}
+		*update_model = false;
+	}
+	return false;
+} 
+
+void bbr2_main (struct sock* sk, const struct rate_sample_ccid6* rs) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	struct bbr_context ctx = { 0 };
+	bool update_model = true;
+	u32 bw;
+	bbr_update_round_start (sk, rs, &ctx);
+	if (hc->round_start) {
+		hc->rounds_since_probe = min_t (s32, hc->rounds_since_probe + 1, 0xFF);
+		bbr2_update_ecn_alpha (sk);
+	}
+
+	hc->ecn_in_round |= rs->is_ece;
+	bbr_calculate_bw_sample (sk, rs, &ctx);
+	
+	if (bbr2_fast_path (sk, &update_model, rs, &ctx)) {
+		goto out;
+	}
+	
+	if (update_model) {
+		bbr2_update_model (sk, rs, &ctx);
+	}
+	
+	bbr_update_gains (sk);
+	bw = bbr_bw (sk);
+	bbr_set_pacing_rate (sk, bw, hc->pacing_gain);
+	bbr_set_cwnd (sk, rs, rs->acked_sacked, bw, hc->cwnd_gain, hc->tx_cwnd, &ctx);
+	bbr2_bound_cwnd_for_inflight_model (sk);
+
+	ccid6_pr_debug("sk=%p bw=%d hc->bw_lo=%u, hc->bw_hi[0]=%d, hc->bw_hi[1]=%d, sample_rate=%lldkbps, hc->mode=%d, hc->cycle_idx=%d\n", sk, bw, hc->bw_lo, hc->bw_hi[0], hc->bw_hi[1], bbr_rate_kbps(sk, ctx.sample_bw), hc->mode, hc->cycle_idx);
+
+out:
+	hc->prev_ca_state = hc->curr_ca_state;
+
+	hc->loss_in_cycle |= rs->lost > 0;
+	hc->ecn_in_cycle |= rs->delivered_ce > 0;
+
+	ccid6_pr_debug("sk=%p mode=%d phase=%d min_rtt=%d cwnd=%d cwnd_gain=%d pacing_gain=%d\n",
+	 		sk, hc->mode, hc->cycle_idx, hc->min_rtt_us, 
+			hc->tx_cwnd, hc->cwnd_gain, hc->pacing_gain);
+
+}
+
+static void bbr2_init (struct sock* sk, struct ccid6_hc_tx_sock *hc) {	
+	bbr_init (sk, hc);
+	hc->params.beta = min_t(u32, 0xFFU, bbr_beta);
+	hc->params.ecn_alpha_gain = min_t(u32, 0xFFU, bbr_ecn_alpha_gain);
+	hc->params.ecn_alpha_init = min_t(u32, BBR_UNIT, bbr_ecn_alpha_init);
+	hc->params.ecn_factor = min_t(u32, 0xFFU, bbr_ecn_factor);
+	hc->params.ecn_thresh = min_t(u32, 0xFFU, bbr_ecn_thresh);
+	hc->params.ecn_max_rtt_us = min_t(u32, 0x7ffffU, bbr_ecn_max_rtt_us);
+	hc->params.ecn_reprobe_gain = min_t(u32, 0x1FF, bbr_ecn_reprobe_gain);
+	hc->params.loss_thresh = min_t(u32, 0xFFU, bbr_loss_thresh);
+	hc->params.full_loss_cnt = min_t(u32, 0xFU, bbr_full_loss_cnt);
+	hc->params.full_ecn_cnt = min_t(u32, 0x3U, bbr_full_ecn_cnt);
+	hc->params.inflight_headroom = min_t(u32, 0xFFU, bbr_inflight_headroom);
+	hc->params.bw_probe_pif_gain = min_t(u32, 0x1FFU, bbr_bw_probe_pif_gain);
+	hc->params.bw_probe_reno_gain = min_t(u32, 0x1FFU, bbr_bw_probe_reno_gain);
+	hc->params.bw_probe_max_rounds = min_t(u32, 0xFFU, bbr_bw_probe_max_rounds);
+	hc->params.bw_probe_rand_rounds = min_t(u32, 0xFU, bbr_bw_probe_rand_rounds);
+	hc->params.bw_probe_base_us = min_t(u32, (1 << 26) - 1, bbr_bw_probe_base_us);
+	hc->params.bw_probe_rand_us = min_t(u32, (1 << 26) - 1, bbr_bw_probe_rand_us);
+	hc->params.undo = bbr_undo;
+	hc->params.fast_path = bbr_fast_path ? 1 : 0;
+	hc->params.refill_add_inc = min_t(u32, 0x3U, bbr_refill_add_inc);
+
+	/* BBR v2 state: */
+	hc->initialized = 1;
+	/* Start sampling ECN mark rate after first full flight is ACKed: */
+	hc->loss_round_delivered = hc->delivered + 1;
+	hc->loss_round_start = 0;
+	hc->undo_bw_lo = 0;
+	hc->undo_inflight_lo = 0;
+	hc->undo_inflight_hi = 0;
+	hc->loss_events_in_round = 0;
+	hc->startup_ecn_rounds = 0;
+	bbr2_reset_congestion_signals(hc);
+	hc->bw_lo = ~0U;
+	hc->bw_hi[0] = 0;
+	hc->bw_hi[1] = 0;
+	hc->inflight_lo = ~0U;
+	hc->inflight_hi = ~0U;
+	hc->bw_probe_up_cnt = ~0U;
+	hc->bw_probe_up_acks = 0;
+	hc->bw_probe_up_rounds = 0;
+	hc->probe_wait_us = 0;
+	hc->stopped_risky_probe = 0;
+	hc->ack_phase = BBR_ACKS_INIT;
+	hc->rounds_since_probe = 0;
+	hc->bw_probe_samples = 0;
+	hc->prev_probe_too_high = 0;
+	hc->ecn_eligible = 0;
+	hc->ecn_alpha = hc->params.ecn_alpha_init;
+	hc->alpha_last_delivered = 0;
+	hc->alpha_last_delivered_ce = 0;
+
+	//tp->fast_ack_mode = min_t(u32, 0x2U, bbr_fast_ack_mode); // NA
+
+	// What to do here?
+	/*if((tp->ecn_flags & TCP_ECN_OK) && bbr_ecn_enable)
+		tp->ecn_flags |= TCP_ECN_ECT_PERMANENT;*/
+}
+
+/* Called when the given skb was just marked lost.*/
+static void bbr2_skb_marked_lost (struct sock* sk, const struct ccid6_seq* seqp) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	struct rate_sample_ccid6 rs;
+	
+	/* Capture "current" data over the full round trip of loss,
+	 * to have a better chance to see the full capacity of the path.
+	*/
+	if (!hc->loss_in_round) /* first loss in this round trip? */ {
+		hc->loss_round_delivered = hc->delivered; /* set round trip */
+	}
+	hc->loss_in_round = 1;
+	hc->loss_in_cycle = 1;
+
+	if (!hc->bw_probe_samples) {
+		return; /* not an skb sent while probing for bandwidth */
+	}
+	if (unlikely (!seqp->delivered_mstamp)) {
+		return; /* skb was SACKed, reneged, marked lost; ignore it */
+	}
+	/* We are probing for bandwidth. Construct a rate sample that
+	 * estimates what happened in the flight leading up to this lost skb,
+	 * then see if the loss rate went too high, and if so at which packet.
+	 */
+	memset (&rs, 0, sizeof (rs));
+	rs.tx_in_flight = seqp->in_flight;
+
+	rs.lost = hc->lost - seqp->lost;
+	rs.is_app_limited = seqp->is_app_limited;
+	if (bbr2_is_inflight_too_high (sk, &rs)) {
+		rs.tx_in_flight = bbr2_inflight_hi_from_lost_skb (sk, &rs, 1);
+		bbr2_handle_inflight_too_high (sk, &rs);
+	}
+}
+
+// When to call this?
+/*static u32 bbr2_undo_cwnd (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	hc->debug_undo = 1;
+	hc->full_bw = 0;   
+	hc->full_bw_cnt = 0;
+	hc->loss_in_round = 0;
+
+	if (!hc->params.undo) {
+		return hc->tx_cwnd;
+	}
+
+	hc->bw_lo = max (hc->bw_lo, hc->undo_bw_lo);
+	hc->inflight_lo = max (hc->inflight_lo, hc->undo_inflight_lo);
+	hc->inflight_hi = max (hc->inflight_hi, hc->undo_inflight_hi);
+	return hc->prior_cwnd;
+}*/
+
+// Never called by CCID5
+// Called upon entering loss recovery
+static u32 bbr2_ssthresh (struct sock* sk) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	bbr_save_cwnd (sk);
+	hc->undo_bw_lo	 = hc->bw_lo;
+	hc->undo_inflight_lo	= hc->inflight_lo;
+	hc->undo_inflight_hi	= hc->inflight_hi;
+	return hc->tx_ssthresh;
+}
+
+/*static enum tcp_bbr2_phase bbr2_get_phase (struct ccid6_hc_tx_sock *hc) {
+	switch (hc->mode) {
+		case BBR_STARTUP:
+			return BBR2_PHASE_STARTUP;
+		case BBR_DRAIN:
+			return BBR2_PHASE_DRAIN;
+		case BBR_PROBE_BW:
+			break;
+		case BBR_PROBE_RTT:
+			return BBR2_PHASE_PROBE_RTT;
+		default:
+			return BBR2_PHASE_INVALID;
+	}
+	switch (hc->cycle_idx) {
+		case BBR_BW_PROBE_UP:
+			return BBR2_PHASE_PROBE_BW_UP;
+		case BBR_BW_PROBE_DOWN:
+			return BBR2_PHASE_PROBE_BW_DOWN;
+		case BBR_BW_PROBE_CRUISE:
+			return BBR2_PHASE_PROBE_BW_CRUISE;
+		case BBR_BW_PROBE_REFILL:
+			return BBR2_PHASE_PROBE_BW_REFILL;
+		default:
+			return BBR2_PHASE_INVALID;
+	}
+}*/
+
+/* static size_t bbr2_get_info (struct sock* sk, u32 ext, int* attr, union tcp_cc_info* info) {
+   return 0;
+   } */
+
+static void bbr2_set_state (struct sock* sk, u8 new_state) {
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	if (new_state == DCCP_CA_Loss) {
+		hc->prev_ca_state = DCCP_CA_Loss;
+		hc->full_bw = 0;
+		if (!bbr2_is_probing_bandwidth (sk) && hc->inflight_lo == ~0U) {
+			hc->inflight_lo = hc->prior_cwnd;
+		}
+	} else if (hc->prev_ca_state == DCCP_CA_Loss && new_state != DCCP_CA_Loss) {
+		hc->tx_cwnd = max (hc->tx_cwnd, hc->prior_cwnd);
+		hc->try_fast_path = 0; 
+	}
+}
+
+/*****************************************************/
+/*       FUNCTIONS PART OF tcp_bbr2.c END HERE       */
+/*****************************************************/
+static void ccid6_hc_tx_rto_expire(struct timer_list *t)
+{
+	//struct sock *sk = (struct sock *)data;
+	//struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	struct ccid6_hc_tx_sock *hc = from_timer(hc, t, tx_rtotimer);
+	struct sock *sk = hc->sk;
+	const bool sender_was_blocked = ccid6_cwnd_network_limited(hc);
+
+	bh_lock_sock(sk);
+	if (sock_owned_by_user(sk)) {
+		sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + HZ / 5);
+		goto out;
+	}
+
+	if (sk->sk_state == DCCP_CLOSED)
+		goto out;
+
+	/* back-off timer */
+	hc->tx_rto <<= 1;
+	if (hc->tx_rto > DCCP_RTO_MAX)
+		hc->tx_rto = DCCP_RTO_MAX;
+
+	/* Mark loss */
+	bbr2_skb_marked_lost(sk, hc->tx_seqt); // is this enough? <---------------------------------
+
+	/* adjust pipe, cwnd etc */
+	hc->tx_ssthresh = bbr2_ssthresh(sk);
+	if (hc->tx_ssthresh < 2)
+		hc->tx_ssthresh = 2;
+
+	hc->lost += hc->tx_pipe; // not sure
+	hc->tx_cwnd	= 1; // not sure
+	hc->tx_pipe	= 0; // not sure
+	bbr2_set_state(sk, DCCP_CA_Loss);
+
+	/* clear state about stuff we sent */
+	hc->tx_seqt = hc->tx_seqh;
+
+	/* clear ack ratio state. */
+	hc->tx_rpseq    = 0;
+	hc->tx_rpdupack = -1;
+	ccid6_change_l_ack_ratio(sk, 1);
+
+	/* if we were blocked before, we may now send cwnd=1 packet */
+	if (sender_was_blocked)
+		dccp_tasklet_schedule(sk);
+	/* restart backed-off timer */
+	sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
+out:
+	bh_unlock_sock(sk);
+	sock_put(sk);
+}
+
+static int ccid6_hc_tx_send_packet(struct sock *sk, struct sk_buff *skb)
+{
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	if (!hc->pr_init) {
+		/* alerab: Pacing rate is initialized here, and only here.
+		 * This is so that pacing does not block outgoing acks when
+		 * data traffic is unidirectional.
+		*/
+		bbr_init_pacing_rate_from_rtt(sk, hc);
+	}
+	
+	ccid6_rate_check_app_limited(sk, skb->truesize); 
+	hc->bytes_att += skb->len;
+
+	/* Allow extra packet(s) to be sent during the drain phase */
+	if (hc->mode==BBR_PROBE_RTT && hc->tx_extrapkt) {
+		hc->tx_extrapkt = false;
+		return CCID_PACKET_SEND_AT_ONCE;
+	}
+
+	if (ccid6_cwnd_network_limited(hc))
+		return CCID_PACKET_WILL_DEQUEUE_LATER;
+	return CCID_PACKET_SEND_AT_ONCE;
+}
+
+
+static int ccid6_hc_tx_parse_options(struct sock *sk, u8 packet_type,
+					 u8 option, u8 *optval, u8 optlen)
+{
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+
+	switch (option) {
+	case DCCPO_ACK_VECTOR_0:
+	case DCCPO_ACK_VECTOR_1:
+		return dccp_ackvec_parsed_add(&hc->tx_av_chunks, optval, optlen,
+						  option - DCCPO_ACK_VECTOR_0);
+	}
+	return 0;
+}
+
+static void ccid6_hc_tx_packet_recv(struct sock *sk, struct sk_buff *skb)
+{
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	const bool sender_was_blocked = ccid6_cwnd_network_limited(hc);
+	struct dccp_ackvec_parsed *avp;
+	u64 ackno, seqno;
+	struct ccid6_seq *seqp;
+	int done = 0;
+	bool not_rst = 0;
+	unsigned int maxincr = 0;
+	struct rate_sample_ccid6 rs_i = { .prior_delivered = 0 };
+	struct rate_sample_ccid6 *rs = &rs_i;
+	u32 delivered = hc->delivered;
+	u32 lost = hc->lost;
+	u64 now_mstamp;
+	bool loss_event = false;
+
+	/* Get timestamp */
+	now_mstamp = dp->dccps_mstamp;
+
+	rs->prior_in_flight = hc->tx_pipe;
+
+	/* check reverse path congestion */
+	seqno = DCCP_SKB_CB(skb)->dccpd_seq;
+
+	/* XXX this whole "algorithm" is broken.  Need to fix it to keep track
+	 * of the seqnos of the dupacks so that rpseq and rpdupack are correct
+	 * -sorbo.
+	 */
+	/* need to bootstrap */
+	if (hc->tx_rpdupack == -1) {
+		hc->tx_rpdupack = 0;
+		hc->tx_rpseq    = seqno;
+	} else {
+		/* check if packet is consecutive */
+		if (dccp_delta_seqno(hc->tx_rpseq, seqno) == 1)
+			hc->tx_rpseq = seqno;
+		/* it's a later packet */
+		else if (after48(seqno, hc->tx_rpseq)) {
+			hc->tx_rpdupack++;
+
+			/* check if we got enough dupacks */
+			if (hc->tx_rpdupack >= NUMDUPACK) {
+				hc->tx_rpdupack = -1; /* XXX lame */
+				hc->tx_rpseq    = 0;
+#ifdef __CCID6_COPES_GRACEFULLY_WITH_ACK_CONGESTION_CONTROL__
+				/*
+				 * FIXME: Ack Congestion Control is broken; in
+				 * the current state instabilities occurred with
+				 * Ack Ratios greater than 1; causing hang-ups
+				 * and long RTO timeouts. This needs to be fixed
+				 * before opening up dynamic changes. -- gerrit
+				 */
+				ccid6_change_l_ack_ratio(sk, 2 * dp->dccps_l_ack_ratio);
+#endif
+			}
+		}
+	}
+
+	/* check forward path congestion */
+	if (dccp_packet_without_ack(skb)) {
+		return;
+	}
+
+	/* still didn't send out new data packets */
+	if (hc->tx_seqh == hc->tx_seqt) {
+		/* Is this the reason for the lockups? No, but it helps. */
+		goto done;
+	}
+
+	ackno = DCCP_SKB_CB(skb)->dccpd_ack_seq;
+	if (after48(ackno, hc->tx_high_ack))
+		hc->tx_high_ack = ackno;
+
+	seqp = hc->tx_seqt;
+	while (before48(seqp->ccid6s_seq, ackno)) {
+		seqp = seqp->ccid6s_next;
+		if (seqp == hc->tx_seqh) {
+			seqp = hc->tx_seqh->ccid6s_prev;
+			not_rst = 1;
+			break;
+		}
+	}
+
+	/*
+	 * In slow-start, cwnd can increase up to a maximum of Ack Ratio/2
+	 * packets per acknowledgement. Rounding up avoids that cwnd is not
+	 * advanced when Ack Ratio is 1 and gives a slight edge otherwise.
+	 */
+	if (hc->tx_cwnd < hc->tx_ssthresh)
+		maxincr = DIV_ROUND_UP(dp->dccps_l_ack_ratio, 2);
+
+	/* go through all ack vectors */
+	list_for_each_entry(avp, &hc->tx_av_chunks, node) {
+		/* go through this ack vector */
+		for (; avp->len--; avp->vec++) {
+			u64 ackno_end_rl = SUB48(ackno,
+						 dccp_ackvec_runlen(avp->vec));
+			/* if the seqno we are analyzing is larger than the
+			 * current ackno, then move towards the tail of our
+			 * seqnos.
+			 */
+			while (after48(seqp->ccid6s_seq, ackno)) {
+				if (seqp == hc->tx_seqt) {
+					done = 1;
+					break;
+				}
+				seqp = seqp->ccid6s_prev;
+			}
+			if (done)
+				break;
+
+			/* check all seqnos in the range of the vector
+			 * run length
+			 */
+			while (between48(seqp->ccid6s_seq,ackno_end_rl,ackno)) {
+				const u8 state = dccp_ackvec_state(avp->vec);
+
+				/* new packet received or marked */
+				if (state != DCCPAV_NOT_RECEIVED && !seqp->ccid6s_acked) {
+					if (state == DCCPAV_ECN_MARKED) {
+						hc->lost++; // technically not lost
+						ccid6_pr_debug("skb ecn marked!\n");
+					}
+					ccid6_rtt_estimator(sk, ccid6_jiffies32 - seqp->ccid6s_sent);
+					seqp->ccid6s_acked = 1;
+					hc->delivered++;
+					hc->tx_pipe--;	
+
+					bbr2_set_state(sk, DCCP_CA_Open); // is this needed ???
+
+					ccid6_rate_skb_delivered(sk, seqp, rs);
+					if (seqp->ccid6s_seq == ackno)	{ 
+						rs->rtt_us = tcp_stamp_us_delta(now_mstamp, seqp->sent_mstamp);
+						hc->rtt_us = rs->rtt_us;
+					}
+				}
+				if (seqp == hc->tx_seqt) {
+					done = 1;
+					break;
+				}
+				seqp = seqp->ccid6s_prev;
+			}
+			if (done)
+				break;
+
+			ackno = SUB48(ackno_end_rl, 1);
+		}
+		if (done)
+			break;
+	}
+
+	/* The state about what is acked should be correct now
+	 * Check for NUMDUPACK
+	 */
+	seqp = hc->tx_seqt;
+	while (before48(seqp->ccid6s_seq, hc->tx_high_ack)) {
+		seqp = seqp->ccid6s_next;
+		if (seqp == hc->tx_seqh) {
+			seqp = hc->tx_seqh->ccid6s_prev;
+			break;
+		}
+	}
+	done = 0;
+	while (1) {
+		if (seqp->ccid6s_acked) {
+			done++;
+			if (done == NUMDUPACK)
+				break;
+		}
+		if (seqp == hc->tx_seqt)
+			break;
+		seqp = seqp->ccid6s_prev;
+	}
+
+	/* If there are at least 3 acknowledgements, anything unacknowledged
+	 * below the last sequence number is considered lost
+	 */
+	if (done == NUMDUPACK) {
+		struct ccid6_seq *last_acked = seqp;
+
+		/* check for lost packets */
+		while (1) {
+			if (!seqp->ccid6s_acked) {
+				ccid6_pr_debug("Packet lost: %llu\n",
+						   (unsigned long long)seqp->ccid6s_seq);
+				/* XXX need to traverse from tail -> head in
+				 * order to detect multiple congestion events in
+				 * one ack vector.
+				 */
+
+				bbr2_skb_marked_lost(sk, seqp);
+				loss_event = true;
+
+				hc->lost++;
+				hc->tx_pipe--;
+			}
+			if (seqp == hc->tx_seqt)
+				break;
+			seqp = seqp->ccid6s_prev;
+		}
+
+		hc->tx_seqt = last_acked;
+	}
+
+	if (loss_event) {
+		hc->tx_ssthresh = bbr2_ssthresh(sk);
+		if (hc->tx_ssthresh < 2)
+			hc->tx_ssthresh = 2;
+
+		if (hc->mode != BBR_STARTUP)
+			bbr2_set_state(sk, DCCP_CA_Recovery); // alerab: or DCCP_CA_Loss?
+	}
+
+	/* trim acked packets in tail */
+	while (hc->tx_seqt != hc->tx_seqh) {
+		if (!hc->tx_seqt->ccid6s_acked)
+			break;
+
+		hc->tx_seqt = hc->tx_seqt->ccid6s_next;
+	}
+
+	/* restart RTO timer if not all outstanding data has been acked */
+	if (hc->tx_pipe == 0) {
+		sk_stop_timer(sk, &hc->tx_rtotimer);
+	}
+	else if (!not_rst)
+		sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
+	delivered = hc->delivered - delivered;
+	lost = hc->lost - lost;	
+	ccid6_rate_gen(sk, delivered, lost, now_mstamp, rs);
+
+	bbr2_main(sk, rs);
+done:
+	/* check if incoming Acks allow pending packets to be sent */
+	if (sender_was_blocked && !ccid6_cwnd_network_limited(hc))
+		dccp_tasklet_schedule(sk);
+	dccp_ackvec_parsed_cleanup(&hc->tx_av_chunks);
+}
+
+static int ccid6_hc_tx_init(struct ccid *ccid, struct sock *sk)
+{
+	struct ccid6_hc_tx_sock *hc = ccid_priv(ccid);
+	struct dccp_sock *dp = dccp_sk(sk);
+	u32 max_ratio;
+	ccid6_pr_debug("init ccid6 sk %p", sk);
+
+	/* RFC 4341, 5: initialise ssthresh to arbitrarily high (max) value */
+	hc->tx_ssthresh = ~0U;
+
+	/* Use larger initial windows (RFC 4341, section 5). */
+	hc->tx_cwnd = 10;
+	hc->tx_expected_wnd = hc->tx_cwnd;
+
+	/* Make sure that Ack Ratio is enabled and within bounds. */
+	max_ratio = DIV_ROUND_UP(hc->tx_cwnd, 2);
+	if (dp->dccps_l_ack_ratio == 0 || dp->dccps_l_ack_ratio > max_ratio)
+		dp->dccps_l_ack_ratio = max_ratio;
+
+	/* XXX init ~ to window size... */
+	if (ccid6_hc_tx_alloc_seq(hc))
+		return -ENOMEM;
+
+	hc->tx_rto	 = DCCP_TIMEOUT_INIT;
+	hc->tx_rpdupack  = -1;
+	hc->tx_last_cong = hc->tx_lsndtime = hc->tx_cwnd_stamp = ccid6_jiffies32;
+	hc->tx_cwnd_used = 0;
+	hc->tx_pipe = 0;
+
+	hc->curr_ca_state = DCCP_CA_Open;
+	
+	hc->pr_init = 0;
+	//hc->rtprop_fix=0;
+	hc->tx_extrapkt=false;
+	
+	hc->restore_cwnd = 0;
+	hc->restore_ackrt = 0;
+	hc->restore_seqwin = 0;
+
+	hc->delivered = 0;
+	hc->delivered_ce = 0;
+
+	bbr2_init(sk, hc);
+
+	hc->sk		 = sk;
+	timer_setup(&hc->tx_rtotimer, ccid6_hc_tx_rto_expire, 0);
+	//setup_timer(&hc->tx_rtotimer, ccid6_hc_tx_rto_expire,
+	//		(unsigned long)sk);
+	INIT_LIST_HEAD(&hc->tx_av_chunks);
+
+	return 0;
+}
+
+static void ccid6_hc_tx_exit(struct sock *sk)
+{
+	struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	int i;
+
+	sk_stop_timer(sk, &hc->tx_rtotimer);
+
+	for (i = 0; i < hc->tx_seqbufc; i++)
+		kfree(hc->tx_seqbuf[i]);
+	hc->tx_seqbufc = 0;
+	dccp_ackvec_parsed_cleanup(&hc->tx_av_chunks);
+}
+
+static void ccid6_hc_rx_packet_recv(struct sock *sk, struct sk_buff *skb)
+{
+	//ccid6_pr_debug("enter %p", sk);
+
+	struct ccid6_hc_rx_sock *hc = ccid6_hc_rx_sk(sk);
+	
+	//ccid6_mstamp_refresh(ccid6_hc_tx_sk(sk)); // or after the first if-statement?
+
+	if (!dccp_data_packet(skb))
+		return;
+	if (++hc->rx_num_data_pkts >= dccp_sk(sk)->dccps_r_ack_ratio) {
+		//ccid6_pr_debug("%p send ack, ack ratio %ul", sk, dccp_sk(sk)->dccps_r_ack_ratio);
+		dccp_send_ack(sk);
+		hc->rx_num_data_pkts = 0;
+	}
+}
+
+// Function to read h values and make them available for dccp
+static void ccid6_hc_tx_get_info(struct sock *sk, struct tcp_info *info)
+{
+	info->tcpi_rto = ccid6_hc_tx_sk(sk)->tx_rto;
+	info->tcpi_rtt = ccid6_hc_tx_sk(sk)->tx_srtt;
+	info->tcpi_rttvar = ccid6_hc_tx_sk(sk)->tx_mrtt;
+	info->tcpi_segs_out = ccid6_hc_tx_sk(sk)->tx_pipe;
+	info->tcpi_snd_cwnd = ccid6_hc_tx_sk(sk)->tx_cwnd;
+	info->tcpi_last_data_sent = ccid6_hc_tx_sk(sk)->tx_lsndtime;
+}
+
+// NOTE: #define DCCP_SOCKOPT_CCID_TX_INFO 192 in include/uapi/linux/dccp.h
+// NOTE: #define DCCP_SOCKOPT_CCID_LIM_RTO 193 in include/uapi/linux/dccp.h
+
+struct dccp_ccid6_tx { // Pieska modification, added struct
+  u32 tx_cwnd;	
+  u32 tx_pipe;	
+  u32 tx_srtt;	
+  u32 tx_mrtt;	
+  u32 tx_rto;
+  u32 tx_min_rtt;		
+  u32 tx_delivered;	
+};
+
+// Pieska modification, added function
+static int ccid6_hc_tx_getsockopt(struct sock *sk, const int optname, int len,
+				  u32 __user *optval, int __user *optlen)
+{
+  struct ccid6_hc_tx_sock *hc = ccid6_hc_tx_sk(sk);
+	struct dccp_ccid6_tx tx;
+	const void *val;
+
+	switch (optname) {
+	case DCCP_SOCKOPT_CCID_TX_INFO:
+		if (len < sizeof(tx))
+			return -EINVAL;
+		memset(&tx, 0, sizeof(tx));
+		tx.tx_cwnd = hc->tx_cwnd;
+		tx.tx_pipe = hc->tx_pipe;
+		tx.tx_srtt = hc->tx_srtt;
+		tx.tx_mrtt = hc->tx_mrtt;
+		tx.tx_rto = hc->tx_rto;
+		tx.tx_min_rtt = hc->min_rtt_us;
+		tx.tx_delivered = hc->delivered;
+		len = sizeof(tx);
+		val = &tx;
+		break;
+	case DCCP_SOCKOPT_CCID_LIM_RTO:
+        hc->exp_inc_rtotimer = 0;
+        break;
+	default:
+		return -ENOPROTOOPT;
+	}
+
+	if (put_user(len, optlen) || copy_to_user(optval, val, len))
+		return -EFAULT;
+
+	return 0;
+}
+
+struct ccid_operations ccid6_ops = {
+	.ccid_id		  			= DCCPC_CCID6,
+	.ccid_name		  			= "BBRv2-like",
+	.ccid_hc_tx_obj_size		= sizeof(struct ccid6_hc_tx_sock),
+	.ccid_hc_tx_init	  		= ccid6_hc_tx_init,
+	.ccid_hc_tx_exit	  		= ccid6_hc_tx_exit,
+	.ccid_hc_tx_send_packet		= ccid6_hc_tx_send_packet,
+	.ccid_hc_tx_packet_sent		= ccid6_hc_tx_packet_sent,
+	.ccid_hc_tx_parse_options	= ccid6_hc_tx_parse_options,
+	.ccid_hc_tx_packet_recv		= ccid6_hc_tx_packet_recv,
+	.ccid_hc_tx_get_info 		= ccid6_hc_tx_get_info,
+	.ccid_hc_rx_obj_size	  	= sizeof(struct ccid6_hc_rx_sock),
+	.ccid_hc_rx_packet_recv	  	= ccid6_hc_rx_packet_recv,
+  .ccid_hc_tx_getsockopt	  = ccid6_hc_tx_getsockopt, // Pieska modification, added operation
+};
+
+#ifdef CONFIG_IP_DCCP_CCID6_DEBUG
+module_param(ccid6_debug, bool, 0644);
+MODULE_PARM_DESC(ccid6_debug, "Enable CCID-6 debug messages");
+#endif
diff --git a/net/dccp/ccids/ccid6.h b/net/dccp/ccids/ccid6.h
new file mode 100644
index 0000000000000..f0b22a6f830a7
--- /dev/null
+++ b/net/dccp/ccids/ccid6.h
@@ -0,0 +1,345 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ *  Copyright (C) 2022 by Alexander Rabitsch, Karlstad University for Deutsche Telekom AG
+ *
+ *  This code is a version of the BBR algorithm for the DCCP protocol.
+ *	Due to that, it copies and adapts as much code as possible from 
+ *	net/ipv4/tcp_bbr.c, net/ipv4/tcp_rate.c, net/dccp/ccids/ccid5.c, 
+ *	and net/dccp/ccids/ccid2.c
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef _DCCP_CCID6_H_
+#define _DCCP_CCID6_H_
+
+#include <linux/timer.h>
+#include <linux/types.h>
+#include "../ccid.h"
+#include "../dccp.h"
+#include <linux/win_minmax.h>
+
+/*
+ * CCID-6 timestamping faces the same issues as TCP timestamping.
+ * Hence we reuse/share as much of the code as possible.
+ */
+#define ccid6_jiffies32	((u32)jiffies)
+
+/* NUMDUPACK parameter from RFC 4341, p. 6 */
+#define NUMDUPACK	3
+
+
+#define CYCLE_LEN	8
+
+struct ccid6_seq {
+	u64			ccid6s_seq;
+	u32			ccid6s_sent,
+				delivered,
+				delivered_ce;
+	int			ccid6s_acked;
+	bool 		is_app_limited;
+	//struct skb_mstamp	 sent_mstamp;
+	u64			sent_mstamp;
+	//struct skb_mstamp	 first_tx_mstamp;
+	u64			first_tx_mstamp;
+	//struct skb_mstamp	 delivered_mstamp;
+	u64			delivered_mstamp;
+
+    u32         in_flight;
+    u32         lost;	/* packets lost so far upon tx of skb */
+
+	struct ccid6_seq	*ccid6s_prev;
+	struct ccid6_seq	*ccid6s_next;
+};
+
+#define CCID6_SEQBUF_LEN 1024
+#define CCID6_SEQBUF_MAX 128
+
+/*
+ * Multiple of congestion window to keep the sequence window at
+ * (RFC 4340 7.5.2)
+ */
+#define CCID6_WIN_CHANGE_FACTOR 5
+
+struct ccid6_hc_tx_sock {
+	u32			tx_cwnd;
+	u32			tx_ssthresh;
+	u32			tx_pipe;
+	struct ccid6_seq	*tx_seqbuf[CCID6_SEQBUF_MAX];
+	int			tx_seqbufc;
+	struct ccid6_seq	*tx_seqh;
+	struct ccid6_seq	*tx_seqt;
+    u32 exp_inc_rtotimer;
+
+
+	/* RTT measurement: variables/principles are the same as in TCP */
+	u32			tx_srtt,
+				tx_mrtt,
+				tx_mdev,
+				tx_mdev_max,
+				tx_rttvar,
+				tx_rto;
+	u64			tx_rtt_seq:48;
+	struct timer_list	tx_rtotimer;
+	struct sock		*sk;
+
+	/* Congestion Window validation (optional, RFC 2861) */
+	u32			tx_cwnd_used,
+				tx_expected_wnd,
+				tx_cwnd_stamp,
+				tx_lsndtime;
+
+	u64			tx_rpseq;
+	int			tx_rpdupack;
+	u32			tx_last_cong;
+	u64			tx_high_ack;
+	struct list_head	tx_av_chunks;
+
+	u32                     rtt_us;
+
+	/* Rate sample population for BBR */
+	//struct skb_mstamp	 first_tx_mstamp;
+	u64			first_tx_mstamp;
+	//struct skb_mstamp	 delivered_mstamp;
+	u64			delivered_mstamp;
+	u32			delivered,
+                delivered_ce,
+				app_limited;
+
+    /* variables of BBR struct */
+	u32			lt_bw;
+	u32			lt_last_delivered;   /* LT intvl start: tp->delivered */
+	u32			lt_last_stamp;
+	u32			lt_last_lost;
+	//u32			pacing_gain:10,	/* current gain for setting pacing rate */
+	//			cwnd_gain:10,	/* current gain for setting cwnd */
+	//			full_bw_reached:1,   /* reached full bw in Startup? */
+	//			full_bw_cnt:2,	/* number of rounds without large bw gains */
+	//			cycle_idx:3,	/* current index in pacing_gain cycle array */
+	//			has_seen_rtt:1, /* have we seen an RTT sample yet? */
+	//			unused_b:5;
+	//u32			next_rtt_delivered;
+	//struct skb_mstamp cycle_mstamp;
+	//u64			cycle_mstamp;
+	//u32			rtt_cnt;
+	struct minmax bw;
+	//u32			prior_cwnd;	/* prior cwnd upon entering loss recovery */
+	//u32			full_bw;	/* recent bw, to estimate if pipe is full */
+	u32 		bytes_att;
+	u32 		bytes_sent;
+	u32			curr_ca_state; 
+	bool		pr_init;
+	//bool		rtprop_fix;
+	u32			lost;
+	bool			tx_extrapkt;
+	u64			prior_ackrt;
+	u64			prior_seqwin;
+
+    /* New in BBRv2 */
+    	u32	min_rtt_us;	        /* min RTT in min_rtt_win_sec window */
+	u32	min_rtt_stamp;	        /* timestamp of min_rtt_us */
+	u32	probe_rtt_done_stamp;   /* end time for BBR_PROBE_RTT mode */
+	u32	probe_rtt_min_us;	/* min RTT in bbr_probe_rtt_win_ms window */
+	u32	probe_rtt_min_stamp;	/* timestamp of probe_rtt_min_us*/
+	u32     next_rtt_delivered; /* scb->tx.delivered at end of round */
+	u32	prior_rcv_nxt;	/* tp->rcv_nxt when CE state last changed */
+	u64	cycle_mstamp;	     /* time of this cycle phase start */
+	u32     mode:3,		     /* current bbr_mode in state machine */
+		prev_ca_state:3,     /* CA state on previous ACK */
+		packet_conservation:1,  /* use packet conservation? */
+    	restore_cwnd:1,	     /* decided to revert cwnd to old value */
+		restore_ackrt:1,     /* decided to revert ack_ratio to old value */
+		restore_seqwin:1,    /* decided to revert seq_window to old value */
+		round_start:1,	     /* start of packet-timed tx->ack round? */
+        //tso_segs_goal:7,     /* segments we want in each skb we send */
+		ce_state:1,          /* If most recent data has CE bit set */
+		bw_probe_up_rounds:5,   /* cwnd-limited rounds in PROBE_UP */
+		try_fast_path:1, 	/* can we take fast path? */
+		unused2:8,
+		idle_restart:1,	     /* restarting after idle? */
+		probe_rtt_round_done:1,  /* a BBR_PROBE_RTT round at 4 pkts? */
+		cycle_idx:3,	/* current index in pacing_gain cycle array */
+		has_seen_rtt:1;	     /* have we seen an RTT sample yet? */
+	u32	pacing_gain:11,	/* current gain for setting pacing rate */
+		cwnd_gain:11,	/* current gain for setting cwnd */
+		full_bw_reached:1,   /* reached full bw in Startup? */
+		full_bw_cnt:2,	/* number of rounds without large bw gains */
+		init_cwnd:7;	/* initial cwnd */
+	u32	prior_cwnd;	/* prior cwnd upon entering loss recovery */
+	u32	full_bw;	/* recent bw, to estimate if pipe is full */
+
+	/* For tracking ACK aggregation: */
+	u64	ack_epoch_mstamp;	/* start of ACK sampling epoch */
+	u16	extra_acked[2];		/* max excess data ACKed in epoch */
+	u32	ack_epoch_acked:20,	/* packets (S)ACKed in sampling epoch */
+		extra_acked_win_rtts:5,	/* age of extra_acked, in round trips */
+		extra_acked_win_idx:1,	/* current index in extra_acked array */
+	/* BBR v2 state: */
+		unused1:2,
+		startup_ecn_rounds:2,	/* consecutive hi ECN STARTUP rounds */
+		loss_in_cycle:1,	/* packet loss in this cycle? */
+		ecn_in_cycle:1;		/* ECN in this cycle? */
+	u32	loss_round_delivered; /* scb->tx.delivered ending loss round */
+	u32	undo_bw_lo;	     /* bw_lo before latest losses */
+	u32	undo_inflight_lo;    /* inflight_lo before latest losses */
+	u32	undo_inflight_hi;    /* inflight_hi before latest losses */
+	u32	bw_latest;	 /* max delivered bw in last round trip */
+	u32	bw_lo;		 /* lower bound on sending bandwidth */
+	u32	bw_hi[2];	 /* upper bound of sending bandwidth range*/
+	u32	inflight_latest; /* max delivered data in last round trip */
+	u32	inflight_lo;	 /* lower bound of inflight data range */
+	u32	inflight_hi;	 /* upper bound of inflight data range */
+	u32	bw_probe_up_cnt; /* packets delivered per inflight_hi incr */
+	u32	bw_probe_up_acks;  /* packets (S)ACKed since inflight_hi incr */
+	u32	probe_wait_us;	 /* PROBE_DOWN until next clock-driven probe */
+	u32	ecn_eligible:1,	/* sender can use ECN (RTT, handshake)? */
+		ecn_alpha:9,	/* EWMA delivered_ce/delivered; 0..256 */
+		bw_probe_samples:1,    /* rate samples reflect bw probing? */
+		prev_probe_too_high:1, /* did last PROBE_UP go too high? */
+		stopped_risky_probe:1, /* last PROBE_UP stopped due to risk? */
+		rounds_since_probe:8,  /* packet-timed rounds since probed bw */
+		loss_round_start:1,    /* loss_round_delivered round trip? */
+		loss_in_round:1,       /* loss marked in this round trip? */
+		ecn_in_round:1,	       /* ECN marked in this round trip? */
+		ack_phase:3,	       /* bbr_ack_phase: meaning of ACKs */
+		loss_events_in_round:4,/* losses in STARTUP round */
+		initialized:1;	       /* has bbr_init() been called? */
+	u32	alpha_last_delivered;	 /* tp->delivered    at alpha update */
+	u32	alpha_last_delivered_ce; /* tp->delivered_ce at alpha update */
+
+
+	/* Params configurable using setsockopt. Refer to correspoding
+	 * module param for detailed description of params.
+	 */
+	struct ccid6_params {
+		u32	high_gain:11,		/* max allowed value: 2047 */
+			drain_gain:10,		/* max allowed value: 1023 */
+			cwnd_gain:11;		/* max allowed value: 2047 */
+		u32	cwnd_min_target:4,	/* max allowed value: 15 */
+			min_rtt_win_sec:5,	/* max allowed value: 31 */
+			probe_rtt_mode_ms:9,	/* max allowed value: 511 */
+			full_bw_cnt:3,		/* max allowed value: 7 */
+            bw_rtts:5,          
+			cwnd_tso_budget:1,	/* allowed values: {0, 1} */
+			unused3:1,
+			drain_to_target:1,	/* boolean */
+			precise_ece_ack:1,	/* boolean */
+			extra_acked_in_startup:1, /* allowed values: {0, 1} */
+			fast_path:1;		/* boolean */
+		u32	full_bw_thresh:10,	/* max allowed value: 1023 */
+			startup_cwnd_gain:11,	/* max allowed value: 2047 */
+			bw_probe_pif_gain:9,	/* max allowed value: 511 */
+			usage_based_cwnd:1, 	/* boolean */
+			unused2:1;
+		u16	probe_rtt_win_ms:14,	/* max allowed value: 16383 */
+			refill_add_inc:2;	/* max allowed value: 3 */
+		u16	extra_acked_gain:11,	/* max allowed value: 2047 */
+			extra_acked_win_rtts:5; /* max allowed value: 31*/
+		u16	pacing_gain[CYCLE_LEN]; /* max allowed value: 1023 */
+		/* Mostly BBR v2 parameters below here: */
+		u32	ecn_alpha_gain:8,	/* max allowed value: 255 */
+			ecn_factor:8,		/* max allowed value: 255 */
+			ecn_thresh:8,		/* max allowed value: 255 */
+			beta:8;			/* max allowed value: 255 */
+		u32	ecn_max_rtt_us:19,	/* max allowed value: 524287 */
+			bw_probe_reno_gain:9,	/* max allowed value: 511 */
+			full_loss_cnt:4;	/* max allowed value: 15 */
+		u32	probe_rtt_cwnd_gain:8,	/* max allowed value: 255 */
+			inflight_headroom:8,	/* max allowed value: 255 */
+			loss_thresh:8,		/* max allowed value: 255 */
+			bw_probe_max_rounds:8;	/* max allowed value: 255 */
+		u32	bw_probe_rand_rounds:4, /* max allowed value: 15 */
+			bw_probe_base_us:26,	/* usecs: 0..2^26-1 (67 secs) */
+			full_ecn_cnt:2;		/* max allowed value: 3 */
+		u32	bw_probe_rand_us:26,	/* usecs: 0..2^26-1 (67 secs) */
+			undo:1,			/* boolean */
+		tso_rtt_shift:4,	/* max allowed value: 15 */
+			unused5:1;
+		u32	ecn_reprobe_gain:9,	/* max allowed value: 511 */
+			unused1:14,
+			ecn_alpha_init:9;	/* max allowed value: 256 */
+	} params;
+
+	struct {
+		u32	snd_isn; 
+		u32	rs_bw; 
+		u32	target_cwnd; 
+		u8	undo:1,
+			unused:7;
+		char event;
+		u16	unused2;
+	} debug;
+};
+
+struct rate_sample_ccid6 {
+	//u32	prior_mstamp;
+	//struct	skb_mstamp prior_mstamp; /* starting timestamp for interval */
+	u64  prior_mstamp;
+	u32  prior_delivered;	/* tp->delivered at "prior_mstamp" */
+	s32  delivered;		/* number of packets delivered over interval */
+	s64  interval_us;
+	//long interval_us;	/* time for tp->delivered to incr "delivered" */
+	//long rtt_us;		/* RTT of last (S)ACKed packet (or -1) */
+	u32 rtt_us;
+	int  losses;		/* number of packets marked lost upon ACK  */
+	u32  acked_sacked;	/* number of packets newly (S)ACKed upon ACK */
+	u32  prior_in_flight;	/* in flight before this ACK */
+	bool is_app_limited;	/* is sample from packet with bubble in pipe? */
+	bool is_retrans;	/* is sample from retransmission? */
+	bool is_ack_delayed;
+
+	bool is_ece;
+	u32 lost;
+	u32 prior_delivered_ce;
+	u32 delivered_ce;
+  	u32 tx_in_flight;
+};
+
+static inline bool ccid6_cwnd_network_limited(struct ccid6_hc_tx_sock *hc)
+{
+	return hc->tx_pipe >= hc->tx_cwnd;
+}
+
+/*
+ * Convert RFC 3390 larger initial window into an equivalent number of packets.
+ * This is based on the numbers specified in RFC 5681, 3.1.
+ */
+static inline u32 ccid6_rfc3390_bytes_to_pkts(const u32 smss)
+{
+	return smss <= 1095 ? 4 : (smss > 2190 ? 2 : 3);
+}
+
+/**
+ * struct ccid6_hc_rx_sock  -  Receiving end of CCID6 half-connection
+ * @rx_num_data_pkts: number of data packets received since last feedback
+ */
+struct ccid6_hc_rx_sock {
+	u32	rx_num_data_pkts;
+};
+
+static inline struct ccid6_hc_tx_sock *ccid6_hc_tx_sk(const struct sock *sk)
+{
+	return ccid_priv(dccp_sk(sk)->dccps_hc_tx_ccid);
+}
+
+static inline struct ccid6_hc_rx_sock *ccid6_hc_rx_sk(const struct sock *sk)
+{
+	return ccid_priv(dccp_sk(sk)->dccps_hc_rx_ccid);
+}
+#endif /* _DCCP_CCID6_H_ */
diff --git a/net/dccp/ccids/ccid7.c b/net/dccp/ccids/ccid7.c
new file mode 100755
index 0000000000000..45b215ec5b6d6
--- /dev/null
+++ b/net/dccp/ccids/ccid7.c
@@ -0,0 +1,1082 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ *  Copyright (C) 2022 by Marcus Pieska, Karlstad University for Deutsche Telekom AG
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ * 
+ * CUBIC integrates a new slow start algorithm, called HyStart.
+ * The details of HyStart are presented in
+ *  Sangtae Ha and Injong Rhee,
+ *  "Taming the Elephants: New TCP Slow Start", NCSU TechReport 2008.
+ * Available from:
+ *  http://netsrv.csc.ncsu.edu/export/hystart_techreport_2008.pdf
+ *
+ * All testing results are available from:
+ * http://netsrv.csc.ncsu.edu/wiki/index.php/TCP_Testing
+ *
+ * Unless CUBIC is enabled and congestion window is large
+ * this behaves the same as the original Reno.
+ */
+
+/*
+ * This implementation should follow RFC 4341
+ */
+#include <linux/slab.h>
+#include "../feat.h"
+#include "ccid7.h"
+
+
+#ifdef CONFIG_IP_DCCP_CCID7_DEBUG
+static bool ccid7_debug;
+#define ccid7_pr_debug(format, a...)	DCCP_PR_DEBUG(ccid7_debug, format, ##a)
+#else
+#define ccid7_pr_debug(format, a...)
+#endif
+
+/* Function pointer to either get SRTT or MRTT ...*/
+u32 (*ccid7_get_delay_val)(struct ccid7_hc_tx_sock *hc) = ccid7_mrtt_as_delay;
+EXPORT_SYMBOL_GPL(ccid7_get_delay_val);
+
+#define HYSTART_DELAY	0x1
+#define HYSTART_MIN_SAMPLES	8
+#define HYSTART_DELAY_MIN	(4U)
+#define HYSTART_DELAY_MAX	(16U)
+#define HYSTART_DELAY_THRESH(x)	clamp(x, HYSTART_DELAY_MIN, HYSTART_DELAY_MAX)
+
+#define CSS_GROWTH_DIVISOR 4 
+#define CSS_ROUNDS 5
+
+static int hystart = 1;
+static int hystart_detect = HYSTART_DELAY;
+static int hystart_low_window = 16;
+
+const static s32 cub_shift = 10;
+const static s32 cub_fact = 1024;
+const static s32 cub_b = 717; // 0.7 * 1024
+const static s32 cub_c = 410; // 0.4 * 1024
+
+static inline u32 bictcp_clock (void) 
+{
+#if HZ < 1000
+	return ktime_to_ms (ktime_get_real());
+#else
+	return jiffies_to_msecs (jiffies);
+#endif
+}
+
+static u32 in_slow_start(struct sock *sk)
+{
+  struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+  return hc->tx_cwnd < hc->tx_ssthresh;  
+}
+
+static u32 cub_root(u64 a) 
+{
+	u32 x, b, shift;
+	static const u8 v[] = {
+		/* 0x00 */    0,   54,   54,   54,  118,  118,  118,  118,
+		/* 0x08 */  123,  129,  134,  138,  143,  147,  151,  156,
+		/* 0x10 */  157,  161,  164,  168,  170,  173,  176,  179,
+		/* 0x18 */  181,  185,  187,  190,  192,  194,  197,  199,
+		/* 0x20 */  200,  202,  204,  206,  209,  211,  213,  215,
+		/* 0x28 */  217,  219,  221,  222,  224,  225,  227,  229,
+		/* 0x30 */  231,  232,  234,  236,  237,  239,  240,  242,
+		/* 0x38 */  244,  245,  246,  248,  250,  251,  252,  254,
+	};
+
+	b = fls64 (a);
+	if (b < 7) {
+		return ((u32)v[(u32)a] + 35) >> 6;
+	}
+	b = ((b * 84) >> 8) - 1;
+	shift = (a >> (b * 3));
+	x = ((u32)(((u32)v[shift] + 10) << b)) >> 6;
+	x = (2 * x + (u32)div64_u64(a, (u64)x * (u64)(x - 1)));
+	x = ((x * 341) >> 10);
+	return x;
+}
+
+static inline u32 get_w_est_rfc8312 (struct ccid7_hc_tx_sock* hc, s64 time) 
+{
+  /* W_est(t) = W_max*beta_cubic + [3*(1-beta_cubic)/(1+beta_cubic)] * (t/RTT) */
+  u32 rtts = (time - hc->ref_t) / ((hc->tx_srtt) >> 1);
+  u32 ai = 3*cub_fact*(cub_fact-cub_b)/(cub_fact+cub_b);
+  u32 w_est = hc->tx_wmax*cub_b + ai*rtts;
+  return w_est >>= cub_shift;
+}
+
+static inline u32 get_cwnd_rfc8312 (struct ccid7_hc_tx_sock *hc, s64 time) 
+{
+  /* W_cubic(t) = C*(t-K)^3 + W_max (Eq. 1) */
+  s64 tv = (time - hc->ref_t - hc->cub_k), raw_cwnd;
+  raw_cwnd = (cub_b * tv) >> cub_shift; 
+  raw_cwnd = (raw_cwnd * tv) >> cub_shift; 
+  raw_cwnd = (raw_cwnd * tv) >> cub_shift; 
+  raw_cwnd = (raw_cwnd) >> cub_shift;
+  raw_cwnd = raw_cwnd + hc->tx_wmax;
+  if (raw_cwnd < 4)
+    raw_cwnd = 4;
+  return (u32)raw_cwnd;
+} 
+
+static inline void set_cwnd_rfc8312 (struct ccid7_hc_tx_sock* hc) 
+{
+  s64 time = bictcp_clock();
+  u32 w_est = get_w_est_rfc8312 (hc, time);
+  u32 cwnd = get_cwnd_rfc8312 (hc, time);
+  hc->tx_cwnd = max(w_est, cwnd);
+}
+
+static inline void set_k_rfc8312 (struct ccid7_hc_tx_sock* hc) 
+{
+  /* K = cub_root(W_max*(1-beta_cubic)/C) */
+  hc->cub_k = cub_root (hc->tx_wmax*(cub_fact-cub_b)/cub_c);
+  hc->cub_k <<= cub_shift;
+} 
+
+static inline void bictcp_hystart_reset(struct sock *sk, struct ccid7_hc_tx_sock *hc)
+{
+  struct dccp_sock *dp = dccp_sk(sk);
+  
+  /* have previously found, now in css */
+  if (hc->found != 0) {
+    hc->css_round_count++;
+    if (hc->curr_rtt < hc->prev_rtt) {
+      hc->found = hc->css_round_count = 0;
+    }  
+    else if (hc->css_round_count > CSS_ROUNDS) {
+      hc->found = hystart_detect;
+      hc->ref_t = bictcp_clock();
+      hc->css_round_count = 0;  
+      hc->css_pkt_count = 0;  
+      hc->cub_k = 0;
+      hc->tx_wmax = hc->tx_cwnd; 
+      hc->tx_ssthresh = hc->tx_cwnd;
+      set_cwnd_rfc8312 (hc);
+    }
+  }
+
+  hc->round_start = hc->last_ack = bictcp_clock();
+	hc->end_seq = dp->dccps_gss + hc->tx_cwnd;
+	hc->prev_rtt = hc->curr_rtt;
+  hc->curr_rtt = ~0U;
+	hc->sample_cnt = 0;
+}
+
+static void hystart_update(struct sock *sk, u32 delay)
+{
+  struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+
+  /* obtain the minimum delay of more than sampling packets */
+	if (hystart_detect & HYSTART_DELAY) {
+		if (hc->sample_cnt < HYSTART_MIN_SAMPLES) {
+			hc->sample_cnt++;
+      if (hc->curr_rtt > delay)
+				hc->curr_rtt = delay;
+		} else if (hc->prev_rtt > 0) {
+			u32 n = max((hc->prev_rtt >> 4), 2U);
+      if (hc->curr_rtt > hc->prev_rtt + n) {
+				hc->found |= HYSTART_DELAY;
+			}
+		}
+	}
+  
+  /* have newly found, need to end cycle and begin css */
+  if (hc->found != 0 && hc->css_round_count == 0) {
+    bictcp_hystart_reset(sk, hc);
+  }
+}
+
+static int ccid7_hc_tx_alloc_seq(struct ccid7_hc_tx_sock *hc)
+{
+	struct ccid7_seq *seqp;
+	int i;
+
+	/* check if we have space to preserve the pointer to the buffer */
+	if (hc->tx_seqbufc >= (sizeof(hc->tx_seqbuf) /
+			       sizeof(struct ccid7_seq *)))
+		return -ENOMEM;
+
+	/* allocate buffer and initialize linked list */
+	seqp = kmalloc(CCID7_SEQBUF_LEN * sizeof(struct ccid7_seq), gfp_any());
+	if (seqp == NULL)
+		return -ENOMEM;
+
+	for (i = 0; i < (CCID7_SEQBUF_LEN - 1); i++) {
+		seqp[i].ccid7s_next = &seqp[i + 1];
+		seqp[i + 1].ccid7s_prev = &seqp[i];
+	}
+	seqp[CCID7_SEQBUF_LEN - 1].ccid7s_next = seqp;
+	seqp->ccid7s_prev = &seqp[CCID7_SEQBUF_LEN - 1];
+
+	/* This is the first allocation.  Initiate the head and tail.  */
+	if (hc->tx_seqbufc == 0)
+		hc->tx_seqh = hc->tx_seqt = seqp;
+	else {
+		/* link the existing list with the one we just created */
+		hc->tx_seqh->ccid7s_next = seqp;
+		seqp->ccid7s_prev = hc->tx_seqh;
+
+		hc->tx_seqt->ccid7s_prev = &seqp[CCID7_SEQBUF_LEN - 1];
+		seqp[CCID7_SEQBUF_LEN - 1].ccid7s_next = hc->tx_seqt;
+	}
+
+	/* store the original pointer to the buffer so we can free it */
+	hc->tx_seqbuf[hc->tx_seqbufc] = seqp;
+	hc->tx_seqbufc++;
+
+	return 0;
+}
+
+static int ccid7_hc_tx_send_packet(struct sock *sk, struct sk_buff *skb)
+{
+	if (ccid7_cwnd_network_limited(ccid7_hc_tx_sk(sk)))
+		return CCID_PACKET_WILL_DEQUEUE_LATER;
+	return CCID_PACKET_SEND_AT_ONCE;
+}
+
+static void ccid7_change_l_ack_ratio(struct sock *sk, u32 val)
+{
+	u32 max_ratio = DIV_ROUND_UP(ccid7_hc_tx_sk(sk)->tx_cwnd, 2);
+
+	/*
+	 * Ensure that Ack Ratio does not exceed ceil(cwnd/2), which is (2) from
+	 * RFC 4341, 6.1.2. We ignore the statement that Ack Ratio 2 is always
+	 * acceptable since this causes starvation/deadlock whenever cwnd < 2.
+	 * The same problem arises when Ack Ratio is 0 (ie. Ack Ratio disabled).
+	 */
+	if (val == 0 || val > max_ratio) {
+		DCCP_WARN("Limiting Ack Ratio (%u) to %u\n", val, max_ratio);
+		val = max_ratio;
+	}
+	dccp_feat_signal_nn_change(sk, DCCPF_ACK_RATIO,
+				   min_t(u32, val, DCCPF_ACK_RATIO_MAX));
+}
+
+static void ccid7_check_l_ack_ratio(struct sock *sk)
+{
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+
+	/*
+	 * After a loss, idle period, application limited period, or RTO we
+	 * need to check that the ack ratio is still less than the congestion
+	 * window. Otherwise, we will send an entire congestion window of
+	 * packets and got no response because we haven't sent ack ratio
+	 * packets yet.
+	 * If the ack ratio does need to be reduced, we reduce it to half of
+	 * the congestion window (or 1 if that's zero) instead of to the
+	 * congestion window. This prevents problems if one ack is lost.
+	 */
+	if (dccp_feat_nn_get(sk, DCCPF_ACK_RATIO) > hc->tx_cwnd)
+		ccid7_change_l_ack_ratio(sk, hc->tx_cwnd/2 ? : 1U);
+}
+
+static void ccid7_change_l_seq_window(struct sock *sk, u64 val)
+{
+	dccp_feat_signal_nn_change(sk, DCCPF_SEQUENCE_WINDOW,
+				   clamp_val(val, DCCPF_SEQ_WMIN,
+						  DCCPF_SEQ_WMAX));
+}
+
+static void dccp_tasklet_schedule(struct sock *sk)
+{
+	struct tasklet_struct *t = &dccp_sk(sk)->dccps_xmitlet;
+
+	if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
+		sock_hold(sk);
+		__tasklet_schedule(t);
+	}
+}
+
+static void ccid7_hc_tx_rto_expire(struct timer_list *t)
+{
+	//struct sock *sk = (struct sock *)data;
+	//struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+	struct ccid7_hc_tx_sock *hc = from_timer(hc, t, tx_rtotimer);
+	struct sock *sk = hc->sk;
+	const bool sender_was_blocked = ccid7_cwnd_network_limited(hc);
+
+	bh_lock_sock(sk);
+	if (sock_owned_by_user(sk)) {
+		sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + HZ / 5);
+		goto out;
+	}
+
+	ccid7_pr_debug("RTO_EXPIRE\n");
+
+	if (sk->sk_state == DCCP_CLOSED)
+		goto out;
+
+	/* back-off timer */
+	if (hc->exp_inc_rtotimer) {
+    hc->tx_rto <<= 1;
+    if (hc->tx_rto > DCCP_RTO_MAX)
+      hc->tx_rto = DCCP_RTO_MAX;
+  }
+  
+	/* adjust pipe, cwnd etc */
+	hc->tx_ssthresh = hc->tx_cwnd / 2;
+	if (hc->tx_ssthresh < 2)
+		hc->tx_ssthresh = 2;
+	hc->tx_cwnd	= 1;
+	hc->tx_pipe	= 0;
+
+	/* clear state about stuff we sent */
+	hc->tx_seqt = hc->tx_seqh;
+	hc->tx_packets_acked = 0;
+
+	/* clear ack ratio state. */
+	hc->tx_rpseq    = 0;
+	hc->tx_rpdupack = -1;
+	ccid7_change_l_ack_ratio(sk, 1);
+
+	/* if we were blocked before, we may now send cwnd=1 packet */
+	if (sender_was_blocked)
+		dccp_tasklet_schedule(sk);
+	/* restart backed-off timer */
+	sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
+out:
+	bh_unlock_sock(sk);
+	sock_put(sk);
+}
+
+/*
+ *	Congestion window validation (RFC 2861).
+ */
+static bool ccid7_do_cwv = true;
+module_param(ccid7_do_cwv, bool, 0644);
+MODULE_PARM_DESC(ccid7_do_cwv, "Perform RFC2861 Congestion Window Validation");
+
+/**
+ * ccid7_update_used_window  -  Track how much of cwnd is actually used
+ * This is done in addition to CWV. The sender needs to have an idea of how many
+ * packets may be in flight, to set the local Sequence Window value accordingly
+ * (RFC 4340, 7.5.2). The CWV mechanism is exploited to keep track of the
+ * maximum-used window. We use an EWMA low-pass filter to filter out noise.
+ */
+static void ccid7_update_used_window(struct ccid7_hc_tx_sock *hc, u32 new_wnd)
+{
+	hc->tx_expected_wnd = (3 * hc->tx_expected_wnd + new_wnd) / 4;
+}
+
+/* This borrows the code of tcp_cwnd_application_limited() */
+static void ccid7_cwnd_application_limited(struct sock *sk, const u32 now)
+{
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+	/* don't reduce cwnd below the initial window (IW) */
+	u32 init_win = rfc3390_bytes_to_packets(dccp_sk(sk)->dccps_mss_cache),
+	    win_used = max(hc->tx_cwnd_used, init_win);
+
+	if (win_used < hc->tx_cwnd) {
+		hc->tx_ssthresh = max(hc->tx_ssthresh,
+				     (hc->tx_cwnd >> 1) + (hc->tx_cwnd >> 2));
+		hc->tx_cwnd = (hc->tx_cwnd + win_used) >> 1;
+		dccp_pr_debug("%s: tx_cwnd set to %d for sk %p", __func__, hc->tx_cwnd, sk);
+	}
+	hc->tx_cwnd_used  = 0;
+	hc->tx_cwnd_stamp = now;
+
+	ccid7_check_l_ack_ratio(sk);
+}
+
+/* This borrows the code of tcp_cwnd_restart() */
+static void ccid7_cwnd_restart(struct sock *sk, const u32 now)
+{
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+	u32 cwnd = hc->tx_cwnd, restart_cwnd,
+	    iwnd = rfc3390_bytes_to_packets(dccp_sk(sk)->dccps_mss_cache);
+	s32 delta = now - hc->tx_lsndtime;
+
+	hc->tx_ssthresh = max(hc->tx_ssthresh, (cwnd >> 1) + (cwnd >> 2));
+
+	/* don't reduce cwnd below the initial window (IW) */
+	restart_cwnd = min(cwnd, iwnd);
+
+	while ((delta -= hc->tx_rto) >= 0 && cwnd > restart_cwnd)
+		cwnd >>= 1;
+	hc->tx_cwnd = max(cwnd, restart_cwnd);
+	hc->tx_cwnd_stamp = now;
+	hc->tx_cwnd_used  = 0;
+
+	ccid7_check_l_ack_ratio(sk);
+}
+
+static void ccid7_hc_tx_packet_sent(struct sock *sk, unsigned int len)
+{
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+	const u32 now = ccid7_jiffies32;
+	struct ccid7_seq *next;
+
+	/* slow-start after idle periods (RFC 2581, RFC 2861) */
+	if (ccid7_do_cwv && !hc->tx_pipe &&
+	    (s32)(now - hc->tx_lsndtime) >= hc->tx_rto)
+		ccid7_cwnd_restart(sk, now);
+
+	hc->tx_lsndtime = now;
+	hc->tx_pipe += 1;
+
+	/* see whether cwnd was fully used (RFC 2861), update expected window */
+	if (ccid7_cwnd_network_limited(hc)) {
+		ccid7_update_used_window(hc, hc->tx_cwnd);
+		hc->tx_cwnd_used  = 0;
+		hc->tx_cwnd_stamp = now;
+	} else {
+		if (hc->tx_pipe > hc->tx_cwnd_used)
+			hc->tx_cwnd_used = hc->tx_pipe;
+
+		ccid7_update_used_window(hc, hc->tx_cwnd_used);
+
+		if (ccid7_do_cwv && (s32)(now - hc->tx_cwnd_stamp) >= hc->tx_rto)
+			ccid7_cwnd_application_limited(sk, now);
+	}
+
+	hc->tx_seqh->ccid7s_seq   = dp->dccps_gss;
+	hc->tx_seqh->ccid7s_acked = 0;
+	hc->tx_seqh->ccid7s_sent  = now;
+
+	next = hc->tx_seqh->ccid7s_next;
+	/* check if we need to alloc more space */
+	if (next == hc->tx_seqt) {
+		if (ccid7_hc_tx_alloc_seq(hc)) {
+			DCCP_CRIT("packet history - out of memory!");
+			/* FIXME: find a more graceful way to bail out */
+			return;
+		}
+		next = hc->tx_seqh->ccid7s_next;
+		BUG_ON(next == hc->tx_seqt);
+	}
+	hc->tx_seqh = next;
+
+	ccid7_pr_debug("cwnd=%d pipe=%d\n", hc->tx_cwnd, hc->tx_pipe);
+
+	/*
+	 * FIXME: The code below is broken and the variables have been removed
+	 * from the socket struct. The `ackloss' variable was always set to 0,
+	 * and with arsent there are several problems:
+	 *  (i) it doesn't just count the number of Acks, but all sent packets;
+	 *  (ii) it is expressed in # of packets, not # of windows, so the
+	 *  comparison below uses the wrong formula: Appendix A of RFC 4341
+	 *  comes up with the number K = cwnd / (R^2 - R) of consecutive windows
+	 *  of data with no lost or marked Ack packets. If arsent were the # of
+	 *  consecutive Acks received without loss, then Ack Ratio needs to be
+	 *  decreased by 1 when
+	 *	      arsent >=  K * cwnd / R  =  cwnd^2 / (R^3 - R^2)
+	 *  where cwnd / R is the number of Acks received per window of data
+	 *  (cf. RFC 4341, App. A). The problems are that
+	 *  - arsent counts other packets as well;
+	 *  - the comparison uses a formula different from RFC 4341;
+	 *  - computing a cubic/quadratic equation each time is too complicated.
+	 *  Hence a different algorithm is needed.
+	 */
+#if 0
+	/* Ack Ratio.  Need to maintain a concept of how many windows we sent */
+	hc->tx_arsent++;
+	/* We had an ack loss in this window... */
+	if (hc->tx_ackloss) {
+		if (hc->tx_arsent >= hc->tx_cwnd) {
+			hc->tx_arsent  = 0;
+			hc->tx_ackloss = 0;
+		}
+	} else {
+		/* No acks lost up to now... */
+		/* decrease ack ratio if enough packets were sent */
+		if (dp->dccps_l_ack_ratio > 1) {
+			/* XXX don't calculate denominator each time */
+			int denom = dp->dccps_l_ack_ratio * dp->dccps_l_ack_ratio -
+				    dp->dccps_l_ack_ratio;
+
+			denom = hc->tx_cwnd * hc->tx_cwnd / denom;
+
+			if (hc->tx_arsent >= denom) {
+				ccid7_change_l_ack_ratio(sk, dp->dccps_l_ack_ratio - 1);
+				hc->tx_arsent = 0;
+			}
+		} else {
+			/* we can't increase ack ratio further [1] */
+			hc->tx_arsent = 0; /* or maybe set it to cwnd*/
+		}
+	}
+#endif
+
+	sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
+
+#ifdef CONFIG_IP_DCCP_CCID7_DEBUG
+	do {
+		struct ccid7_seq *seqp = hc->tx_seqt;
+
+		while (seqp != hc->tx_seqh) {
+			ccid7_pr_debug("out seq=%llu acked=%d time=%u\n",
+				       (unsigned long long)seqp->ccid7s_seq,
+				       seqp->ccid7s_acked, seqp->ccid7s_sent);
+			seqp = seqp->ccid7s_next;
+		}
+	} while (0);
+	ccid7_pr_debug("=========\n");
+#endif
+}
+
+/**
+ * ccid7_rtt_estimator - Sample RTT and compute RTO using RFC2988 algorithm
+ * This code is almost identical with TCP's tcp_rtt_estimator(), since
+ * - it has a higher sampling frequency (recommended by RFC 1323),
+ * - the RTO does not collapse into RTT due to RTTVAR going towards zero,
+ * - it is simple (cf. more complex proposals such as Eifel timer or research
+ *   which suggests that the gain should be set according to window size),
+ * - in tests it was found to work well with CCID7 [gerrit].
+ */
+static void ccid7_rtt_estimator(struct sock *sk, const long mrtt)
+{
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+	long m = mrtt ? : 1;
+
+	hc->tx_mrtt = mrtt;
+
+	if (hc->tx_srtt == 0) {
+		/* First measurement m */
+		hc->tx_srtt = m << 3;
+		hc->tx_mdev = m << 1;
+
+		hc->tx_mdev_max = max(hc->tx_mdev, tcp_rto_min(sk));
+		hc->tx_rttvar   = hc->tx_mdev_max;
+
+		hc->tx_rtt_seq  = dccp_sk(sk)->dccps_gss;
+	} else {
+		/* Update scaled SRTT as SRTT += 1/8 * (m - SRTT) */
+		m -= (hc->tx_srtt >> 3);
+		hc->tx_srtt += m;
+
+		/* Similarly, update scaled mdev with regard to |m| */
+		if (m < 0) {
+			m = -m;
+			m -= (hc->tx_mdev >> 2);
+			/*
+			 * This neutralises RTO increase when RTT < SRTT - mdev
+			 * (see P. Sarolahti, A. Kuznetsov,"Congestion Control
+			 * in Linux TCP", USENIX 2002, pp. 49-62).
+			 */
+			if (m > 0)
+				m >>= 3;
+		} else {
+			m -= (hc->tx_mdev >> 2);
+		}
+		hc->tx_mdev += m;
+
+		if (hc->tx_mdev > hc->tx_mdev_max) {
+			hc->tx_mdev_max = hc->tx_mdev;
+			if (hc->tx_mdev_max > hc->tx_rttvar)
+				hc->tx_rttvar = hc->tx_mdev_max;
+		}
+
+		/*
+		 * Decay RTTVAR at most once per flight, exploiting that
+		 *  1) pipe <= cwnd <= Sequence_Window = W  (RFC 4340, 7.5.2)
+		 *  2) AWL = GSS-W+1 <= GAR <= GSS          (RFC 4340, 7.5.1)
+		 * GAR is a useful bound for FlightSize = pipe.
+		 * AWL is probably too low here, as it over-estimates pipe.
+		 */
+		if (after48(dccp_sk(sk)->dccps_gar, hc->tx_rtt_seq)) {
+			if (hc->tx_mdev_max < hc->tx_rttvar)
+				hc->tx_rttvar -= (hc->tx_rttvar -
+						  hc->tx_mdev_max) >> 2;
+			hc->tx_rtt_seq  = dccp_sk(sk)->dccps_gss;
+			hc->tx_mdev_max = tcp_rto_min(sk);
+		}
+	}
+
+	/*
+	 * Set RTO from SRTT and RTTVAR
+	 * As in TCP, 4 * RTTVAR >= TCP_RTO_MIN, giving a minimum RTO of 200 ms.
+	 * This agrees with RFC 4341, 5:
+	 *	"Because DCCP does not retransmit data, DCCP does not require
+	 *	 TCP's recommended minimum timeout of one second".
+	 */
+	hc->tx_rto = (hc->tx_srtt >> 3) + hc->tx_rttvar;
+
+	if (hc->tx_rto > DCCP_RTO_MAX)
+		hc->tx_rto = DCCP_RTO_MAX;
+}
+
+static void ccid7_new_ack(struct sock *sk, struct ccid7_seq *seqp,
+			  unsigned int *maxincr)
+{
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+	struct dccp_sock *dp = dccp_sk(sk);
+	int r_seq_used = hc->tx_cwnd / dp->dccps_l_ack_ratio;
+
+	if (hc->tx_cwnd < dp->dccps_l_seq_win &&
+	    r_seq_used < dp->dccps_r_seq_win) {
+		if (hc->tx_cwnd < hc->tx_ssthresh) {
+			if (*maxincr > 0 && ++hc->tx_packets_acked >= 2) {
+				if (hc->css_round_count > 0) {
+          hc->css_pkt_count++;
+          if (hc->css_pkt_count >= CSS_GROWTH_DIVISOR) {
+            hc->tx_cwnd += 1;
+            hc->css_pkt_count = 0;
+          }
+          *maxincr    -= 1;
+          hc->tx_packets_acked = 0;
+        }
+        else {
+				  hc->tx_cwnd += 1;
+				  *maxincr    -= 1;
+				  hc->tx_packets_acked = 0;
+			  }
+			}
+		} else {
+      set_cwnd_rfc8312 (hc);
+		}
+	}
+
+	/*
+	 * Adjust the local sequence window and the ack ratio to allow about
+	 * 5 times the number of packets in the network (RFC 4340 7.5.2)
+	 */
+	if (r_seq_used * CCID7_WIN_CHANGE_FACTOR >= dp->dccps_r_seq_win)
+		ccid7_change_l_ack_ratio(sk, dp->dccps_l_ack_ratio * 2);
+	else if (r_seq_used * CCID7_WIN_CHANGE_FACTOR < dp->dccps_r_seq_win/2)
+		ccid7_change_l_ack_ratio(sk, dp->dccps_l_ack_ratio / 2 ? : 1U);
+
+	if (hc->tx_cwnd * CCID7_WIN_CHANGE_FACTOR >= dp->dccps_l_seq_win)
+		ccid7_change_l_seq_window(sk, dp->dccps_l_seq_win * 2);
+	else if (hc->tx_cwnd * CCID7_WIN_CHANGE_FACTOR < dp->dccps_l_seq_win/2)
+		ccid7_change_l_seq_window(sk, dp->dccps_l_seq_win / 2);
+
+	/*
+	 * FIXME: RTT is sampled several times per acknowledgment (for each
+	 * entry in the Ack Vector), instead of once per Ack (as in TCP SACK).
+	 * This causes the RTT to be over-estimated, since the older entries
+	 * in the Ack Vector have earlier sending times.
+	 * The cleanest solution is to not use the ccid7s_sent field at all
+	 * and instead use DCCP timestamps: requires changes in other places.
+	 */
+	ccid7_rtt_estimator(sk, ccid7_jiffies32 - seqp->ccid7s_sent);
+  
+	/* first time call or link delay decreases */
+  if (hc->min_rtt > hc->tx_mrtt)
+		hc->min_rtt = hc->tx_mrtt;
+  
+	/* hystart triggers when cwnd is larger than some threshold */
+  if (hystart && in_slow_start(sk)) {
+    if (hc->tx_cwnd >= hystart_low_window)
+      hystart_update(sk, hc->tx_mrtt);
+    if (after(seqp->ccid7s_seq, hc->end_seq))
+      bictcp_hystart_reset(sk, hc);
+  }
+}
+
+static void ccid7_congestion_event(struct sock *sk, struct ccid7_seq *seqp)
+{
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+
+	if ((s32)(seqp->ccid7s_sent - hc->tx_last_cong) < 0) {
+		ccid7_pr_debug("Multiple losses in an RTT---treating as one\n");
+		return;
+	}
+
+	hc->tx_last_cong = ccid7_jiffies32;
+
+  hc->tx_wmax = hc->tx_cwnd; 
+  hc->tx_ssthresh = (hc->tx_cwnd * cub_b) >> cub_shift;
+  hc->tx_ssthresh = hc->tx_ssthresh > 2? hc->tx_ssthresh: 2; 
+  hc->tx_cwnd = (hc->tx_cwnd * cub_b) >> cub_shift; 
+  if (hc->tx_wmax < hc->tx_wmax_prev) { 
+    hc->tx_wmax_prev = hc->tx_wmax; 
+    hc->tx_wmax = (hc->tx_wmax * (cub_fact + cub_b)) >> (cub_shift + 1);
+  } else {
+    hc->tx_wmax_prev = hc->tx_wmax;
+  }
+  set_k_rfc8312 (hc);
+  hc->ca_rx_ct = hc->loss_ct = 0; 
+  hc->ref_t = bictcp_clock();
+  hc->css_round_count = 0;
+	ccid7_check_l_ack_ratio(sk);
+}
+
+static int ccid7_hc_tx_parse_options(struct sock *sk, u8 packet_type,
+				     u8 option, u8 *optval, u8 optlen)
+{
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+
+	switch (option) {
+	case DCCPO_ACK_VECTOR_0:
+	case DCCPO_ACK_VECTOR_1:
+		return dccp_ackvec_parsed_add(&hc->tx_av_chunks, optval, optlen,
+					      option - DCCPO_ACK_VECTOR_0);
+	}
+	return 0;
+}
+
+static void ccid7_hc_tx_packet_recv(struct sock *sk, struct sk_buff *skb)
+{
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+	const bool sender_was_blocked = ccid7_cwnd_network_limited(hc);
+	struct dccp_ackvec_parsed *avp;
+	u64 ackno, seqno;
+	struct ccid7_seq *seqp;
+	int done = 0;
+	bool not_rst = 0;
+	unsigned int maxincr = 0;
+
+	/* check reverse path congestion */
+	seqno = DCCP_SKB_CB(skb)->dccpd_seq;
+
+	/* XXX this whole "algorithm" is broken.  Need to fix it to keep track
+	 * of the seqnos of the dupacks so that rpseq and rpdupack are correct
+	 * -sorbo.
+	 */
+	/* need to bootstrap */
+	if (hc->tx_rpdupack == -1) {
+		hc->tx_rpdupack = 0;
+		hc->tx_rpseq    = seqno;
+	} else {
+		/* check if packet is consecutive */
+		if (dccp_delta_seqno(hc->tx_rpseq, seqno) == 1)
+			hc->tx_rpseq = seqno;
+		/* it's a later packet */
+		else if (after48(seqno, hc->tx_rpseq)) {
+			hc->tx_rpdupack++;
+
+			/* check if we got enough dupacks */
+			if (hc->tx_rpdupack >= NUMDUPACK) {
+				hc->tx_rpdupack = -1; /* XXX lame */
+				hc->tx_rpseq    = 0;
+#ifdef __CCID7_COPES_GRACEFULLY_WITH_ACK_CONGESTION_CONTROL__
+				/*
+				 * FIXME: Ack Congestion Control is broken; in
+				 * the current state instabilities occurred with
+				 * Ack Ratios greater than 1; causing hang-ups
+				 * and long RTO timeouts. This needs to be fixed
+				 * before opening up dynamic changes. -- gerrit
+				 */
+				ccid7_change_l_ack_ratio(sk, 2 * dp->dccps_l_ack_ratio);
+#endif
+			}
+		}
+	}
+
+	/* check forward path congestion */
+	if (dccp_packet_without_ack(skb))
+		return;
+
+	/* still didn't send out new data packets */
+	if (hc->tx_seqh == hc->tx_seqt)
+		goto done;
+
+	ackno = DCCP_SKB_CB(skb)->dccpd_ack_seq;
+	if (after48(ackno, hc->tx_high_ack))
+		hc->tx_high_ack = ackno;
+
+	seqp = hc->tx_seqt;
+	while (before48(seqp->ccid7s_seq, ackno)) {
+		seqp = seqp->ccid7s_next;
+		if (seqp == hc->tx_seqh) {
+			seqp = hc->tx_seqh->ccid7s_prev;
+			not_rst = 1;
+			break;
+		}
+	}
+
+	/*
+	 * In slow-start, cwnd can increase up to a maximum of Ack Ratio/2
+	 * packets per acknowledgement. Rounding up avoids that cwnd is not
+	 * advanced when Ack Ratio is 1 and gives a slight edge otherwise.
+	 */
+	if (hc->tx_cwnd < hc->tx_ssthresh)
+		maxincr = DIV_ROUND_UP(dp->dccps_l_ack_ratio, 2);
+
+	/* go through all ack vectors */
+	list_for_each_entry(avp, &hc->tx_av_chunks, node) {
+		/* go through this ack vector */
+		for (; avp->len--; avp->vec++) {
+			u64 ackno_end_rl = SUB48(ackno,
+						 dccp_ackvec_runlen(avp->vec));
+
+			ccid7_pr_debug("ackvec %llu |%u,%u|\n",
+				       (unsigned long long)ackno,
+				       dccp_ackvec_state(avp->vec) >> 6,
+				       dccp_ackvec_runlen(avp->vec));
+			/* if the seqno we are analyzing is larger than the
+			 * current ackno, then move towards the tail of our
+			 * seqnos.
+			 */
+			while (after48(seqp->ccid7s_seq, ackno)) {
+				if (seqp == hc->tx_seqt) {
+					done = 1;
+					break;
+				}
+				seqp = seqp->ccid7s_prev;
+			}
+			if (done)
+				break;
+
+
+			/* check all seqnos in the range of the vector
+			 * run length
+			 */
+
+			while (between48(seqp->ccid7s_seq,ackno_end_rl,ackno)) {
+				const u8 state = dccp_ackvec_state(avp->vec);
+
+				/* new packet received or marked */
+				if (state != DCCPAV_NOT_RECEIVED &&
+				    !seqp->ccid7s_acked) {
+					if (state == DCCPAV_ECN_MARKED)
+						ccid7_congestion_event(sk,
+								       seqp);
+					else
+						ccid7_new_ack(sk, seqp,
+							      &maxincr);
+
+					seqp->ccid7s_acked = 1;
+					ccid7_pr_debug("Got ack for %llu\n",
+						       (unsigned long long)seqp->ccid7s_seq);
+					hc->tx_pipe--;
+				}
+				if (seqp == hc->tx_seqt) {
+					done = 1;
+					break;
+				}
+				seqp = seqp->ccid7s_prev;
+			}
+			if (done)
+				break;
+
+			ackno = SUB48(ackno_end_rl, 1);
+		}
+		if (done)
+			break;
+	}
+
+	/* The state about what is acked should be correct now
+	 * Check for NUMDUPACK
+	 */
+	seqp = hc->tx_seqt;
+	while (before48(seqp->ccid7s_seq, hc->tx_high_ack)) {
+		seqp = seqp->ccid7s_next;
+		if (seqp == hc->tx_seqh) {
+			seqp = hc->tx_seqh->ccid7s_prev;
+			break;
+		}
+	}
+	done = 0;
+	while (1) {
+		if (seqp->ccid7s_acked) {
+			done++;
+			if (done == NUMDUPACK)
+				break;
+		}
+		if (seqp == hc->tx_seqt)
+			break;
+		seqp = seqp->ccid7s_prev;
+	}
+
+	/* If there are at least 3 acknowledgements, anything unacknowledged
+	 * below the last sequence number is considered lost
+	 */
+	if (done == NUMDUPACK) {
+		struct ccid7_seq *last_acked = seqp;
+
+		/* check for lost packets */
+		while (1) {
+			if (!seqp->ccid7s_acked) {
+				ccid7_pr_debug("Packet lost: %llu\n",
+					       (unsigned long long)seqp->ccid7s_seq);
+				/* XXX need to traverse from tail -> head in
+				 * order to detect multiple congestion events in
+				 * one ack vector.
+				 */
+				ccid7_congestion_event(sk, seqp);
+				hc->tx_pipe--;
+			}
+			if (seqp == hc->tx_seqt)
+				break;
+			seqp = seqp->ccid7s_prev;
+		}
+
+		hc->tx_seqt = last_acked;
+	}
+
+	/* trim acked packets in tail */
+	while (hc->tx_seqt != hc->tx_seqh) {
+		if (!hc->tx_seqt->ccid7s_acked)
+			break;
+
+		hc->tx_seqt = hc->tx_seqt->ccid7s_next;
+	}
+
+	/* restart RTO timer if not all outstanding data has been acked */
+	if (hc->tx_pipe == 0)
+		sk_stop_timer(sk, &hc->tx_rtotimer);
+	else if(!not_rst)
+		sk_reset_timer(sk, &hc->tx_rtotimer, jiffies + hc->tx_rto);
+done:
+	/* check if incoming Acks allow pending packets to be sent */
+	if (sender_was_blocked && !ccid7_cwnd_network_limited(hc))
+		dccp_tasklet_schedule(sk);
+	dccp_ackvec_parsed_cleanup(&hc->tx_av_chunks);
+}
+
+static int ccid7_hc_tx_init(struct ccid *ccid, struct sock *sk)
+{
+	struct ccid7_hc_tx_sock *hc = ccid_priv(ccid);
+	struct dccp_sock *dp = dccp_sk(sk);
+	u32 max_ratio;
+
+	/* RFC 4341, 5: initialise ssthresh to arbitrarily high (max) value */
+	hc->tx_ssthresh = ~0U;
+  hc->curr_rtt = ~0U;
+  hc->min_rtt = ~0U;
+  hc->found = 0;
+  hc->css_round_count = 0;
+  hc->css_pkt_count = 0;
+  hc->exp_inc_rtotimer = 0;
+
+	/* Use larger initial windows (RFC 4341, section 5). */
+	hc->tx_cwnd = rfc3390_bytes_to_packets(dp->dccps_mss_cache);
+	hc->tx_expected_wnd = hc->tx_cwnd;
+
+	/* Make sure that Ack Ratio is enabled and within bounds. */
+	max_ratio = DIV_ROUND_UP(hc->tx_cwnd, 2);
+	if (dp->dccps_l_ack_ratio == 0 || dp->dccps_l_ack_ratio > max_ratio)
+		dp->dccps_l_ack_ratio = max_ratio;
+
+	/* XXX init ~ to window size... */
+	if (ccid7_hc_tx_alloc_seq(hc))
+		return -ENOMEM;
+
+  hc->ref_t = bictcp_clock();
+	hc->tx_rto = DCCP_TIMEOUT_INIT;
+	hc->tx_rpdupack  = -1;
+	hc->tx_last_cong = hc->tx_lsndtime = hc->tx_cwnd_stamp = ccid7_jiffies32;
+	hc->tx_cwnd_used = 0;
+	//setup_timer(&hc->tx_rtotimer, ccid7_hc_tx_rto_expire,
+	//		(unsigned long)sk);
+	hc->sk		 = sk;
+	timer_setup(&hc->tx_rtotimer, ccid7_hc_tx_rto_expire, 0);
+	INIT_LIST_HEAD(&hc->tx_av_chunks);
+  
+	if (hystart)
+		bictcp_hystart_reset(sk, hc);
+  
+	return 0;
+}
+
+static void ccid7_hc_tx_exit(struct sock *sk)
+{
+	struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+	int i;
+
+	sk_stop_timer(sk, &hc->tx_rtotimer);
+
+	for (i = 0; i < hc->tx_seqbufc; i++)
+		kfree(hc->tx_seqbuf[i]);
+	hc->tx_seqbufc = 0;
+	dccp_ackvec_parsed_cleanup(&hc->tx_av_chunks);
+}
+
+static void ccid7_hc_rx_packet_recv(struct sock *sk, struct sk_buff *skb)
+{
+	struct ccid7_hc_rx_sock *hc = ccid7_hc_rx_sk(sk);
+	//printk(KERN_INFO "natrm: enter ccid7_hc_rx_packet_recv %p", sk);
+
+	if (!dccp_data_packet(skb))
+		return;
+	if (++hc->rx_num_data_pkts >= dccp_sk(sk)->dccps_r_ack_ratio) {
+		dccp_send_ack(sk);
+		hc->rx_num_data_pkts = 0;
+	}
+}
+
+static void ccid7_hc_tx_get_info(struct sock *sk, struct tcp_info *info)
+{
+	info->tcpi_rto = ccid7_hc_tx_sk(sk)->tx_rto;
+	info->tcpi_rtt = ccid7_hc_tx_sk(sk)->tx_srtt;
+	info->tcpi_rttvar = ccid7_hc_tx_sk(sk)->tx_mrtt;
+	info->tcpi_segs_out = ccid7_hc_tx_sk(sk)->tx_pipe;
+	info->tcpi_snd_cwnd = ccid7_hc_tx_sk(sk)->tx_cwnd;
+	info->tcpi_last_data_sent = ccid7_hc_tx_sk(sk)->tx_lsndtime;
+}
+
+// NOTE: #define DCCP_SOCKOPT_CCID_TX_INFO 192 in include/uapi/linux/dccp.h
+// NOTE: #define DCCP_SOCKOPT_CCID_LIM_RTO 193 in include/uapi/linux/dccp.h
+
+struct dccp_ccid7_tx { // Pieska modification, added struct
+  u32 tx_cwnd;	
+  u32 tx_pipe;	
+  u32 tx_srtt;	
+  u32 tx_mrtt;	
+  u32 tx_rto;
+  u32 tx_min_rtt;		
+  u32 tx_delivered;	
+};
+
+// Pieska modification, added function
+static int ccid7_hc_tx_getsockopt(struct sock *sk, const int optname, int len,
+				  u32 __user *optval, int __user *optlen)
+{
+  struct ccid7_hc_tx_sock *hc = ccid7_hc_tx_sk(sk);
+	struct dccp_ccid7_tx tx;
+	const void *val;
+
+	switch (optname) {
+	case DCCP_SOCKOPT_CCID_TX_INFO:
+		if (len < sizeof(tx))
+			return -EINVAL;
+		memset(&tx, 0, sizeof(tx));
+		tx.tx_cwnd = hc->tx_cwnd;
+		tx.tx_pipe = hc->tx_pipe;
+		tx.tx_srtt = hc->tx_srtt;
+		tx.tx_mrtt = hc->tx_mrtt;
+		tx.tx_rto = hc->tx_rto;
+		tx.tx_min_rtt = 0;
+		tx.tx_delivered = 0;
+		len = sizeof(tx);
+		val = &tx;
+		break;
+	case DCCP_SOCKOPT_CCID_LIM_RTO:
+        hc->exp_inc_rtotimer = 0;
+        break;
+	default:
+		return -ENOPROTOOPT;
+	}
+
+	if (put_user(len, optlen) || copy_to_user(optval, val, len))
+		return -EFAULT;
+
+	return 0;
+}
+
+struct ccid_operations ccid7_ops = {
+	.ccid_id		  = DCCPC_CCID7,
+	.ccid_name		  = "TCP-like",
+	.ccid_hc_tx_obj_size	  = sizeof(struct ccid7_hc_tx_sock),
+	.ccid_hc_tx_init	  = ccid7_hc_tx_init,
+	.ccid_hc_tx_exit	  = ccid7_hc_tx_exit,
+	.ccid_hc_tx_send_packet	  = ccid7_hc_tx_send_packet,
+	.ccid_hc_tx_packet_sent	  = ccid7_hc_tx_packet_sent,
+	.ccid_hc_tx_parse_options = ccid7_hc_tx_parse_options,
+	.ccid_hc_tx_packet_recv	  = ccid7_hc_tx_packet_recv,
+	.ccid_hc_tx_get_info	  = ccid7_hc_tx_get_info,
+	.ccid_hc_rx_obj_size	  = sizeof(struct ccid7_hc_rx_sock),
+	.ccid_hc_rx_packet_recv	  = ccid7_hc_rx_packet_recv,
+  .ccid_hc_tx_getsockopt	  = ccid7_hc_tx_getsockopt, // Pieska modification, added operation
+};
+
+#ifdef CONFIG_IP_DCCP_CCID7_DEBUG
+module_param(ccid7_debug, bool, 0644);
+MODULE_PARM_DESC(ccid7_debug, "Enable CCID-2 debug messages");
+#endif
diff --git a/net/dccp/ccids/ccid7.h b/net/dccp/ccids/ccid7.h
new file mode 100755
index 0000000000000..a7cf6f63068e1
--- /dev/null
+++ b/net/dccp/ccids/ccid7.h
@@ -0,0 +1,188 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ *  Copyright (C) 2022 by Marcus Pieska, Karlstad University for Deutsche Telekom AG
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef _DCCP_CCID7_H_
+#define _DCCP_CCID7_H_
+
+#include <linux/timer.h>
+#include <linux/types.h>
+#include "../ccid.h"
+#include "../dccp.h"
+
+/*
+ * CCID-2 timestamping faces the same issues as TCP timestamping.
+ * Hence we reuse/share as much of the code as possible.
+ */
+#define ccid7_jiffies32	((u32)jiffies)
+
+/* NUMDUPACK parameter from RFC 4341, p. 6 */
+#define NUMDUPACK	3
+
+struct ccid7_seq {
+	u64			ccid7s_seq;
+	u32			ccid7s_sent;
+	int			ccid7s_acked;
+	struct ccid7_seq	*ccid7s_prev;
+	struct ccid7_seq	*ccid7s_next;
+};
+
+#define CCID7_SEQBUF_LEN 1024
+#define CCID7_SEQBUF_MAX 128
+
+/*
+ * Multiple of congestion window to keep the sequence window at
+ * (RFC 4340 7.5.2)
+ */
+#define CCID7_WIN_CHANGE_FACTOR 5
+
+/**
+ * struct ccid7_hc_tx_sock - CCID7 TX half connection
+ * @tx_{cwnd,ssthresh,pipe}: as per RFC 4341, section 5
+ * @tx_packets_acked:	     Ack counter for deriving cwnd growth (RFC 3465)
+ * @tx_srtt:		     smoothed RTT estimate, scaled by 2^3
+ * @tx_mdev:		     smoothed RTT variation, scaled by 2^2
+ * @tx_mdev_max:	     maximum of @mdev during one flight
+ * @tx_rttvar:		     moving average/maximum of @mdev_max
+ * @tx_rto:		     RTO value deriving from SRTT and RTTVAR (RFC 2988)
+ * @tx_rtt_seq:		     to decay RTTVAR at most once per flight
+ * @tx_cwnd_used:	     actually used cwnd, W_used of RFC 2861
+ * @tx_expected_wnd:	     moving average of @tx_cwnd_used
+ * @tx_cwnd_stamp:	     to track idle periods in CWV
+ * @tx_lsndtime:	     last time (in jiffies) a data packet was sent
+ * @tx_rpseq:		     last consecutive seqno
+ * @tx_rpdupack:	     dupacks since rpseq
+ * @tx_av_chunks:	     list of Ack Vectors received on current skb
+ */
+struct ccid7_hc_tx_sock {
+	u32			tx_cwnd;
+	u32			tx_ssthresh;
+	u32			tx_pipe;
+	u32			tx_packets_acked;
+	struct ccid7_seq	*tx_seqbuf[CCID7_SEQBUF_MAX];
+	int			tx_seqbufc;
+	struct ccid7_seq	*tx_seqh;
+	struct ccid7_seq	*tx_seqt;
+
+	/* RTT measurement: variables/principles are the same as in TCP */
+	u32			tx_srtt,
+				  tx_mrtt,	/* Raw RTT value as measured by CCID */
+				  tx_mdev,
+				  tx_mdev_max,
+				  tx_rttvar,
+				  tx_rto;
+	u64			tx_rtt_seq:48;
+	struct timer_list	tx_rtotimer;
+  struct sock   *sk;
+
+  /* Cubic related */
+  u32     css_round_count,
+          css_pkt_count,
+          round_start,
+          sample_cnt,
+          min_rtt,
+          prev_rtt,
+          curr_rtt,
+          last_ack,
+          end_seq,
+          found,
+          w_est,
+          tx_wmax,
+          tx_wmax_prev,
+          ca_rx_ct,
+          loss_ct;
+  s64     cub_c,
+          cub_k,
+          ref_t;
+
+	/* Congestion Window validation (optional, RFC 2861) */
+	u32			exp_inc_rtotimer,
+          tx_cwnd_used,
+				  tx_expected_wnd,
+				  tx_cwnd_stamp,
+				  tx_lsndtime;
+
+	u64			tx_rpseq;
+	int			tx_rpdupack;
+	u32			tx_last_cong;
+	u64			tx_high_ack;
+	struct list_head	tx_av_chunks;
+};
+
+/**
+ * Obtain SRTT value form CCID7 TX sock.
+ */
+static inline u32 ccid7_srtt_as_delay(struct ccid7_hc_tx_sock *hc){
+	dccp_pr_debug("srtt value : %u", hc->tx_srtt);
+	if(hc){ return hc->tx_srtt;	}
+	else{ return 0; }
+}
+
+/**
+ * Obtain MRTT value form CCID7 TX sock.
+ * NOTE: value is scaled by 8 to match SRTT
+ */
+static inline u32 ccid7_mrtt_as_delay(struct ccid7_hc_tx_sock *hc){
+	dccp_pr_debug("mrtt value : %u", hc->tx_mrtt);
+	if(hc){ return (hc->tx_mrtt * 8); }
+	else{ return 0;	}
+}
+
+/* Function pointer to either get SRTT or MRTT ...*/
+extern u32 (*ccid7_get_delay_val)(struct ccid7_hc_tx_sock *hc);
+
+static inline void set_ccid7_srtt_as_delay(void){
+	ccid7_get_delay_val = ccid7_srtt_as_delay;
+}
+
+static inline void set_ccid7_mrtt_as_delay(void){
+	ccid7_get_delay_val = ccid7_mrtt_as_delay;
+}
+
+
+static inline bool ccid7_cwnd_network_limited(struct ccid7_hc_tx_sock *hc)
+{
+	return hc->tx_pipe >= hc->tx_cwnd;
+}
+
+/*
+ * Convert RFC 3390 larger initial window into an equivalent number of packets.
+ * This is based on the numbers specified in RFC 5681, 3.1.
+ */
+/*static inline u32 ccid7_rfc3390_bytes_to_pkts(const u32 smss)
+{
+	return smss <= 1095 ? 4 : (smss > 2190 ? 2 : 3);
+}*/
+
+/**
+ * struct ccid7_hc_rx_sock  -  Receiving end of CCID-2 half-connection
+ * @rx_num_data_pkts: number of data packets received since last feedback
+ */
+struct ccid7_hc_rx_sock {
+	u32	rx_num_data_pkts;
+};
+
+static inline struct ccid7_hc_tx_sock *ccid7_hc_tx_sk(const struct sock *sk)
+{
+	return ccid_priv(dccp_sk(sk)->dccps_hc_tx_ccid);
+}
+
+static inline struct ccid7_hc_rx_sock *ccid7_hc_rx_sk(const struct sock *sk)
+{
+	return ccid_priv(dccp_sk(sk)->dccps_hc_rx_ccid);
+}
+#endif /* _DCCP_CCID7_H_ */
diff --git a/net/dccp/dccp.h b/net/dccp/dccp.h
index 1f748ed1279d3..10b8165a82b31 100644
--- a/net/dccp/dccp.h
+++ b/net/dccp/dccp.h
@@ -89,14 +89,33 @@ void dccp_time_wait(struct sock *sk, int state, int timeo);
 #define DCCP_SANE_RTT_MIN	100
 #define DCCP_FALLBACK_RTT	(USEC_PER_SEC / 5)
 #define DCCP_SANE_RTT_MAX	(3 * USEC_PER_SEC)
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+#define DCCP_KEEPALIVE_SND_INTVL	(20*HZ)
+#define DCCP_KEEPALIVE_RCV_INTVL	(7200*HZ)	
+#endif
+#define DCCP_EVENT_CLOSE	0x0001
 
 /* sysctl variables for DCCP */
 extern int  sysctl_dccp_request_retries;
 extern int  sysctl_dccp_retries1;
 extern int  sysctl_dccp_retries2;
 extern int  sysctl_dccp_tx_qlen;
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+extern int  sysctl_dccp_keepalive_enable;
+extern int  sysctl_dccp_keepalive_snd_intvl;
+extern int  sysctl_dccp_keepalive_rcv_intvl;
+#endif
 extern int  sysctl_dccp_sync_ratelimit;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+/* Reordering role: Controls addition of new DCCP options on client side
+ * (overall sequence number, delay value transmission)
+ * true  : client role
+ * false : server role
+ */
+extern bool mpdccp_role;
+#endif
+
 /*
  *	48-bit sequence number arithmetic (signed and unsigned)
  */
@@ -216,6 +235,10 @@ void dccp_v4_send_check(struct sock *sk, struct sk_buff *skb);
 int dccp_retransmit_skb(struct sock *sk);
 
 void dccp_send_ack(struct sock *sk);
+void dccp_send_ack_entail(struct sock *sk);
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+void dccp_send_keepalive(struct sock *sk);
+#endif
 void dccp_reqsk_send_ack(const struct sock *sk, struct sk_buff *skb,
 			 struct request_sock *rsk);
 
@@ -230,6 +253,7 @@ bool dccp_qpolicy_full(struct sock *sk);
 void dccp_qpolicy_drop(struct sock *sk, struct sk_buff *skb);
 struct sk_buff *dccp_qpolicy_top(struct sock *sk);
 struct sk_buff *dccp_qpolicy_pop(struct sock *sk);
+void dccp_qpolicy_unlink (struct sock *sk, struct sk_buff *skb);
 bool dccp_qpolicy_param_ok(struct sock *sk, __be32 param);
 
 /*
@@ -292,6 +316,9 @@ int dccp_getsockopt(struct sock *sk, int level, int optname,
 		    char __user *optval, int __user *optlen);
 int dccp_setsockopt(struct sock *sk, int level, int optname,
 		    sockptr_t optval, unsigned int optlen);
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+void dccp_set_keepalive(struct sock *sk, int val);
+#endif
 int dccp_ioctl(struct sock *sk, int cmd, int *karg);
 int dccp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);
 int dccp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int flags,
@@ -308,6 +335,10 @@ int dccp_send_reset(struct sock *sk, enum dccp_reset_codes code);
 void dccp_send_close(struct sock *sk, const int active);
 int dccp_invalid_packet(struct sk_buff *skb);
 u32 dccp_sample_rtt(struct sock *sk, long delta);
+void dccp_finish_passive_close(struct sock *sk);
+
+int register_dccp_notifier(struct notifier_block *nb);
+int unregister_dccp_notifier(struct notifier_block *nb);
 
 static inline bool dccp_bad_service_code(const struct sock *sk,
 					const __be32 service)
@@ -341,14 +372,44 @@ struct dccp_skb_cb {
 	__u8  dccpd_type:4;
 	__u8  dccpd_ccval:4;
 	__u8  dccpd_reset_code,
-	      dccpd_reset_data[3];
+	      dccpd_reset_data[3];     
 	__u16 dccpd_opt_len;
 	__u64 dccpd_seq;
 	__u64 dccpd_ack_seq;
+	__u64 dccpd_mpseq;
 };
 
 #define DCCP_SKB_CB(__skb) ((struct dccp_skb_cb *)&((__skb)->cb[0]))
 
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+/* Keepalive conf */
+static inline int dccp_keepalive_rcv_when(const struct dccp_sock *dp)
+{
+	return dp->dccps_keepalive_rcv_intvl;
+}
+
+static inline int dccp_keepalive_snd_when(const struct dccp_sock *dp)
+{
+	return dp->dccps_keepalive_snd_intvl;
+}
+
+static inline u32 dccp_keepalive_rcv_elapsed(const struct dccp_sock *dp)
+{
+	
+	dccp_pr_debug("jif %u lrcv %u", tcp_jiffies32, dp->dccps_lrcvtime);
+	return tcp_jiffies32 - dp->dccps_lrcvtime;
+
+}
+
+static inline u32 dccp_keepalive_snd_elapsed(const struct dccp_sock *dp)
+{
+	
+	dccp_pr_debug("jif %u lsnd %u", tcp_jiffies32, dp->dccps_lsndtime);
+	return tcp_jiffies32 - dp->dccps_lsndtime;
+
+}
+#endif
+
 /* RFC 4340, sec. 7.7 */
 static inline int dccp_non_data_packet(const struct sk_buff *skb)
 {
@@ -411,7 +472,7 @@ static inline void dccp_update_gsr(struct sock *sk, u64 seq)
 	 * Adjust SWL so that it is not below ISR. In contrast to RFC 4340,
 	 * 7.5.1 we perform this check beyond the initial handshake: W/W' are
 	 * always > 32, so for the first W/W' packets in the lifetime of a
-	 * connection we always have to adjust SWL.
+	 * connection we always have to adjust SWL..
 	 * A second reason why we are doing this is that the window depends on
 	 * the feature-remote value of Sequence Window: nothing stops the peer
 	 * from updating this value while we are busy adjusting SWL for the
@@ -461,6 +522,7 @@ void dccp_feat_list_purge(struct list_head *fn_list);
 
 int dccp_insert_options(struct sock *sk, struct sk_buff *skb);
 int dccp_insert_options_rsk(struct dccp_request_sock *, struct sk_buff *);
+int dccp_insert_options_rsk_mp(const struct sock *sk, struct dccp_request_sock *dreq, struct sk_buff *skb);
 u32 dccp_timestamp(void);
 void dccp_timestamping_init(void);
 int dccp_insert_option(struct sk_buff *skb, unsigned char option,
diff --git a/net/dccp/feat.c b/net/dccp/feat.c
index 54086bb05c42c..dff8a09154302 100644
--- a/net/dccp/feat.c
+++ b/net/dccp/feat.c
@@ -21,6 +21,9 @@
 #include <linux/slab.h>
 #include "ccid.h"
 #include "feat.h"
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+#include "mpdccp.h"
+#endif
 
 /* feature-specific sysctls - initialised to the defaults from RFC 4340, 6.4 */
 unsigned long	sysctl_dccp_sequence_window __read_mostly = 100;
@@ -147,6 +150,7 @@ static const struct {
  *  | DCCPF_MIN_CSUM_COVER     | X  |     | X  |    |   0     |  9.2.1    |
  *  | DCCPF_DATA_CHECKSUM      | X  |     | X  |    |   0     |  9.3.1    |
  *  | DCCPF_SEND_LEV_RATE      | X  |     | X  |    |   0     | 4342/8.4  |
+ *  | DCCPF_MULTIPATH          |    |  X  |    | X  |   1     | non-std   |
  *  +--------------------------+----+-----+----+----+---------+-----------+
  */
 } dccp_feat_table[] = {
@@ -160,6 +164,7 @@ static const struct {
 	{ DCCPF_MIN_CSUM_COVER,  FEAT_AT_RX, FEAT_SP, 0,   dccp_hdlr_min_cscov},
 	{ DCCPF_DATA_CHECKSUM,	 FEAT_AT_RX, FEAT_SP, 0,   NULL },
 	{ DCCPF_SEND_LEV_RATE,	 FEAT_AT_RX, FEAT_SP, 0,   NULL },
+	{ DCCPF_MULTIPATH,	 FEAT_AT_TX, FEAT_SP, MPDCCP_VERS_UNDEFINED << 4, NULL },
 };
 #define DCCP_FEAT_SUPPORTED_MAX		ARRAY_SIZE(dccp_feat_table)
 
@@ -172,7 +177,7 @@ static const struct {
 static int dccp_feat_index(u8 feat_num)
 {
 	/* The first 9 entries are occupied by the types from RFC 4340, 6.4 */
-	if (feat_num > DCCPF_RESERVED && feat_num <= DCCPF_DATA_CHECKSUM)
+	if (feat_num > DCCPF_RESERVED && feat_num <= DCCPF_MULTIPATH)
 		return feat_num - 1;
 
 	/*
@@ -223,8 +228,9 @@ static const char *dccp_feat_fname(const u8 feat)
 		[DCCPF_SEND_NDP_COUNT]	= "Send NDP Count",
 		[DCCPF_MIN_CSUM_COVER]	= "Min. Csum Coverage",
 		[DCCPF_DATA_CHECKSUM]	= "Send Data Checksum",
+		[DCCPF_MULTIPATH]	= "MPDCCP Capable",
 	};
-	if (feat > DCCPF_DATA_CHECKSUM && feat < DCCPF_MIN_CCID_SPECIFIC)
+	if (feat > DCCPF_MULTIPATH && feat < DCCPF_MIN_CCID_SPECIFIC)
 		return feature_names[DCCPF_RESERVED];
 
 	if (feat ==  DCCPF_SEND_LEV_RATE)
@@ -599,7 +605,7 @@ static u8 dccp_feat_is_valid_sp_val(u8 feat_num, u8 val)
 {
 	switch (feat_num) {
 	case DCCPF_CCID:
-		return val == DCCPC_CCID2 || val == DCCPC_CCID3;
+		return (val == DCCPC_CCID2 || val == DCCPC_CCID3 || val == DCCPC_CCID5 || val == DCCPC_CCID6 || val == DCCPC_CCID7);
 	/* Type-check Boolean feature values: */
 	case DCCPF_SHORT_SEQNOS:
 	case DCCPF_ECN_INCAPABLE:
@@ -610,6 +616,8 @@ static u8 dccp_feat_is_valid_sp_val(u8 feat_num, u8 val)
 		return val < 2;
 	case DCCPF_MIN_CSUM_COVER:
 		return val < 16;
+	case DCCPF_MULTIPATH:
+		return ((val >> 4) < MPDCCP_VERS_MAX);
 	}
 	return 0;			/* feature unknown */
 }
@@ -635,7 +643,7 @@ int dccp_feat_insert_opts(struct dccp_sock *dp, struct dccp_request_sock *dreq,
 {
 	struct list_head *fn = dreq ? &dreq->dreq_featneg : &dp->dccps_featneg;
 	struct dccp_feat_entry *pos, *next;
-	u8 opt, type, len, *ptr, nn_in_nbo[DCCP_OPTVAL_MAXLEN];
+	u8 opt, type, len, *ptr, nn_in_nbo[8*DCCP_OPTVAL_MAXLEN];
 	bool rpt;
 
 	/* put entries into @skb in the order they appear in the list */
@@ -661,13 +669,14 @@ int dccp_feat_insert_opts(struct dccp_sock *dp, struct dccp_request_sock *dreq,
 				return -1;
 			}
 		}
+
 		dccp_feat_print_opt(opt, pos->feat_num, ptr, len, 0);
 
 		if (dccp_insert_fn_opt(skb, opt, pos->feat_num, ptr, len, rpt))
 			return -1;
+
 		if (pos->needs_mandatory && dccp_insert_option_mandatory(skb))
 			return -1;
-
 		if (skb->sk->sk_state == DCCP_OPEN &&
 		    (opt == DCCPO_CONFIRM_R || opt == DCCPO_CONFIRM_L)) {
 			/*
@@ -763,6 +772,7 @@ int dccp_feat_register_sp(struct sock *sk, u8 feat, u8 is_local,
 	return __feat_register_sp(&dccp_sk(sk)->dccps_featneg, feat, is_local,
 				  0, list, len);
 }
+EXPORT_SYMBOL_GPL(dccp_feat_register_sp);
 
 /**
  * dccp_feat_nn_get  -  Query current/pending value of NN feature
@@ -930,6 +940,12 @@ static const struct ccid_dependency *dccp_feat_ccid_deps(u8 ccid, bool is_local)
 		return ccid2_dependencies[is_local];
 	case DCCPC_CCID3:
 		return ccid3_dependencies[is_local];
+	case DCCPC_CCID5:
+		return ccid2_dependencies[is_local];
+	case DCCPC_CCID6:
+		return ccid2_dependencies[is_local];
+	case DCCPC_CCID7:
+		return ccid2_dependencies[is_local];
 	default:
 		return NULL;
 	}
@@ -987,9 +1003,10 @@ int dccp_feat_finalise_settings(struct dccp_sock *dp)
 	list_for_each_entry(entry, fn, node)
 		if (entry->feat_num == DCCPF_CCID && entry->val.sp.len == 1)
 			ccids[entry->is_local] = entry->val.sp.vec[0];
-	while (i--)
+	while (i--){
 		if (ccids[i] > 0 && dccp_feat_propagate_ccid(fn, ccids[i], i))
 			return -1;
+	}
 	dccp_feat_print_fnlist(fn);
 	return 0;
 }
@@ -1006,7 +1023,6 @@ int dccp_feat_server_ccid_dependencies(struct dccp_request_sock *dreq)
 	struct list_head *fn = &dreq->dreq_featneg;
 	struct dccp_feat_entry *entry;
 	u8 is_local, ccid;
-
 	for (is_local = 0; is_local <= 1; is_local++) {
 		entry = dccp_feat_list_lookup(fn, DCCPF_CCID, is_local);
 
@@ -1294,6 +1310,46 @@ static u8 dccp_feat_confirm_recv(struct list_head *fn, u8 is_mandatory, u8 opt,
 			    : DCCP_RESET_CODE_OPTION_ERROR;
 }
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+static u8 dccp_mpcap_change_recv(struct dccp_request_sock *dreq, struct list_head *fn,
+									u8 opt, u8 *val, u8 len)
+{
+	u8 ret;
+	ret = dccp_feat_change_recv(fn, 0, opt, DCCPF_MULTIPATH , val, len, 1);
+	if (dreq) {
+		if (!ret) {
+			struct dccp_feat_entry *entry = dccp_feat_list_lookup(fn, DCCPF_MULTIPATH, 1);
+			if (entry && entry->val.sp.len) {
+				dreq->multipath_ver = entry->val.sp.vec[0] >> 4;
+				dccp_pr_debug("success MP version reconciliation with client: %d\n", dreq->multipath_ver);
+			}
+		} else if (ret == DCCP_RESET_CODE_OPTION_ERROR) {
+			dccp_pr_debug("failed MP version reconciliation with client, send empty confirmation\n");
+			ret = dccp_push_empty_confirm(fn, DCCPF_MULTIPATH, 1);
+		}
+	}
+	return ret;
+}
+
+static u8 dccp_mpcap_confirm_recv(struct sock *sk, struct list_head *fn,
+									u8 opt, u8 *val, u8 len)
+{
+
+	u8 ret;
+	ret = dccp_feat_confirm_recv(fn, 0, opt, DCCPF_MULTIPATH, val , len, 0);
+	if (!ret) {
+			struct dccp_feat_entry *entry = dccp_feat_list_lookup(fn, DCCPF_MULTIPATH, 0);
+			if (entry && entry->val.sp.len) {
+				dccp_sk(sk)->multipath_ver = entry->val.sp.vec[0] >> 4;
+				dccp_pr_debug("confirmed MP version with server: %d\n", dccp_sk(sk)->multipath_ver);
+			} else {
+				dccp_pr_debug("failed MP version negotiation with server\n");
+			}
+	}
+	return ret;
+}
+#endif
+
 /**
  * dccp_feat_handle_nn_established  -  Fast-path reception of NN options
  * @sk:		socket of an established DCCP connection
@@ -1419,12 +1475,24 @@ int dccp_feat_parse_options(struct sock *sk, struct dccp_request_sock *dreq,
 		switch (opt) {
 		case DCCPO_CHANGE_L:
 		case DCCPO_CHANGE_R:
-			return dccp_feat_change_recv(fn, mandatory, opt, feat,
-						     val, len, server);
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+			if (is_mpdccp(sk) && server && (feat == DCCPF_MULTIPATH))
+				return dccp_mpcap_change_recv(dreq, fn, opt, val, len);
+			else if(!is_mpdccp(sk) && server && (feat == DCCPF_MULTIPATH))
+				dccp_push_empty_confirm(fn, DCCPF_MULTIPATH, 1);
+			else
+#endif
+				return dccp_feat_change_recv(fn, mandatory, opt, feat,
+										     val, len, server);
 		case DCCPO_CONFIRM_R:
 		case DCCPO_CONFIRM_L:
-			return dccp_feat_confirm_recv(fn, mandatory, opt, feat,
-						      val, len, server);
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+			if (is_mpdccp(sk) && !server && (feat == DCCPF_MULTIPATH))
+				return dccp_mpcap_confirm_recv(sk, fn, opt, val, len);
+			else
+#endif
+				return dccp_feat_confirm_recv(fn, mandatory, opt, feat,
+										      val, len, server);
 		}
 		break;
 	/*
@@ -1575,3 +1643,4 @@ int dccp_feat_activate_values(struct sock *sk, struct list_head *fn_list)
 	dp->dccps_hc_rx_ackvec = NULL;
 	return -1;
 }
+EXPORT_SYMBOL(dccp_feat_activate_values);
diff --git a/net/dccp/input.c b/net/dccp/input.c
index 2cbb757a894f8..926a0d594eb33 100644
--- a/net/dccp/input.c
+++ b/net/dccp/input.c
@@ -11,6 +11,10 @@
 #include <linux/slab.h>
 
 #include <net/sock.h>
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+# include <net/mpdccp.h>
+# include "mpdccp.h"
+#endif
 
 #include "ackvec.h"
 #include "ccid.h"
@@ -27,6 +31,17 @@ static void dccp_enqueue_skb(struct sock *sk, struct sk_buff *skb)
 	sk->sk_data_ready(sk);
 }
 
+/* Refresh clocks of a DCCP socket,
+ * ensuring monotically increasing values.
+ */
+static void dccp_mstamp_refresh(struct dccp_sock *dp)
+{
+	u64 val = tcp_clock_ns();
+
+	dp->dccps_clock_cache = val;
+	dp->dccps_mstamp = div_u64(val, NSEC_PER_USEC);
+}
+
 static void dccp_fin(struct sock *sk, struct sk_buff *skb)
 {
 	/*
@@ -137,6 +152,8 @@ static u16 dccp_reset_code_convert(const u8 code)
 	[DCCP_RESET_CODE_BAD_SERVICE_CODE]   = EBADRQC,
 	[DCCP_RESET_CODE_OPTION_ERROR]	     = EILSEQ,
 	[DCCP_RESET_CODE_MANDATORY_ERROR]    = EOPNOTSUPP,
+
+	[DCCP_RESET_CODE_MPDCCP_ABORTED] = ESHUTDOWN,
 	};
 
 	return code >= DCCP_MAX_RESET_CODES ? 0 : error_code[code];
@@ -151,6 +168,19 @@ static void dccp_rcv_reset(struct sock *sk, struct sk_buff *skb)
 	/* Queue the equivalent of TCP fin so that dccp_recvmsg exits the loop */
 	dccp_fin(sk, skb);
 
+	if(err == ESHUTDOWN){				//MPDCCP Fast-Close requires RST reply
+		switch (sk->sk_state) {
+		case DCCP_OPEN:
+			dccp_set_state(sk, DCCP_PASSIVE_CLOSE);
+		case DCCP_PASSIVE_CLOSE:
+			return;
+		case DCCP_ACTIVE_CLOSEREQ:
+			dccp_send_reset(sk, DCCP_RESET_CODE_MPDCCP_ABORTED);
+			dccp_done(sk);
+			return;
+		}
+	}
+
 	if (err && !sock_flag(sk, SOCK_DEAD))
 		sk_wake_async(sk, SOCK_WAKE_IO, POLL_ERR);
 	dccp_time_wait(sk, DCCP_TIME_WAIT, 0);
@@ -169,7 +199,12 @@ static void dccp_handle_ackvec_processing(struct sock *sk, struct sk_buff *skb)
 
 static void dccp_deliver_input_to_ccids(struct sock *sk, struct sk_buff *skb)
 {
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	struct dccp_sock *dp = dccp_sk(sk);
+	dp->dccps_lrcvtime = tcp_jiffies32;
+#else
 	const struct dccp_sock *dp = dccp_sk(sk);
+#endif
 
 	/* Don't deliver to RX CCID when node has shut down read end. */
 	if (!(sk->sk_shutdown & RCV_SHUTDOWN))
@@ -284,10 +319,16 @@ static int __dccp_rcv_established(struct sock *sk, struct sk_buff *skb,
 				  const struct dccp_hdr *dh, const unsigned int len)
 {
 	struct dccp_sock *dp = dccp_sk(sk);
-
 	switch (dccp_hdr(skb)->dccph_type) {
 	case DCCP_PKT_DATAACK:
 	case DCCP_PKT_DATA:
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+		/* Data packets on MP need additional checks */
+		if (is_mpdccp(sk)) {
+			if (mpdccp_rcv_established(sk))
+				goto discard;
+		}
+#endif
 		/*
 		 * FIXME: schedule DATA_DROPPED (RFC 4340, 11.7.2) if and when
 		 * - sk_shutdown == RCV_SHUTDOWN, use Code 1, "Not Listening"
@@ -364,6 +405,8 @@ static int __dccp_rcv_established(struct sock *sk, struct sk_buff *skb,
 int dccp_rcv_established(struct sock *sk, struct sk_buff *skb,
 			 const struct dccp_hdr *dh, const unsigned int len)
 {
+	dccp_mstamp_refresh(dccp_sk(sk)); // alerab
+  
 	if (dccp_check_seqno(sk, skb))
 		goto discard;
 
@@ -371,6 +414,7 @@ int dccp_rcv_established(struct sock *sk, struct sk_buff *skb,
 		return 1;
 
 	dccp_handle_ackvec_processing(sk, skb);
+
 	dccp_deliver_input_to_ccids(sk, skb);
 
 	return __dccp_rcv_established(sk, skb, dh, len);
@@ -420,6 +464,13 @@ static int dccp_rcv_request_sent_state_process(struct sock *sk,
 		if (dccp_parse_options(sk, NULL, skb))
 			return 1;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+		if (is_mpdccp(sk)) {
+			if (mpdccp_rcv_request_sent_state_process(sk, skb))
+				return 1;
+		}
+#endif
+
 		/* Obtain usec RTT sample from SYN exchange (used by TFRC). */
 		if (likely(dp->dccps_options_received.dccpor_timestamp_echo))
 			dp->dccps_syn_rtt = dccp_sample_rtt(sk, 10 * (tstamp -
@@ -494,7 +545,15 @@ static int dccp_rcv_request_sent_state_process(struct sock *sk,
 			__kfree_skb(skb);
 			return 0;
 		}
-		dccp_send_ack(sk);
+
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+		/* For MPDCCP the ACK must be in the rtx queue */
+		if (is_mpdccp(sk) && MPDCCP_CB(sk) && !MPDCCP_CB(sk)->fallback_sp)
+			dccp_send_ack_entail(sk);
+		else
+#endif
+			dccp_send_ack(sk);
+
 		return -1;
 	}
 
@@ -554,8 +613,16 @@ static int dccp_rcv_respond_partopen_state_process(struct sock *sk,
 		}
 
 		dp->dccps_osr = DCCP_SKB_CB(skb)->dccpd_seq;
+
 		dccp_set_state(sk, DCCP_OPEN);
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+		if (is_mpdccp(sk)) {
+			if (mpdccp_rcv_respond_partopen_state_process(sk, dh->dccph_type))
+				break;
+		}
+#endif
+
 		if (dh->dccph_type == DCCP_PKT_DATAACK ||
 		    dh->dccph_type == DCCP_PKT_DATA) {
 			__dccp_rcv_established(sk, skb, dh, len);
@@ -576,7 +643,6 @@ int dccp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 	const int old_state = sk->sk_state;
 	bool acceptable;
 	int queued = 0;
-
 	/*
 	 *  Step 3: Process LISTEN state
 	 *
@@ -671,6 +737,8 @@ int dccp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
 		goto discard;
 	}
 
+	dccp_mstamp_refresh(dp); // alerab
+
 	switch (sk->sk_state) {
 	case DCCP_REQUESTING:
 		queued = dccp_rcv_request_sent_state_process(sk, skb, dh, len);
diff --git a/net/dccp/ipv4.c b/net/dccp/ipv4.c
index 6abfd1c0a5bc6..4bda1553cb365 100644
--- a/net/dccp/ipv4.c
+++ b/net/dccp/ipv4.c
@@ -23,6 +23,10 @@
 #include <net/tcp_states.h>
 #include <net/xfrm.h>
 #include <net/secure_seq.h>
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+#  include <net/mpdccp_link.h>
+#  include <net/mpdccp.h>
+#endif
 
 #include "ackvec.h"
 #include "ccid.h"
@@ -54,6 +58,11 @@ int dccp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
 
 	if (usin->sin_family != AF_INET)
 		return -EAFNOSUPPORT;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if (mpdccp_isactive(sk) > 0 || try_mpdccp(sk) == 1) {
+		return mpdccp_connect (sk, uaddr, addr_len);
+	}
+#endif
 
 	nexthop = daddr = usin->sin_addr.s_addr;
 
@@ -269,7 +278,7 @@ static int dccp_v4_err(struct sk_buff *skb, u32 info)
 		return 0;
 	}
 	seq = dccp_hdr_seq(dh);
-	if (sk->sk_state == DCCP_NEW_SYN_RECV) {
+	if (sk->sk_state == DCCP_NEW_SYN_RECV && type!=ICMP_REDIRECT) {
 		dccp_req_err(sk, seq);
 		return 0;
 	}
@@ -561,6 +570,26 @@ static void dccp_v4_ctl_send_reset(const struct sock *sk, struct sk_buff *rxskb)
 
 static void dccp_v4_reqsk_destructor(struct request_sock *req)
 {
+	struct dccp_request_sock *dreq;
+	dreq = dccp_rsk(req);
+
+	/* Release meta socket reference when request id destroyed */
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if (dreq->link_info) {
+		mpdccp_link_put (dreq->link_info);
+		dreq->link_info = NULL;
+	}
+	if (dreq->meta_sk) {
+		dccp_pr_debug("releasing meta socket %p from request\n", dreq->meta_sk);
+		sock_put(dreq->meta_sk);
+		dreq->meta_sk = NULL;
+	}
+	if (dreq->mpdccp_loc_cix) {
+		mpdccp_link_free_cid(dreq->mpdccp_loc_cix);
+		dreq->mpdccp_loc_cix = 0;
+	}
+#endif
+
 	dccp_feat_list_purge(&dccp_rsk(req)->dreq_featneg);
 	kfree(rcu_dereference_protected(inet_rsk(req)->ireq_opt, 1));
 }
@@ -619,6 +648,13 @@ int dccp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
 	if (dccp_parse_options(sk, dreq, skb))
 		goto drop_and_free;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if ((mpdccp_isactive(sk) > 0) && dreq && (dreq->multipath_ver != MPDCCP_VERS_UNDEFINED)) {
+		if (mpdccp_conn_request(sk, dreq))
+			goto drop_and_free;
+	}
+#endif
+
 	ireq = inet_rsk(req);
 	sk_rcv_saddr_set(req_to_sk(req), ip_hdr(skb)->daddr);
 	sk_daddr_set(req_to_sk(req), ip_hdr(skb)->saddr);
@@ -642,6 +678,11 @@ int dccp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
 	dreq->dreq_gss     = dreq->dreq_iss;
 	dreq->dreq_service = service;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	/* put here path detection - HACK: should be in mpdccp code, but this is a lot of work */
+	dreq->link_info = mpdccp_link_find_by_skb (read_pnet(&sk->sk_net), skb);
+#endif
+
 	if (dccp_v4_send_response(sk, req))
 		goto drop_and_free;
 
@@ -662,8 +703,9 @@ int dccp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
 	struct dccp_hdr *dh = dccp_hdr(skb);
 
 	if (sk->sk_state == DCCP_OPEN) { /* Fast path */
-		if (dccp_rcv_established(sk, skb, dh, skb->len))
+		if (dccp_rcv_established(sk, skb, dh, skb->len)){
 			goto reset;
+		}
 		return 0;
 	}
 
@@ -795,8 +837,8 @@ static int dccp_v4_rcv(struct sk_buff *skb)
 	iph = ip_hdr(skb);
 	/* Step 1: If header checksum is incorrect, drop packet and return */
 	if (dccp_v4_csum_finish(skb, iph->saddr, iph->daddr)) {
-		DCCP_WARN("dropped packet with invalid checksum\n");
-		goto discard_it;
+		//DCCP_WARN("dropped packet with invalid checksum\n");
+		//goto discard_it;
 	}
 
 	dh = dccp_hdr(skb);
@@ -852,6 +894,7 @@ static int dccp_v4_rcv(struct sk_buff *skb)
 		sock_hold(sk);
 		refcounted = true;
 		nsk = dccp_check_req(sk, skb, req);
+
 		if (!nsk) {
 			reqsk_put(req);
 			goto discard_and_relse;
@@ -939,6 +982,11 @@ static int dccp_v4_init_sock(struct sock *sk)
 	return err;
 }
 
+static int inet_dccp_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
+{
+	return inet_bind (sock, uaddr, addr_len);
+}
+
 static struct timewait_sock_ops dccp_timewait_sock_ops = {
 	.twsk_obj_size	= sizeof(struct inet_timewait_sock),
 };
@@ -953,6 +1001,9 @@ static struct proto dccp_v4_prot = {
 	.init			= dccp_v4_init_sock,
 	.setsockopt		= dccp_setsockopt,
 	.getsockopt		= dccp_getsockopt,
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	.keepalive		= dccp_set_keepalive,
+#endif
 	.sendmsg		= dccp_sendmsg,
 	.recvmsg		= dccp_recvmsg,
 	.backlog_rcv		= dccp_v4_do_rcv,
@@ -982,7 +1033,7 @@ static const struct proto_ops inet_dccp_ops = {
 	.family		   = PF_INET,
 	.owner		   = THIS_MODULE,
 	.release	   = inet_release,
-	.bind		   = inet_bind,
+	.bind		   = inet_dccp_bind,
 	.connect	   = inet_stream_connect,
 	.socketpair	   = sock_no_socketpair,
 	.accept		   = inet_accept,
diff --git a/net/dccp/ipv6.c b/net/dccp/ipv6.c
index 83de3952d3783..57d625b463836 100644
--- a/net/dccp/ipv6.c
+++ b/net/dccp/ipv6.c
@@ -29,6 +29,11 @@
 #include <net/secure_seq.h>
 #include <net/sock.h>
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+# include <net/mpdccp.h>
+# include <net/mpdccp_link.h>
+#endif
+
 #include "dccp.h"
 #include "ipv6.h"
 #include "feat.h"
@@ -244,6 +249,21 @@ static int dccp_v6_send_response(const struct sock *sk, struct request_sock *req
 
 static void dccp_v6_reqsk_destructor(struct request_sock *req)
 {
+	struct dccp_request_sock *dreq;
+	dreq = dccp_rsk(req);
+
+	/* Release meta socket reference when request id destroyed */
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if (dreq->link_info) {
+		mpdccp_link_put (dreq->link_info);
+		dreq->link_info = NULL;
+	}
+	if (dreq->meta_sk) {
+		dccp_pr_debug("releasing meta socket %p from request\n", dreq->meta_sk);
+		sock_put(dreq->meta_sk);
+		dreq->meta_sk = NULL;
+	}
+#endif
 	dccp_feat_list_purge(&dccp_rsk(req)->dreq_featneg);
 	kfree(inet_rsk(req)->ipv6_opt);
 	kfree_skb(inet_rsk(req)->pktopts);
@@ -350,9 +370,15 @@ static int dccp_v6_conn_request(struct sock *sk, struct sk_buff *skb)
 	if (dccp_parse_options(sk, dreq, skb))
 		goto drop_and_free;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if ((mpdccp_isactive(sk) > 0) && dreq && (dreq->multipath_ver != MPDCCP_VERS_UNDEFINED)) {
+		if (mpdccp_conn_request(sk, dreq))
+			goto drop_and_free;
+	}
+#endif
+
 	ireq = inet_rsk(req);
 	ireq->ir_v6_rmt_addr = ipv6_hdr(skb)->saddr;
-	ireq->ir_v6_loc_addr = ipv6_hdr(skb)->daddr;
 	ireq->ireq_family = AF_INET6;
 	ireq->ir_mark = inet_request_mark(sk, skb);
 
@@ -827,6 +853,12 @@ static int dccp_v6_connect(struct sock *sk, struct sockaddr *uaddr,
 	if (usin->sin6_family != AF_INET6)
 		return -EAFNOSUPPORT;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if (mpdccp_is_meta(sk)) {
+		return mpdccp_connect (sk, uaddr, addr_len);
+	}
+#endif
+
 	memset(&fl6, 0, sizeof(fl6));
 
 	if (np->sndflow) {
@@ -1020,12 +1052,17 @@ static int dccp_v6_init_sock(struct sock *sk)
 	return err;
 }
 
+static int inet6_dccp_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
+{
+	return inet6_bind (sock, uaddr, addr_len);
+}
+
+>>>>>>> 454a0276d00a2 (Add MP-DCCP support for 5.10.136)
 static struct timewait_sock_ops dccp6_timewait_sock_ops = {
 	.twsk_obj_size	= sizeof(struct dccp6_timewait_sock),
 };
 
 static struct proto dccp_v6_prot = {
-	.name		   = "DCCPv6",
 	.owner		   = THIS_MODULE,
 	.close		   = dccp_close,
 	.connect	   = dccp_v6_connect,
@@ -1063,7 +1100,7 @@ static const struct proto_ops inet6_dccp_ops = {
 	.family		   = PF_INET6,
 	.owner		   = THIS_MODULE,
 	.release	   = inet6_release,
-	.bind		   = inet6_bind,
+	.bind		   = inet6_dccp_bind,
 	.connect	   = inet_stream_connect,
 	.socketpair	   = sock_no_socketpair,
 	.accept		   = inet_accept,
diff --git a/net/dccp/minisocks.c b/net/dccp/minisocks.c
index 64d805b27adde..8f879862e18df 100644
--- a/net/dccp/minisocks.c
+++ b/net/dccp/minisocks.c
@@ -15,12 +15,19 @@
 #include <net/sock.h>
 #include <net/xfrm.h>
 #include <net/inet_timewait_sock.h>
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+#  include <net/mpdccp.h>
+#endif
 
 #include "ackvec.h"
 #include "ccid.h"
 #include "dccp.h"
 #include "feat.h"
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+#  include "mpdccp.h"
+#endif
+
 struct inet_timewait_death_row dccp_death_row = {
 	.tw_refcount = REFCOUNT_INIT(1),
 	.sysctl_max_tw_buckets = NR_FILE * 2,
@@ -192,11 +199,35 @@ struct sock *dccp_check_req(struct sock *sk, struct sk_buff *skb,
 	if (dccp_parse_options(sk, dreq, skb))
 		 goto drop;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if (is_mpdccp(sk)) {
+		/* Copy the negotiated features as they will be purged in syn_recv_sock() */
+		dccp_feat_clone_list(&dreq->dreq_featneg, &dreq->dreq_featneg_mp);
+	}
+#endif
+
 	child = inet_csk(sk)->icsk_af_ops->syn_recv_sock(sk, skb, req, NULL,
 							 req, &own_req);
 	if (child) {
-		child = inet_csk_complete_hashdance(sk, child, req, own_req);
-		goto out;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+			if (is_mpdccp(sk) && own_req) {
+				int ret;
+				struct sock *master_sk;
+				ret = mpdccp_check_req(sk, child, req, skb, &master_sk);
+				if (ret < 0) {
+					dccp_pr_debug("error mpdccp_check_req\n");
+					bh_unlock_sock(child);
+					sock_put(child);
+					child = NULL;
+					goto drop;
+				} else {
+					dccp_pr_debug("after mpdccp_check_req meta %p master %p\n", child, master_sk);
+					child = master_sk;
+				}
+			} else
+#endif
+				child = inet_csk_complete_hashdance(sk, child, req, own_req);
+			goto out;
 	}
 
 	DCCP_SKB_CB(skb)->dccpd_reset_code = DCCP_RESET_CODE_TOO_BUSY;
@@ -223,6 +254,7 @@ int dccp_child_process(struct sock *parent, struct sock *child,
 {
 	int ret = 0;
 	const int state = child->sk_state;
+	dccp_pr_debug("enter child process child %p parent %p\n", child, parent);
 
 	if (!sock_owned_by_user(child)) {
 		ret = dccp_rcv_state_process(child, skb, dccp_hdr(skb),
@@ -264,6 +296,14 @@ int dccp_reqsk_init(struct request_sock *req,
 	inet_rsk(req)->ir_num	   = ntohs(dccp_hdr(skb)->dccph_dport);
 	inet_rsk(req)->acked	   = 0;
 	dreq->dreq_timestamp_echo  = 0;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	dreq->meta_sk              = NULL;
+	dreq->multipath_ver        = MPDCCP_VERS_UNDEFINED;
+	dreq->mpdccp_loc_key.type  = DCCPK_INVALID;
+	dreq->mpdccp_rem_key.type  = DCCPK_INVALID;
+	dreq->mpdccp_loc_key.size  = 0;
+	dreq->mpdccp_rem_key.size  = 0;
+#endif
 
 	/* inherit feature negotiation options from listening socket */
 	return dccp_feat_clone_list(&dp->dccps_featneg, &dreq->dreq_featneg);
diff --git a/net/dccp/mpdccp.h b/net/dccp/mpdccp.h
new file mode 100644
index 0000000000000..8711f986dde52
--- /dev/null
+++ b/net/dccp/mpdccp.h
@@ -0,0 +1,453 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ * 
+ * Copyright (C) 2017 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2017 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2020 by Nathalie Romo, Deutsche Telekom AG
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ * Copyright (C) 2021 by Romeo Cane, Deutsche Telekom AG
+ *
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#ifndef _MPDCCP_H
+#define _MPDCCP_H
+
+#include <net/net_namespace.h>
+#include <linux/netpoll.h>
+#include <linux/rculist.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include "dccp.h"
+#include "ccids/ccid2.h"
+#include "mpdccp_version.h"
+#include <net/mpdccp_link_info.h>
+#include <net/mpdccp.h>
+
+/* Max length for entering an IP
+ * TAG-0.7: Replaced by user space tool */
+#define MPDCCP_IPV4_MAX             16
+
+/* Bitfield representing the supported key types for handshake */
+#define MPDCCP_SUPPKEYS  (DCCPKF_PLAIN)
+/* TODO: add support for all key types */
+//#define MPDCCP_SUPPKEYS  (DCCPKF_PLAIN | DCCPKF_C25519_SHA256 | DCCPKF_C25519_SHA512)
+
+
+/* Defines for MPDCCP options.
+ * All length values account for payload ONLY. */
+#define MPDCCP_LEN_ADD_ADDR4        6
+#define MPDCCP_LEN_ADD_ADDR4_PORT   8
+#define MPDCCP_LEN_ADD_ADDR6        18
+#define MPDCCP_LEN_ADD_ADDR6_PORT   20
+#define MPDCCP_LEN_DEL_ADDR         2
+
+/* Define general reordering parameter */
+#define GLOB_SEQNO_INIT             1        // global sequence number of first packet that is sent, TODO superseed with dynamic approach in connection setup
+
+
+/* Debug output, enabled via sysctl */
+#if 0
+/* Debug that translates socket states to human-readable format.
+ * Unused for now, but this is just to beautiful to delete */
+static const char *mpdccp_state_name(const int state)
+{
+    static const char *const dccp_state_names[] = {
+    [DCCP_OPEN]     = "OPEN",
+    [DCCP_REQUESTING]   = "REQUESTING",
+    [DCCP_PARTOPEN]     = "PARTOPEN",
+    [DCCP_LISTEN]       = "LISTEN",
+    [DCCP_RESPOND]      = "RESPOND",
+    [DCCP_CLOSING]      = "CLOSING",
+    [DCCP_ACTIVE_CLOSEREQ]  = "CLOSEREQ",
+    [DCCP_PASSIVE_CLOSE]    = "PASSIVE_CLOSE",
+    [DCCP_PASSIVE_CLOSEREQ] = "PASSIVE_CLOSEREQ",
+    [DCCP_TIME_WAIT]    = "TIME_WAIT",
+    [DCCP_CLOSED]       = "CLOSED",
+    };
+
+    if (state >= DCCP_MAX_STATES)
+        return "INVALID STATE!";
+    else
+        return dccp_state_names[state];
+}
+#endif
+extern int mpdccp_enabled;
+extern bool mpdccp_debug;
+extern bool mpdccp_accept_prio;
+#ifdef CONFIG_IP_MPDCCP_DEBUG
+#define MPDCCP_PRINTK(enable, fmt, args...)   do { if (enable)                  \
+                            printk_ratelimited(fmt, ##args);                    \
+                        } while(0)
+#define MPDCCP_PR_DEBUG(enable, fmt, a...)    DCCP_PRINTK(enable, KERN_DEBUG    \
+                          "%s: " fmt, __func__, ##a)
+// depends on mpdccp_debug, fixed KERN_DEBUG prio, prints func
+#define mpdccp_pr_debug(format, a...)   MPDCCP_PR_DEBUG(mpdccp_debug, format, ##a)
+// depends on mpdccp_debug, variable prio
+#define mpdccp_pr_debug_cat(format, a...)   MPDCCP_PRINTK(mpdccp_debug, format, ##a)
+// depends on mpdccp_debug, fixed KERN_DEBUG prio,
+#define mpdccp_debug(fmt, a...)         mpdccp_pr_debug_cat(KERN_DEBUG fmt, ##a)
+#define mpdccp_sockinfo(sk)             do { mpdccp_pr_debug("Socket info: %s(%p) Prio %d State %s\n",  \
+                                            dccp_role(sk), sk, mpdccp_my_sock(sk)->priority,            \
+                                            mpdccp_state_name(sk->sk_state));                           \
+                                        } while(0)
+#define mpdccp_pr_info(format, a...) MPDCCP_PRINTK(mpdccp_debug, KERN_INFO format, ##a)
+#else
+#define MPDCCP_PRINTK(enable, fmt, args...) 
+#define MPDCCP_PR_DEBUG(enable, fmt, a...)
+#define mpdccp_pr_debug(format, a...)
+#define mpdccp_pr_debug_cat(format, a...)
+#define mpdccp_debug(format, a...)
+#define mpdccp_pr_info(format, a...)
+#endif
+#define mpdccp_pr_error(format, a...) do { printk (KERN_ERR format, ##a); } while (0)
+
+
+/* List traversal macros allowing for multiple readers and a single 
+ * writer to operate concurrently. These must be called with the rcu_read_lock held */
+#define mpdccp_my_sock(sk)                                                                              \
+        ((struct my_sock *)(sk)->sk_user_data)
+
+static inline void
+set_mpdccp(struct sock *sk, struct mpdccp_cb *mpcb) {
+	dccp_sk(sk)->mpdccp = (struct mpdccp_meta_cb) {
+                                .magic = MPDCCP_MAGIC,
+                                .mpcb = mpcb,
+                                .is_meta = 0 };
+
+}
+
+static inline void
+unset_mpdccp(struct sock *sk) {
+	dccp_sk(sk)->mpdccp = (struct mpdccp_meta_cb) {
+                                .magic = 0 };
+}
+
+/* Iterate over all connections */
+#define mpdccp_for_each_conn(list, mpcb)                                                                \
+    list_for_each_entry_rcu(mpcb, &list, connection_list)
+
+
+#define chk_id(x,y) (x != y) ? x : 0
+
+/**
+ * mpdccp_list_first_or_null_rcu - get the first element from a list
+ * @ptr:        the list head to take the element from.
+ * @type:       the type of the struct this is embedded in.
+ * @member:     the name of the list_head within the struct.
+ *
+ * Note that if the list is empty, it returns NULL.
+ *
+ * This primitive may safely run concurrently with the _rcu list-mutation
+ * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
+ */
+#define mpdccp_list_first_or_null_rcu(ptr, type, member) \
+({ \
+    struct list_head *__ptr = (ptr); \
+    struct list_head *__next = READ_ONCE(__ptr->next); \
+    likely(__ptr != __next) ? list_entry_rcu(__next, type, member)->my_sk_sock : NULL; \
+})
+
+/**
+ * mpdccp_list_next_or_null_rcu - get the first element from a list
+ * @head:   the head for the list.
+ * @ptr:        the list head to take the next element from.
+ * @type:       the type of the struct this is embedded in.
+ * @member:     the name of the list_head within the struct.
+ *
+ * Note that if the ptr is at the end of the list, NULL is returned.
+ *
+ * This primitive may safely run concurrently with the _rcu list-mutation
+ * primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().
+ */
+#define mpdccp_list_next_or_null_rcu(head, ptr, type, member) \
+({ \
+    struct list_head *__head = (head); \
+    struct list_head *__ptr = (ptr); \
+    struct list_head *__next = READ_ONCE(__ptr->next); \
+    likely(__next != __head) ? list_entry_rcu(__next, type, \
+                          member)->my_sk_sock : NULL; \
+})
+
+/* Iterate over all subflows of a connection */
+#define mpdccp_for_each_sk(mpcb, sk)                                                                          \
+    for ((sk) = mpdccp_list_first_or_null_rcu(&(mpcb)->psubflow_list, struct my_sock, sk_list);               \
+    sk && &mpdccp_my_sock((sk))->sk_list != &((mpcb)->psubflow_list);                                         \
+    (sk) = mpdccp_list_next_or_null_rcu(&(mpcb)->psubflow_list, &mpdccp_my_sock((sk))->sk_list, struct my_sock, sk_list)) \
+
+/* Iterate over all listening sockets (server side only) */
+#define mpdccp_for_each_listen_sk(mpcb, sk)                                                                   \
+    for ((sk) = mpdccp_list_first_or_null_rcu(&(mpcb)->plisten_list, struct my_sock, sk_list);                \
+    sk && &mpdccp_my_sock((sk))->sk_list != &((mpcb)->plisten_list);                                          \
+    (sk) = mpdccp_list_next_or_null_rcu(&(mpcb)->plisten_list, &mpdccp_my_sock((sk))->sk_list, struct my_sock, sk_list)) \
+
+/* Iterate over all request sockets (client side only) */
+#define mpdccp_for_each_request_sk(mpcb, sk)                                                                   \
+    for ((sk) = mpdccp_list_first_or_null_rcu(&(mpcb)->prequest_list, struct my_sock, sk_list);                \
+    sk && &mpdccp_my_sock((sk))->sk_list != &((mpcb)->prequest_list);                                          \
+    (sk) = mpdccp_list_next_or_null_rcu(&(mpcb)->prequest_list, &mpdccp_my_sock((sk))->sk_list, struct my_sock, sk_list)) \
+
+enum mpdccp_role {
+    MPDCCP_CLIENT,
+    MPDCCP_SERVER,
+    MPDCCP_SERVER_SUBFLOW,  /* A server-side subflow socket */
+    MPDCCP_MAX_FUNC
+};
+
+
+/* A list of all existing connections */
+extern struct list_head __rcu   pconnection_list;
+extern spinlock_t               pconnection_list_lock;
+/* The MPDCCP work queue */
+extern struct workqueue_struct *mpdccp_wq;
+/* Target connection information */
+struct mpdccp_sched_ops;
+struct mpdccp_reorder_ops;
+struct mpdccp_reorder_path_cb;
+struct mpdccp_pm_ops;
+
+/* This struct holds connection-level (i.e., bundling) information. */
+struct mpdccp_cb {
+	/* List of MPDCCP end-to-end connections */
+	struct list_head        connection_list;
+	/* Pointer to list of per-connection subflows */
+	struct list_head __rcu  psubflow_list;
+	spinlock_t              psubflow_list_lock;
+	/* Pointer to list of listening sockets (server side) */
+	struct list_head __rcu  plisten_list;
+	spinlock_t              plisten_list_lock;
+	/* Pointer to list of request sockets (client side) */
+	struct list_head __rcu  prequest_list;
+	/* Pointer to list of addresses */
+	struct list_head __rcu  paddress_list;
+	/* kref for freeing */
+	struct kref             kref;
+	int			to_be_closed;
+	
+	int     multipath_active;
+	
+	/* Local and remote connection-level Data Sequence Number (DSN). 
+	 * MUST be handled with 48-bit functions. */
+	__u64   dsn_local;                      // TAG-0.9: Currently unused
+	__u64   dsn_remote;                     // TAG-0.9: Currently unused
+	
+	/* meta socket */
+	struct sock		*meta_sk;
+	
+	/* Path manager related data */
+	struct mpdccp_pm_ops    *pm_ops;
+	enum   mpdccp_role      role;               // Is this a client or a server?
+	struct sockaddr_storage mpdccp_local_addr;	// Client only: the target address to connect to
+	int                     localaddr_len;	// length of mpdccp_remote_addr;
+	int			    has_localaddr;
+	struct sockaddr_storage mpdccp_remote_addr; // Client only: the target address to connect to
+	int                     remaddr_len;	// length of mpdccp_remote_addr;
+	u16			    server_port;	// Server only 
+	int			    backlog;
+
+	int			up_reported;
+	u8 			master_addr_id;
+	int  		cnt_remote_addrs;
+
+	/* Scheduler related data */
+	struct mpdccp_sched_ops *sched_ops;
+	int			    has_own_sched;
+	struct sk_buff          *next_skb;      // for schedulers sending the skb on >1 flow
+	int    cnt_subflows;                    // Total number of flows we can use
+	int    cnt_listensocks;
+	bool 	do_incr_oallseq;
+	
+	/* Reordering related data */
+	struct mpdccp_reorder_ops *reorder_ops; 
+	void *mpdccp_reorder_cb;                // pointer to cb structure specific to current reordering 
+	int			      has_own_reorder;
+	u64 glob_lfor_seqno;                    // global sequence number of last forwarded packet
+	u64 mp_oall_seqno;
+
+	/* Authentication data */
+	struct mpdccp_key 	mpdccp_loc_keys[MPDCCP_MAX_KEYS];
+	struct mpdccp_key 	mpdccp_rem_key;
+	u8			dkeyA[MPDCCP_MAX_KEY_SIZE * 2];
+	u8			dkeyB[MPDCCP_MAX_KEY_SIZE * 2];
+	int			dkeylen;
+	u32			mpdccp_loc_cix;
+	u32			mpdccp_rem_cix;
+	int			kex_done;
+	u8			mpdccp_suppkeys;
+	int			cur_key_idx;
+	int			fallback_sp;
+
+	int			close_fast;
+	/* First subflow socket */
+	struct sock*		master_sk;
+
+	/* Namespace info */
+	possible_net_t		net;
+};
+
+/* This struct holds subflow-level information. */
+struct my_sock 
+{
+	/* List of per-connection subflows OR listening sockets (server side only) */
+	struct list_head        sk_list;
+	
+	/* A pointer back to the sock this belongs to */
+	struct sock             *my_sk_sock;
+	struct mpdccp_cb        *mpcb;
+	
+	/* send|recv work. TODO: not sure if i need dynamic memory here to re-queue that work. */
+	struct delayed_work     close_work;
+	struct mpdccp_link_info	*link_info;
+	int			link_cnt;
+	int			link_iscpy;
+	int			up_reported;
+	
+	/* Address ID related data */
+	u8 local_addr_id;
+	u8 remote_addr_id;
+	
+	/* Path manager related data */
+	int     if_idx; /* Interface ID, used for event handling */
+
+/* following data is used for sending options*/
+
+	/* send mp_prio option with next packet:*/
+	u8  announce_prio;
+	bool prio_rcvrd;
+	/* prio was changed with this sequence number */
+	u64 last_prio_seq;
+
+	u8 delpath_id;
+	bool delpath_sent;
+
+	/* addpath[0] is used for the length of the option */
+	u8 addpath[MPDCCP_ADDADDR_SIZE];
+	u8 addpath_hmac[MPDCCP_HMAC_SIZE];
+
+	u64 last_addpath_seq;
+
+	/* temporary memory for received options that require sending a confirm as response  */
+	u8 cnf_cache[MPDCCP_CONFIRM_SIZE];
+	u8 cnf_cache_len;
+
+	/* temporary memory for resending unconfirmed options, biggest possible option is MP_ADDADDR */
+	u8 reins_cache[MPDCCP_ADDADDR_SIZE];
+
+	/* Scheduler related data */
+	/* Limit in Bytes. Dont forget to adjust when increasing the
+	 * size of any scheduler's priv data struct*/
+	#define MPDCCP_SCHED_SIZE 64
+	__u8 sched_priv[MPDCCP_SCHED_SIZE] __aligned(8);
+	
+	/* Used to store the original, unmodified callbacks from 
+	 * struct sock. Additional function pointers available in struct sock:
+	 * void            (*sk_write_space)(struct sock *sk);
+	 * void            (*sk_error_report)(struct sock *sk);
+	 */
+	void            (*sk_write_space)(struct sock *sk);
+	void            (*sk_state_change)(struct sock *sk);
+	void            (*sk_data_ready)(struct sock *sk);
+	int             (*sk_backlog_rcv)(struct sock *sk, 
+	                    struct sk_buff *skb);
+	void            (*sk_destruct)(struct sock *sk);
+	
+	/* Reordering related data */
+	struct mpdccp_reorder_path_cb *pcb;
+
+	/* Close in progress flag */
+	int	closing;
+	int saw_mp_close;
+};
+
+
+/* 
+ * protocol functions
+ */
+
+int mpdccp_init_funcs (void);
+int mpdccp_deinit_funcs (void);
+
+int mpdccp_report_new_subflow (struct sock*);
+int mpdccp_report_destroy (struct sock*);
+int mpdccp_report_alldown (struct sock*);
+
+
+int mpdccp_add_client_conn (struct mpdccp_cb *, struct sockaddr *local, int llen, int if_idx, struct sockaddr *rem, int rlen);
+int mpdccp_reconnect_client (struct sock*, int destroy, struct sockaddr*, int addrlen, int ifidx);
+int mpdccp_add_listen_sock (struct mpdccp_cb *, struct sockaddr *local, int llen, int if_idx);
+int mpdccp_close_subflow (struct mpdccp_cb*, struct sock*, int destroy);
+struct sock *mpdccp_select_ann_sock(struct mpdccp_cb *mpcb, u8 id);
+
+struct mpdccp_cb *mpdccp_alloc_mpcb(void);
+
+int mpdccp_destroy_mpcb(struct mpdccp_cb *mpcb);
+
+struct mpdccp_link_info;
+int mpdccp_ctrl_maycpylink (struct sock *sk);
+struct mpdccp_link_info* mpdccp_ctrl_getlink (struct sock *sk);
+struct mpdccp_link_info* mpdccp_ctrl_getcpylink (struct sock *sk);
+int mpdccp_ctrl_has_cfgchg (struct sock *sk);
+void mpdccp_ctrl_cfgupdate (struct sock *sk);
+
+int mpdccp_forward_skb(struct sk_buff *skb, struct mpdccp_cb *mpcb);
+
+/*
+ * Generic MPDCCP functions
+ */
+
+/* Generic MPDCCP data structure management functions */
+int my_sock_init (struct sock *sk, struct mpdccp_cb *mpcb, int if_idx, enum mpdccp_role role);
+void my_sock_destruct (struct sock *sk);
+/* the real xmit */
+int mpdccp_xmit_to_sk (struct sock *sk, struct sk_buff *skb);
+
+void mpdccp_init_announce_prio(struct sock *sk);
+int mpdccp_get_prio(struct sock *sk);
+int mpdccp_link_cpy_set_prio(struct sock *sk, int prio);
+
+/* Functions for authentication */
+int mpdccp_hash_key(const u8 *in, u8 inlen, u32 *token);
+int mpdccp_hmac_sha256(const u8 *key, u8 keylen, const u8 *msg, u8 msglen, u8 *output);
+int mpdccp_generate_key(struct mpdccp_key *key, int key_type);
+
+static inline struct mpdccp_cb *get_mpcb(const struct sock *sk)
+{
+	return MPDCCP_CB(sk);
+}
+
+static inline struct sock *mpdccp_getmeta (const struct sock *sk)
+{
+	struct mpdccp_cb *mpcb = MPDCCP_CB(sk);
+	return mpcb ? mpcb->meta_sk : NULL;
+}
+
+// Inverse function to dccp_sk()
+static inline struct sock *dccp_sk_inv(const struct dccp_sock *dp)
+{
+	return (struct sock *)dp;
+}
+
+static inline u8 get_id(struct sock *sk)
+{
+    return chk_id(mpdccp_my_sock(sk)->local_addr_id, mpdccp_my_sock(sk)->mpcb->master_addr_id);
+}
+
+static inline void mpdccp_set_accept_prio(int val) { mpdccp_accept_prio = val; }
+
+#endif /* _MPDCCP_H */
+
diff --git a/net/dccp/mpdccp_ctrl.c b/net/dccp/mpdccp_ctrl.c
new file mode 100644
index 0000000000000..31e0bb8cd002c
--- /dev/null
+++ b/net/dccp/mpdccp_ctrl.c
@@ -0,0 +1,1376 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2017 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2017 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2020 by Nathalie Romo, Deutsche Telekom AG
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ * Copyright (C) 2021 by Romeo Cane, Deutsche Telekom AG
+ *
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/kernel.h>
+#include <linux/workqueue.h>
+#include <linux/dccp.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/skbuff.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/inet.h>
+#include <linux/kthread.h>
+#include <uapi/linux/net.h>
+
+#include <net/inet_common.h>
+#include <net/inet_sock.h>
+#include <net/protocol.h>
+#include <net/sock.h>
+#include <net/tcp_states.h>
+#include <linux/inetdevice.h>
+#include <net/mpdccp_link.h>
+#include <asm/unaligned.h>
+#include <crypto/hash.h>
+
+#include "mpdccp.h"
+#include "mpdccp_scheduler.h"
+#include "mpdccp_reordering.h"
+#include "mpdccp_pm.h"
+#include "feat.h"
+
+/*
+ * TODO GLOBAL
+ * 4)   CLEANUP TODO: Free structures, close sockets etc on module exit to avoid crash
+ * 5)   Something like using tcp_read_sock() instead of kernel_recvmsg() would be nice; however, there is 
+ *      currently no such function for DCCP. (about 2% performance gain in TCP)
+ * 8)   Implement fragmentation (unsuppported by Linux DCCP stack)
+ * 9)   Implement service code on both sides (no meaningful service codes are defined by IANA)
+ * 12)  Implement data sequence numbers in a new MPDCCP option
+ * KNOWN BUGS
+ * 1)   For now, this requires CONFIG_NET_L3_MASTER_DEV disabled. When enabled, compute_score() in inet_hashtables.c
+ *      will return -1 as exact_dif==true and sk->sk_bound_dev_if != dif (first is 0/not set). Maybe I need to set 
+ *      sk->sk_bound_dev somewhere? I dont know if this is going to bite me. 
+ *      More info at http://netdevconf.org/1.2/papers/ahern-what-is-l3mdev-paper.pdf
+ * 2)   DCCP in the Linux kernel does not support fragmentation and neither does this bundling solution.
+ */
+
+
+#define MPDCCP_SERVER_BACKLOG	1000
+#define DUMP_PACKET_CONTENT	0
+
+static struct kmem_cache *mpdccp_cb_cache __read_mostly;
+
+
+/* A list of all MPDCCP connections (represented as mpcb's) */
+struct list_head __rcu pconnection_list;
+EXPORT_SYMBOL(pconnection_list);
+
+spinlock_t pconnection_list_lock;
+EXPORT_SYMBOL(pconnection_list_lock);
+
+static struct kmem_cache *mpdccp_mysock_cache __read_mostly;
+
+/* Work queue for all reading and writing to/from the socket. */
+struct workqueue_struct *mpdccp_wq;
+EXPORT_SYMBOL(mpdccp_wq);
+
+/* Crypto contexts for hashing */
+struct crypto_shash *tfm_hash;
+struct crypto_shash *tfm_hmac;
+
+/*********************************************************
+ * Work queue functions
+ *********************************************************/
+
+/*
+ * Atomically queue work on a connection after the specified delay.
+ * Returns 0 if work was queued, or an error code otherwise.
+ */
+//static int queue_bnd_work_delay(struct sock *sk, unsigned long delay)
+//{
+//    struct my_sock *my_sk = mpdccp_my_sock(sk);
+//
+//    if (!queue_delayed_work(mpdccp_wq, &my_sk->work, delay)) {
+//        mpdccp_pr_debug("%p - already queued, wq: %p, work: %p, delay: %lu \n", sk, mpdccp_wq, &my_sk->work, delay);
+//        return -EBUSY;
+//    }
+//
+//    mpdccp_pr_debug("queue ok %p wq: %p, work: %p delay: %lu\n", sk, mpdccp_wq, &my_sk->work, delay);
+//    return 0;
+//}
+
+//static int queue_bnd_work(struct sock *sk)
+//{
+//    return queue_bnd_work_delay(sk, 0);
+//}
+
+//static void mpdccp_cancel_bnd_work(struct sock *sk)
+//{
+//    struct my_sock *my_sk = mpdccp_my_sock(sk);
+//
+//    if (cancel_delayed_work(&my_sk->work)) {
+//        mpdccp_pr_debug(" Work %p cancelled\n", sk);
+//    }
+//}
+
+void mpdccp_wq_flush(void)
+{
+    mpdccp_pr_debug("in mpdccp_wq_flush");
+    flush_workqueue(mpdccp_wq);
+}
+
+static int mpdccp_read_from_subflow (struct sock *sk)
+{
+    int peeked, sz, ret, off=0;
+    struct sk_buff *skb = NULL;
+    struct my_sock *my_sk = mpdccp_my_sock(sk);
+    struct mpdccp_cb *mpcb = my_sk->mpcb;
+    const struct dccp_hdr *dh;
+
+    if(!sk)
+        return -EINVAL;
+
+    //skb = __skb_recv_datagram (sk, MSG_DONTWAIT, NULL, &peeked, &off, &ret);
+    skb = __skb_recv_datagram(sk, &sk->sk_receive_queue, MSG_DONTWAIT, &off, &ret);
+    if (!skb) 
+        return 0;
+
+    sz = skb->len;
+    dh = dccp_hdr(skb);
+
+    switch(dh->dccph_type) {
+        case DCCP_PKT_DATA:
+        case DCCP_PKT_DATAACK:
+            if (sz > 0) {
+                /* Forward skb to reordering engine */
+                mpcb->reorder_ops->do_reorder(mpdccp_init_rcv_buff(sk, skb, mpcb));
+                mpdccp_pr_debug("Read %d bytes from socket %p.\n", sz, sk);
+            } else {
+                mpdccp_pr_debug("Read zero-length data from socket %p, discarding\n", sk);
+                __kfree_skb(skb);
+            }
+            break;
+        case DCCP_PKT_CLOSE:
+        case DCCP_PKT_CLOSEREQ:
+            if (!my_sk->closing) {
+                my_sk->closing = 1;
+                schedule_delayed_work(&my_sk->close_work, 0);
+            }
+        case DCCP_PKT_RESET:
+            __kfree_skb(skb);
+            break;
+        default:
+            mpdccp_pr_debug("unhandled packet type %d from socket %p", dh->dccph_type, sk);
+            sz = -1;
+    }
+    return sz;
+}
+
+int mpdccp_forward_skb(struct sk_buff *skb, struct mpdccp_cb *mpcb)
+{
+	struct sock	*meta_sk;
+	int ret;
+	mpdccp_pr_debug("forward packet\n");
+	if (!skb) return -EINVAL;
+	if (!mpcb || !mpcb->meta_sk) {
+		dev_kfree_skb_any(skb);
+		return -EINVAL;
+	}
+	meta_sk = mpcb->meta_sk;
+	/* we should have a separate setting for rx_qlen, for now use tx_qlen */
+	if (dccp_sk(meta_sk)->dccps_tx_qlen &&
+			meta_sk->sk_receive_queue.qlen >= dccp_sk(meta_sk)->dccps_tx_qlen) {
+		/* drop packet - FIXME: differ between drop oldest and drop newest */
+		//mpdccp_pr_debug ("drop packet - queue full\n");
+		printk ("mpdccp_forward_skb: drop packet - queue full\n");
+		dev_kfree_skb_any(skb);
+		return -ENOBUFS;
+	}
+
+	if (meta_sk->sk_state != DCCP_OPEN) {
+		printk ("mpdccp_forward_skb: drop packet - meta socket not open (state %d)\n", meta_sk->sk_state);
+		dev_kfree_skb_any(skb);
+		return -EINVAL;
+	}
+
+	mpdccp_pr_debug ("enqueue packet\n");
+	ret = sock_queue_rcv_skb(meta_sk, skb);
+	if (ret < 0) {
+		/*
+		 * shouldn't happen
+		 */
+		printk(KERN_ERR "%s: sock_queue_rcv_skb failed! err %d bufsize %d\n",
+		       __func__, ret, meta_sk->sk_rcvbuf);
+		dev_kfree_skb_any(skb);
+	}
+#if 0
+	__skb_queue_tail(&meta_sk->sk_receive_queue, skb);
+	skb_set_owner_r(skb, meta_sk);
+	if (meta_sk->sk_data_ready) meta_sk->sk_data_ready(meta_sk);
+#endif
+
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_forward_skb);
+
+
+/* *********************************
+ * mpcb related functions
+ * *********************************/
+
+void mpdccp_cb_get (struct mpdccp_cb *mpcb)
+{
+	if (!mpcb) return;
+	kref_get (&mpcb->kref);
+}
+
+static void mpdccp_free_mpcb (struct mpdccp_cb *mpcb)
+{
+	if (!mpcb) return;
+	kmem_cache_free (mpdccp_cb_cache, mpcb);
+}
+
+static void mpcb_ref_release (struct kref *ref)
+{
+        struct mpdccp_cb *mpcb = ref ? container_of (ref, struct mpdccp_cb, kref) : NULL;
+	if (!mpcb) return;
+        mpdccp_free_mpcb (mpcb);
+}
+
+void mpdccp_cb_put (struct mpdccp_cb *mpcb)
+{
+	if (!mpcb) return;
+	kref_put (&mpcb->kref, mpcb_ref_release);
+}
+
+struct mpdccp_cb *mpdccp_alloc_mpcb(void)
+{
+    int i;
+    struct mpdccp_cb *mpcb = NULL;
+
+    /* Allocate memory for mpcb */
+    mpcb = kmem_cache_zalloc(mpdccp_cb_cache, GFP_ATOMIC);
+    if (!mpcb) {
+        mpdccp_pr_debug("Failed to initialize mpcb.\n");
+        return NULL;
+    }
+
+    /* No locking needed, as nobody can access the struct yet */
+    INIT_LIST_HEAD(&mpcb->psubflow_list);
+    INIT_LIST_HEAD(&mpcb->plisten_list);
+    INIT_LIST_HEAD(&mpcb->prequest_list);
+    INIT_LIST_HEAD(&mpcb->paddress_list);
+    spin_lock_init(&mpcb->psubflow_list_lock);
+    spin_lock_init(&mpcb->plisten_list_lock);
+
+    mpcb->to_be_closed = 0;
+    mpcb->cnt_subflows      = 0;
+    mpcb->cnt_remote_addrs  = 0;
+    mpcb->multipath_active  = 1;     //socket option; always active for now
+    mpcb->dsn_local  = 0;
+    mpcb->dsn_remote = 0;
+
+    /* TODO: move the two settings below to sysctl controls */
+    mpcb->mpdccp_suppkeys = MPDCCP_SUPPKEYS;
+    mpcb->glob_lfor_seqno = GLOB_SEQNO_INIT;
+    mpcb->mp_oall_seqno = GLOB_SEQNO_INIT;
+    for (i=0; i < MPDCCP_MAX_KEYS; i++) {
+        mpcb->mpdccp_loc_keys[i].type = DCCPK_INVALID;
+        mpcb->mpdccp_loc_keys[i].size = 0;
+    }
+    mpcb->mpdccp_rem_key.type  = DCCPK_INVALID;
+    mpcb->mpdccp_rem_key.size  = 0;
+    mpcb->cur_key_idx = 0;
+    mpcb->master_addr_id = 0;
+
+    kref_init (&mpcb->kref);
+
+    mpdccp_init_path_manager(mpcb);
+    mpdccp_init_scheduler(mpcb);
+    mpdccp_init_reordering(mpcb);
+
+    spin_lock_bh(&pconnection_list_lock);
+    list_add_tail_rcu(&mpcb->connection_list, &pconnection_list);
+    mpdccp_pr_debug("Added new entry to pconnection_list @ %p\n", mpcb);
+    spin_unlock_bh(&pconnection_list_lock);
+
+    mpdccp_pr_debug("Sucessfully initialized mpcb at %p.\n", mpcb);
+    
+    return mpcb;
+}
+
+int mpdccp_destroy_mpcb(struct mpdccp_cb *mpcb)
+{
+	struct sock	*sk;
+	int		ret;
+	struct list_head *pos, *temp;
+	struct my_sock *mysk;
+
+	if (!mpcb) return -EINVAL;
+
+	/* Delete the mpcb from the list of MPDCCP connections */
+	spin_lock(&pconnection_list_lock);
+	list_del_rcu(&mpcb->connection_list);
+	INIT_LIST_HEAD(&mpcb->connection_list);
+	spin_unlock(&pconnection_list_lock);
+	mpcb->to_be_closed = 1;
+
+	if(mpcb->pm_ops->del_addr)
+		mpcb->pm_ops->del_addr(mpcb, 0, 0, 1);
+
+	/* close all subflows */
+	list_for_each_safe(pos, temp, &((mpcb)->psubflow_list)) {
+		mysk = list_entry(pos, struct my_sock, sk_list);
+		if (mysk) {
+			sk = mysk->my_sk_sock;
+			ret = mpdccp_close_subflow(mpcb, sk, 1);
+			if (ret < 0) {
+				mpdccp_pr_debug ("error closing subflow: %d\n", ret);
+				return ret;
+			}
+		}
+	}
+
+	/* close all listening sockets */
+	list_for_each_safe(pos, temp, &((mpcb)->plisten_list)) {
+		mysk = list_entry(pos, struct my_sock, sk_list);
+		if (mysk) {
+			sk = mysk->my_sk_sock;	
+			ret = mpdccp_close_subflow(mpcb, sk, 1);
+			if (ret < 0) {
+				mpdccp_pr_debug ("error closing listen socket: %d\n", ret);
+				return ret;
+			}
+		}
+	}
+
+	/* close all request sockets */
+	list_for_each_safe(pos, temp, &((mpcb)->prequest_list)) {
+		mysk = list_entry(pos, struct my_sock, sk_list);
+		if (mysk) {
+			sk = mysk->my_sk_sock;
+			/* Only force close on the sockets in REQUESTING state */
+			if (sk->sk_state == DCCP_REQUESTING) {
+				ret = mpdccp_close_subflow(mpcb, sk, 1);
+				if (ret < 0) {
+					mpdccp_pr_debug ("error closing request socket: %d\n", ret);
+					return ret;
+				}
+			}
+		}
+	}
+
+	mpdccp_cleanup_reordering(mpcb);
+	mpdccp_cleanup_scheduler(mpcb);
+	mpdccp_cleanup_path_manager(mpcb);
+
+	/* release and eventually free mpcb */   
+    mpdccp_link_free_cid(mpcb->mpdccp_loc_cix);
+	mpdccp_cb_put (mpcb);
+	
+	return 0;
+}
+
+
+/******************************************************
+ * 'mysock' custom functions
+ ******************************************************/
+int listen_backlog_rcv (struct sock *sk, struct sk_buff *skb);
+void listen_data_ready (struct sock *sk);
+void mp_state_change (struct sock *sk);
+
+/* TODO: Differentiate between SUBFLOW and LISTEN socket list!!!
+ * Free additional structures and call sk_destruct on the socket*/
+int my_sock_pre_destruct (struct sock *sk)
+{
+    struct my_sock   *my_sk = mpdccp_my_sock(sk);
+    struct mpdccp_cb *mpcb = NULL;
+    int found = 0;
+    int rem_subflows = 0;
+
+    if (!my_sk) return 0;
+
+    /* Delete this subflow from the list of mpcb subflows */
+    mpcb = my_sk->mpcb;
+    if (mpcb) {
+        struct my_sock *pos, *temp;
+        spin_lock(&mpcb->psubflow_list_lock);
+        list_for_each_entry_safe(pos, temp, &((mpcb)->psubflow_list), sk_list) {
+            if (my_sk == pos) {
+                sk = my_sk->my_sk_sock;
+                list_del_rcu(&my_sk->sk_list);
+                INIT_LIST_HEAD(&my_sk->sk_list);
+                mpcb->cnt_subflows--;
+                rem_subflows = mpcb->cnt_subflows;
+                found = 1;
+                break;
+            }
+        }
+        /* If not found in mpcb subflows, it might be in the request list */
+        if (!found) {
+            list_for_each_entry_safe(pos, temp, &((mpcb)->prequest_list), sk_list) {
+                if (my_sk == pos) {
+                    sk = my_sk->my_sk_sock;
+                    list_del_rcu(&my_sk->sk_list);
+                    INIT_LIST_HEAD(&my_sk->sk_list);
+                    break;
+                }
+            }
+        }
+        spin_unlock(&mpcb->psubflow_list_lock);
+    }
+
+    /* report socket destruction */
+    mpdccp_report_destroy (sk);
+
+    /* release link_info struct */
+    if (my_sk->link_info) {
+        const char *name = MPDCCP_LINK_NAME (my_sk->link_info);
+        mpdccp_pr_debug ("remove subflow %p (%s: %d)\n", sk,
+                name ? name : "<copied link>", my_sk->link_info->id);
+        mpdccp_link_put (my_sk->link_info);
+        my_sk->link_info = NULL;
+    }
+
+    sk->sk_user_data = NULL;
+
+    /* Restore old function pointers */
+    sk->sk_data_ready       = my_sk->sk_data_ready;
+    sk->sk_backlog_rcv      = my_sk->sk_backlog_rcv;
+    sk->sk_destruct         = my_sk->sk_destruct;
+    sk->sk_state_change     = my_sk->sk_state_change;
+    sk->sk_write_space      = my_sk->sk_write_space;
+
+    if (my_sk->pcb)
+        mpdccp_free_reorder_path_cb(my_sk->pcb);
+
+    kmem_cache_free(mpdccp_mysock_cache, my_sk);
+
+    mpdccp_pr_debug ("subflow %p removed from mpcb %p (found %d), remaining subflows: %d", sk, mpcb, found, rem_subflows);
+    return found;
+}
+
+void my_sock_final_destruct (struct sock *sk, struct mpdccp_cb *mpcb, int found)
+{
+    if (!sk) return;
+    if (found && mpcb && (mpcb->meta_sk->sk_state != DCCP_CLOSED) && mpcb->cnt_subflows == 0) {
+        struct sock	*msk = mpcb->meta_sk;
+        mpdccp_pr_debug ("closing meta %p\n", msk);
+        sock_hold (msk);
+        dccp_done(msk);
+        mpdccp_report_alldown (msk);
+        sock_put (msk);
+    }
+    if (mpcb) mpdccp_cb_put (mpcb);
+}
+
+void my_sock_destruct (struct sock *sk)
+{
+    struct my_sock   *my_sk = mpdccp_my_sock(sk);
+    struct mpdccp_cb *mpcb = my_sk ? my_sk->mpcb : NULL;
+    int              found=0;
+
+    if (!sk || !my_sk) return;
+    found = my_sock_pre_destruct (sk);
+    my_sock_final_destruct (sk, mpcb, found);
+    if (sk->sk_destruct)
+        sk->sk_destruct (sk);
+}
+
+
+static void mpdccp_close_worker(struct work_struct *work)
+{
+    struct my_sock *my_sk;
+    struct sock *sk;
+    struct socket_wq *wq;
+    my_sk = container_of(work, struct my_sock, close_work.work);
+    sk = my_sk->my_sk_sock;
+
+    /* Finish the passive close stuff before final closure */
+    if ((sk->sk_state == DCCP_PASSIVE_CLOSE)
+        || (sk->sk_state == DCCP_PASSIVE_CLOSEREQ)) {
+
+            if (dccp_sk(sk)->dccps_role == DCCP_ROLE_CLIENT) {
+                /* Wipe any ACK retranmission queue */
+                inet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);
+                if (sk->sk_send_head != NULL) {
+                    kfree_skb(sk->sk_send_head);
+                    sk->sk_send_head = NULL;
+                }
+            }
+            if(my_sk->mpcb->close_fast)
+                dccp_set_state(sk, DCCP_CLOSED);
+            else
+                dccp_finish_passive_close(sk);
+    }
+
+    rcu_read_lock();
+    wq = rcu_dereference(sk->sk_wq);
+    rcu_read_unlock();
+
+    /* Try again later if the wq is not empty, otherwise dccp_close will invalidate it causing a crash */
+    if (skwq_has_sleeper(wq))
+        schedule_delayed_work(&mpdccp_my_sock(sk)->close_work, msecs_to_jiffies(200));
+    else{
+        dccp_sk(sk)->is_fast_close = my_sk->mpcb->close_fast;
+        dccp_close(sk, 0);
+    }
+}
+
+void subflow_write_space(struct sock *sk)
+{
+    struct my_sock *my_sk = (struct my_sock *)sk->sk_user_data;
+    struct mpdccp_cb *mpcb  = my_sk ? my_sk->mpcb : NULL;
+
+    /* Avoid waking any sleeper if the state is not OPEN or CLOSED */
+    if (my_sk && ((sk->sk_state == DCCP_OPEN || sk->sk_state == DCCP_CLOSED) || (mpcb && mpcb->fallback_sp)))
+        my_sk->sk_write_space(sk);
+}
+
+int my_sock_init (struct sock *sk, struct mpdccp_cb *mpcb, int if_idx, enum mpdccp_role role)
+{
+    struct my_sock *my_sk;
+    int ret = 0;
+    mpdccp_pr_debug("Enter my_sock_init().\n");
+
+    if (role == MPDCCP_CLIENT) {
+        ret = dccp_feat_register_sp(sk, DCCPF_MULTIPATH, 0,
+                                    mpdccp_supported_versions,
+                                    ARRAY_SIZE(mpdccp_supported_versions));
+        if (ret < 0 ) {
+            sk->sk_user_data = NULL;
+            goto out;
+        }
+    }
+
+    my_sk = kmem_cache_zalloc(mpdccp_mysock_cache, GFP_ATOMIC);
+    if (!my_sk) {
+        mpdccp_pr_debug("no mem for my_sk\n");
+        ret = -ENOBUFS;
+        goto out;
+    }
+
+    /* No locking needed, as readers do not yet have access to the structure */
+    INIT_LIST_HEAD(&my_sk->sk_list);
+    my_sk->my_sk_sock   = sk;
+    my_sk->mpcb         = mpcb;
+    my_sk->if_idx       = if_idx;
+    my_sk->pcb          = NULL;
+
+    /* Init private scheduler data */
+    memset(my_sk->sched_priv, 0, MPDCCP_SCHED_SIZE);
+
+    /* Save the original socket callbacks before rewriting */
+
+    mpdccp_pr_debug("role %d my_sk %p", role, my_sk);
+    my_sk->sk_data_ready    = sk->sk_data_ready;
+    my_sk->sk_backlog_rcv   = sk->sk_backlog_rcv;
+    my_sk->sk_destruct      = sk->sk_destruct;
+
+    sk->sk_data_ready       = listen_data_ready;
+    sk->sk_backlog_rcv      = listen_backlog_rcv;
+    sk->sk_destruct         = my_sock_destruct;
+    sk->sk_user_data        = my_sk;
+
+    if (role == MPDCCP_CLIENT) {
+        my_sk->sk_state_change  = sk->sk_state_change;
+        sk->sk_state_change     = mp_state_change;
+
+        my_sk->sk_write_space   = sk->sk_write_space;
+        sk->sk_write_space      = subflow_write_space;
+    }
+
+    mpdccp_pr_debug("role %d sk %p kex_done %d", role, sk, mpcb->kex_done);
+    if (!mpcb->kex_done) {
+        /* Set the kex flag for this socket if no key exchange has been done before */
+        dccp_sk(sk)->is_kex_sk = 1;
+
+        if (role == MPDCCP_CLIENT || role == MPDCCP_SERVER) {
+            int i;
+            u8 supp_keys = MPDCCP_SUPPKEYS;
+            /* Generate local keys for each supported type */
+            for (i=0; supp_keys && (i < MPDCCP_MAX_KEYS); i++) {
+                u32 keytype = ffs(supp_keys) - 1;
+                ret = mpdccp_generate_key(&mpcb->mpdccp_loc_keys[i], keytype);
+                if (ret) {
+                    mpdccp_pr_debug("error generating key type %d\n", keytype);
+                    break;
+                }
+                supp_keys &= ~(1 << keytype);
+            }
+        }
+    } else  {
+        if (role == MPDCCP_CLIENT) {
+            /* Generate local nonce */
+            get_random_bytes(&dccp_sk(sk)->mpdccp_loc_nonce, 4);
+            mpdccp_pr_debug("client: generated nonce %x\n", dccp_sk(sk)->mpdccp_loc_nonce);
+        }
+    }
+
+    // Memory is already reserved in struct my_sk
+    mpdccp_pr_debug("Entered my_sock_init\n");
+
+    INIT_DELAYED_WORK(&my_sk->close_work, mpdccp_close_worker);
+    /* Make sure refcount for this module is increased */
+out:
+    return ret;
+}
+EXPORT_SYMBOL_GPL(my_sock_init);
+
+
+int
+mpdccp_ctrl_maycpylink (struct sock *sk)
+{
+    struct my_sock 		*my_sk;
+    struct mpdccp_link_info	*link, *oldlink;
+    int				ret;
+
+    if (!sk) return -EINVAL;
+    my_sk = mpdccp_my_sock (sk);
+    if (!my_sk) return -EINVAL;	/* no mpdccp socket */
+    if (my_sk->link_iscpy) return 0;	/* already copied */
+    ret = mpdccp_link_copy (&link, my_sk->link_info);
+    if (ret < 0) {
+	mpdccp_pr_error ("cannot copy link_info: %d", ret);
+	return ret;
+    }
+    rcu_read_lock ();
+    oldlink = xchg ((__force struct mpdccp_link_info **)&my_sk->link_info, link);
+    my_sk->link_iscpy = 1;
+    strncpy (link->ndev_name, oldlink->ndev_name, IFNAMSIZ+1);
+    rcu_read_unlock ();
+    mpdccp_link_put (oldlink);
+    return 0;
+}
+EXPORT_SYMBOL (mpdccp_ctrl_maycpylink);
+
+struct mpdccp_link_info*
+mpdccp_ctrl_getlink (struct sock *sk)
+{
+    struct my_sock 		*my_sk;
+    struct mpdccp_link_info	*link;
+
+    if (!sk) return NULL;
+    my_sk = mpdccp_my_sock (sk);
+    if (!my_sk) return NULL;	/* no mpdccp socket */
+    rcu_read_lock();
+    link = my_sk->link_info;
+    mpdccp_link_get (link);
+    rcu_read_unlock();
+    return link;
+}
+EXPORT_SYMBOL (mpdccp_ctrl_getlink);
+
+struct mpdccp_link_info*
+mpdccp_ctrl_getcpylink (struct sock *sk)
+{
+    int				ret;
+    struct mpdccp_link_info	*link;
+
+    rcu_read_lock();
+    ret = mpdccp_ctrl_maycpylink (sk);
+    if (ret < 0) {
+	rcu_read_unlock();
+	return NULL;
+    }
+    link = mpdccp_ctrl_getlink (sk);
+    rcu_read_unlock();
+    return link;
+}
+EXPORT_SYMBOL (mpdccp_ctrl_getcpylink);
+
+int
+mpdccp_ctrl_has_cfgchg (struct sock *sk)
+{
+    struct my_sock 		*my_sk;
+    int				ret;
+
+    if (!sk) return 0;
+    my_sk = mpdccp_my_sock (sk);
+    if (!my_sk) return 0;	/* no mpdccp socket */
+    rcu_read_lock();
+    ret = (my_sk->link_cnt != mpdccp_link_cnt(my_sk->link_info));
+    rcu_read_unlock();
+    return ret;
+}
+EXPORT_SYMBOL (mpdccp_ctrl_has_cfgchg);
+
+void
+mpdccp_ctrl_cfgupdate (struct sock *sk)
+{
+    struct my_sock 		*my_sk;
+
+    if (!sk) return;
+    my_sk = mpdccp_my_sock (sk);
+    if (!my_sk) return;	/* no mpdccp socket */
+    rcu_read_lock();
+    my_sk->link_cnt = mpdccp_link_cnt(my_sk->link_info);
+    rcu_read_unlock();
+}
+EXPORT_SYMBOL (mpdccp_ctrl_cfgupdate);
+
+#define LINK_UD_MAGIC	0x33a9c478
+struct link_user_data {
+	int			magic;
+	void			*user_data;
+	struct mpdccp_link_info	*link_info;
+};
+
+
+
+/* ****************************************************************************
+ *  add / remove subflows - called by path manager
+ * ****************************************************************************/
+
+/* This function adds new sockets to existing connections:
+ * - Attempts to establish connections to another endpoint as a client.
+ */
+
+int mpdccp_add_client_conn (	struct mpdccp_cb *mpcb,
+				struct sockaddr *local_address,
+				int locaddr_len,
+				int if_idx,
+				struct sockaddr *remote_address,
+				int remaddr_len)
+{
+	int			ret = 0;
+	int 			flags;
+	struct socket   	*sock; /* The newly created socket */
+	struct sock     	*sk;
+	struct mpdccp_link_info	*link_info = NULL;
+	
+	if (!mpcb || !local_address || !remote_address) return -EINVAL;
+	if (mpcb->role != MPDCCP_CLIENT) return -EINVAL;
+	
+	/* Create a new socket */
+	ret = sock_create_kern(read_pnet(&mpcb->net), PF_INET, SOCK_DCCP, IPPROTO_DCCP, &sock);
+	if (ret < 0) {
+		mpdccp_pr_debug("Failed to create socket (%d).\n", ret);
+		goto out;
+	}
+	
+	sk = sock->sk;
+
+	/* Get the service type from meta socket */
+	dccp_sk(sk)->dccps_service = dccp_sk(mpcb->meta_sk)->dccps_service;
+
+	set_mpdccp(sk, mpcb);
+	
+	ret = my_sock_init (sk, mpcb, if_idx, MPDCCP_CLIENT);
+	if (ret < 0) {
+		mpdccp_pr_debug("Failed to init mysock (%d).\n", ret);
+		sock_release(sock);
+		goto out;
+	}
+	
+	/* Bind the socket to one of the DCCP-enabled IP addresses */
+	ret = kernel_bind(sock, local_address, locaddr_len);
+	if (ret < 0) {
+		mpdccp_pr_debug("Failed to bind socket %p (%d).\n", sk, ret);
+		sock_release(sock);
+		goto out;
+	}
+	if (local_address->sa_family == AF_INET) {
+        int loc_id = 0;
+        union inet_addr addr;
+		struct sockaddr_in 	*local_v4_address = (struct sockaddr_in*)local_address;
+		link_info = mpdccp_link_find_ip4 (&init_net, &local_v4_address->sin_addr, NULL);
+
+        addr.in = local_v4_address->sin_addr;
+        if(mpcb->pm_ops->claim_local_addr)
+            loc_id = mpcb->pm_ops->claim_local_addr(mpcb, AF_INET, &addr);
+
+        if(loc_id < 1){
+            dccp_pr_debug("cant create subflow with unknown address id");
+		    sock_release(sock);
+		    goto out;
+        }
+        if(mpcb->master_addr_id == 0){
+            mpcb->master_addr_id = loc_id;
+            dccp_pr_debug("master_addr_id set %u", mpcb->master_addr_id);
+        }
+        mpdccp_my_sock(sk)->local_addr_id = loc_id;
+	} else if (local_address->sa_family == AF_INET6) {
+		struct sockaddr_in6 	*local_v6_address = (struct sockaddr_in6*)local_address;
+		link_info = mpdccp_link_find_ip6 (&init_net, &local_v6_address->sin6_addr, NULL);
+	}
+	if (!link_info) link_info = mpdccp_getfallbacklink (&init_net);
+	
+	mpdccp_my_sock(sk)->link_info = link_info;
+	mpdccp_my_sock(sk)->link_cnt = mpdccp_link_cnt(link_info);
+	mpdccp_my_sock(sk)->link_iscpy = 0;
+
+	/* Add socket to the request list */
+	spin_lock(&mpcb->psubflow_list_lock);
+	list_add_tail_rcu(&mpdccp_my_sock(sk)->sk_list , &mpcb->prequest_list);
+	mpdccp_pr_debug("Added new entry to prequest_list @ %p\n", mpdccp_my_sock(sk));
+	spin_unlock(&mpcb->psubflow_list_lock);
+
+	/* Only the first (key exchage) socket is blocking */
+	flags = dccp_sk(sk)->is_kex_sk ? 0 : O_NONBLOCK;
+
+	ret = kernel_connect(sock, remote_address, remaddr_len, flags);
+	if ((ret < 0) && (ret != -EINPROGRESS)) {
+#ifdef CONFIG_IP_MPDCCP_DEBUG
+		struct sockaddr_in *their_inaddr_ptr = (struct sockaddr_in *)remote_address; 
+		mpdccp_pr_debug("Failed to connect from sk %pI4 %p (%d).\n",
+			&their_inaddr_ptr->sin_addr, sk, ret);
+#endif
+		sock_release(sock);
+		goto out;
+	}
+
+	if (dccp_sk(sk)->is_kex_sk && mpcb->kex_done) {
+		struct inet_sock *inet_meta = inet_sk(mpcb->meta_sk);
+		struct inet_sock *inet_sub = inet_sk(sk);
+
+		/* MP_KEY sockets can be authorized now. MP_JOIN sockets need to wait one more more ack */
+		dccp_sk(sk)->auth_done = 1;
+
+		/* Reset the flag to avoid inserting MP_KEY options in subsenquent ACKs */
+		dccp_sk(sk)->is_kex_sk = 0;
+
+		/* Update the state and MSS of meta socket */
+		dccp_sk(mpcb->meta_sk)->dccps_mss_cache = dccp_sk(sk)->dccps_mss_cache;
+		mpcb->meta_sk->sk_state = DCCP_OPEN;
+
+		/* Update meta socket inet data with info from subflow */
+		inet_meta->inet_sport = inet_sub->inet_sport;
+		inet_meta->inet_dport = inet_sub->inet_dport;
+		inet_meta->inet_saddr = inet_sub->inet_saddr;
+		inet_meta->inet_daddr = inet_sub->inet_daddr;
+	}
+
+	if (mpcb->fallback_sp) {
+		struct inet_sock *inet_meta = inet_sk(mpcb->meta_sk);
+		struct inet_sock *inet_sub = inet_sk(sk);
+
+		/* Update the state and MSS of meta socket */
+		dccp_sk(mpcb->meta_sk)->dccps_mss_cache = dccp_sk(sk)->dccps_mss_cache;
+		mpcb->meta_sk->sk_state = DCCP_OPEN;
+		dccp_sk(sk)->is_kex_sk = 0;
+
+		/* Update meta socket inet data with info from subflow */
+		inet_meta->inet_sport = inet_sub->inet_sport;
+		inet_meta->inet_dport = inet_sub->inet_dport;
+		inet_meta->inet_saddr = inet_sub->inet_saddr;
+		inet_meta->inet_daddr = inet_sub->inet_daddr;
+	}
+out:
+	return ret;
+}
+EXPORT_SYMBOL (mpdccp_add_client_conn);
+
+int mpdccp_reconnect_client (  struct sock *sk,
+                               int destroy,
+                               struct sockaddr *local_address,
+                               int locaddr_len,
+                               int if_idx)
+{
+    struct my_sock   *my_sk = mpdccp_my_sock(sk);
+    struct mpdccp_cb *mpcb = my_sk ? my_sk->mpcb : NULL;
+    int              found, ret;
+
+    if (!sk || !my_sk) return -EINVAL;
+
+    if (mpdccp_my_sock(sk)->delpath_sent){
+        found = my_sock_pre_destruct (sk);
+        my_sock_final_destruct (sk, mpcb, found);
+        unset_mpdccp(sk);
+        return 0;
+    }
+
+    if (!destroy)
+       found = my_sock_pre_destruct (sk);
+    mpdccp_pr_debug("try to reconnect sk address %pI4. if %d \n", &sk->__sk_common.skc_rcv_saddr, if_idx);
+    ret = mpdccp_add_client_conn(mpcb, local_address, locaddr_len, if_idx,
+                                (struct sockaddr*)&mpcb->mpdccp_remote_addr,
+                                mpcb->remaddr_len);
+    if (ret) {
+       mpdccp_pr_debug("reconnecting to sk address %pI4 (if %d) failed: %d\n",
+                       &sk->__sk_common.skc_rcv_saddr, if_idx, ret);
+       unset_mpdccp(sk);
+       if (!destroy)
+               my_sock_final_destruct (sk, mpcb, found);
+       return ret;
+    }
+    if (destroy)
+       return mpdccp_close_subflow(mpcb, sk, 1);
+    return 0;
+}
+EXPORT_SYMBOL (mpdccp_reconnect_client);
+
+
+int mpdccp_add_listen_sock (	struct mpdccp_cb *mpcb,
+				struct sockaddr *local_address,
+				int locaddr_len,
+				int if_idx)
+{
+    int                 retval	= 0;
+    struct socket   	*sock; /* The newly created socket */
+    struct sock     	*sk;
+    struct mpdccp_link_info	*link_info = NULL;
+
+    if (!mpcb || !local_address) return -EINVAL;
+    if (mpcb->role != MPDCCP_SERVER) return -EINVAL;
+
+    mpdccp_pr_debug ("Create subflow socket\n");
+    /* Create a new socket */
+    retval = sock_create(PF_INET, SOCK_DCCP, IPPROTO_DCCP, &sock);
+    if (retval < 0) {
+        mpdccp_pr_debug("Failed to create socket (%d).\n", retval);
+        goto out;
+    }
+    sock->sk->sk_reuse = SK_FORCE_REUSE;
+
+    sk = sock->sk;
+    set_mpdccp(sk, mpcb);
+
+    mpdccp_pr_debug ("init mysock\n");
+    retval = my_sock_init (sk, mpcb, if_idx, MPDCCP_SERVER);
+    if (retval < 0) {
+        mpdccp_pr_debug("Failed to init mysock (%d).\n", retval);
+        sock_release(sock);
+        goto out;
+    }
+
+    mpdccp_pr_debug ("bind address: %pISc\n", local_address);
+    /* Bind the socket to one of the DCCP-enabled IP addresses */
+    retval = sock->ops->bind(sock, local_address, locaddr_len);
+    if (retval < 0) {
+        mpdccp_pr_debug("Failed to bind socket %p (%d).\n", sk, retval);
+        sock_release(sock);
+        goto out;
+    }
+    if (local_address->sa_family == AF_INET) {
+         struct sockaddr_in 	*local_v4_address = (struct sockaddr_in*)local_address;
+         link_info = mpdccp_link_find_ip4 (&init_net, &local_v4_address->sin_addr, NULL);
+    } else if (local_address->sa_family == AF_INET6) {
+         struct sockaddr_in6 	*local_v6_address = (struct sockaddr_in6*)local_address;
+         link_info = mpdccp_link_find_ip6 (&init_net, &local_v6_address->sin6_addr, NULL);
+    }
+    if (!link_info) link_info = mpdccp_getfallbacklink (&init_net);
+
+    mpdccp_my_sock(sk)->link_info = link_info;
+    mpdccp_my_sock(sk)->link_cnt = mpdccp_link_cnt(link_info);
+    mpdccp_my_sock(sk)->link_iscpy = 0;
+
+
+    mpdccp_pr_debug ("set subflow to listen state\n");
+    retval = sock->ops->listen(sock, MPDCCP_SERVER_BACKLOG);
+    if (retval < 0) {
+        mpdccp_pr_debug("Failed to listen on socket(%d).\n", retval);
+        sock_release(sock);
+        goto out;
+    }
+
+    spin_lock(&mpcb->plisten_list_lock);
+    list_add_tail_rcu(&mpdccp_my_sock(sk)->sk_list , &mpcb->plisten_list);
+    mpcb->cnt_listensocks++;
+    mpdccp_pr_debug("Added new entry to plisten_list @ %p\n", mpdccp_my_sock(sk));
+    spin_unlock(&mpcb->plisten_list_lock);
+
+    mpdccp_pr_debug("server port added successfully. There are %d subflows now.\n",
+			mpcb->cnt_subflows);
+
+out:
+    return retval;
+}
+EXPORT_SYMBOL (mpdccp_add_listen_sock);
+
+int mpdccp_close_subflow (struct mpdccp_cb *mpcb, struct sock *sk, int destroy)
+{
+    if (!mpcb || !sk || !mpdccp_my_sock(sk)) return -EINVAL;
+    mpdccp_pr_debug("enter for %p role %s state %d closing %d", sk, dccp_role(sk), sk->sk_state, mpdccp_my_sock(sk)->closing);
+
+    if(mpcb->pm_ops->del_retrans)
+        mpcb->pm_ops->del_retrans(sock_net(mpcb->meta_sk), sk);
+
+    if(mpcb->close_fast == 2 && sk->sk_state == DCCP_OPEN){
+        dccp_set_state(sk, DCCP_PASSIVE_CLOSE);
+    }
+
+    /* This will call dccp_close() in process context (only once per socket) */
+    if (!mpdccp_my_sock(sk)->closing) {
+        mpdccp_my_sock(sk)->closing = destroy;
+        mpdccp_pr_debug("Close socket(%p) %u", sk, destroy);
+        schedule_delayed_work(&mpdccp_my_sock(sk)->close_work, 0);
+    }
+    return 0;
+}
+EXPORT_SYMBOL (mpdccp_close_subflow);
+
+
+/*select sk to announce data*/
+
+struct sock *mpdccp_select_ann_sock(struct mpdccp_cb *mpcb, u8 id)
+{
+
+    struct sock *sk, *avsk = NULL;
+    /*returns the first avilable socket that is not id - can be improved to 
+     *the latest used or lowest rtt as in mptcp mptcp_select_ack_sock */
+
+    mpdccp_for_each_sk(mpcb, sk) {
+        if (!mpdccp_sk_can_send(sk) || mpdccp_my_sock(sk)->local_addr_id == id)
+            continue;
+        avsk = sk;
+        goto avfound;
+    }
+
+avfound:
+    return avsk;
+}
+EXPORT_SYMBOL(mpdccp_select_ann_sock);
+
+/*
+ * the real xmit function
+ */
+
+int
+mpdccp_xmit_to_sk (
+	struct sock	*sk,
+	struct sk_buff	*skb)
+{
+	int			len, ret=0, isbh;
+	long 			timeo;
+	struct mpdccp_cb	*mpcb;
+	struct sock		*meta_sk;
+
+	if (!skb || !sk) return -EINVAL;
+
+	len = skb->len;
+	if (len > dccp_sk(sk)->dccps_mss_cache) {
+		dccp_sk(meta_sk)->dccps_mss_cache = dccp_sk(sk)->dccps_mss_cache;
+		return -EMSGSIZE;
+	}
+	mpcb = get_mpcb (sk);
+	meta_sk = mpcb ? mpcb->meta_sk : NULL;
+
+	if (in_softirq()) {
+		bh_lock_sock(sk);
+		isbh = 1;
+	} else {
+		lock_sock(sk);
+		isbh = 0;
+	}
+
+	if (dccp_qpolicy_full(sk)) {
+		ret = -EAGAIN;
+		goto out_release;
+	}
+
+	timeo = sock_sndtimeo(sk, /* noblock */ 1);
+
+	/*
+	 * We have to use sk_stream_wait_connect here to set sk_write_pending,
+	 * so that the trick in dccp_rcv_request_sent_state_process.
+	 */
+	/* Wait for a connection to finish. */
+	if ((1 << sk->sk_state) & ~(DCCPF_OPEN | DCCPF_PARTOPEN))
+		if ((ret = sk_stream_wait_connect(sk, &timeo)) != 0)
+			goto out_release;
+
+	if (skb->next && meta_sk) dccp_qpolicy_unlink (meta_sk, skb);
+	skb_set_owner_w(skb, sk);
+	dccp_qpolicy_push(sk, skb);
+
+	if (!timer_pending(&dccp_sk(sk)->dccps_xmit_timer))
+		dccp_write_xmit(sk);
+
+	mpdccp_pr_debug("packet with %d bytes sent\n", len);
+
+out_release:
+	if(isbh)
+		bh_unlock_sock(sk);
+	else
+		release_sock(sk);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(mpdccp_xmit_to_sk);
+
+/* Process listen state by calling original backlog_rcv callback
+ * and accept the connection */
+int listen_backlog_rcv (struct sock *sk, struct sk_buff *skb)
+{
+    int ret = 0;
+    struct my_sock *my_sk = mpdccp_my_sock(sk);
+    struct mpdccp_cb *mpcb  = my_sk->mpcb;
+
+    mpdccp_pr_debug("Executing backlog_rcv callback. sk %p my_sk %p bklog %p \n", sk, my_sk, my_sk->sk_backlog_rcv);
+
+    if (my_sk->sk_backlog_rcv) {
+	mpdccp_pr_debug("There is sk_backlog_rcv");
+        ret = my_sk->sk_backlog_rcv (sk, skb);
+    }
+
+    if(mpcb && mpcb->reorder_ops->update_pseq)
+        mpcb->reorder_ops->update_pseq(my_sk, skb);
+#if 0 
+    /* If the queue was previously stopped because of a full cwnd,
+    * a returning ACK will open the window again, so we should
+    * re-enable the queue. */
+    if (netif_queue_stopped(mpcb->mpdev->ndev) &&
+        mpdccp_cwnd_available(mpcb)) {
+        netif_wake_queue(mpcb->mpdev->ndev);
+        MPSTATINC (mpcb->mpdev,tx_rearm);
+    }
+#endif
+    
+    /* We can safely ignore ret value of my_sk->sk_backlog_rcv, as it can only return 0 anyways
+     * (and causes a LOT of pain if it was otherwise). */
+    return ret;
+}
+
+void listen_data_ready (struct sock *sk)
+{
+    int ret;
+
+    // Client side setup is not handled by this callback, use original workflow
+    if (sk->sk_state == DCCP_REQUESTING) {
+        struct my_sock *my_sk = mpdccp_my_sock(sk);
+        if (my_sk->sk_data_ready)
+            my_sk->sk_data_ready (sk);
+
+        return;
+    }
+
+    /* If the socket is not listening, it does not belong to a server. */
+    if (sk->sk_state == DCCP_LISTEN) {
+        mpdccp_pr_debug("sk %p is in LISTEN state, not handled\n", sk);
+         return;
+
+    } 
+
+    if ((sk->sk_state == DCCP_OPEN)
+        || (sk->sk_state == DCCP_PARTOPEN)) {
+        //mpdccp_pr_debug("sk %p is in OPEN state. Reading...\n", sk);
+
+        ret = mpdccp_read_from_subflow (sk);
+        if(ret < 0) {
+            mpdccp_pr_debug("Failed to read message from sk %p (%d).\n", sk, ret);
+        }
+    }
+
+    /* TODO: Work queues temporarily disabled. They led to a lot of 
+     * packet loss. */
+    // ret = queue_bnd_work(sk);
+    // if(ret < 0)
+    //     mpdccp_pr_debug("Failed to queue work (%d).\n", ret);
+
+    return;
+}
+
+void mp_state_change(struct sock *sk)
+{
+    struct mpdccp_cb *mpcb;
+    struct sock *subsk;
+    mpdccp_pr_debug("enter sk %p role %s is_kex %d\n", sk, dccp_role(sk), dccp_sk(sk)->is_kex_sk);
+
+    mpcb = MPDCCP_CB(sk);
+
+    if (sk->sk_state == DCCP_OPEN || (sk->sk_state == DCCP_PARTOPEN && mpcb && mpcb->fallback_sp)) {
+        /* Check if the subflow was already added */
+        spin_lock(&mpcb->psubflow_list_lock);
+        mpdccp_for_each_sk(mpcb, subsk) {
+            if (sk == subsk) {
+                mpdccp_pr_debug("sk %p already in subflow_list, skipping\n", sk);
+                spin_unlock(&mpcb->psubflow_list_lock);
+                /* nevertheless report it */
+                mpdccp_report_new_subflow(sk);
+                goto out;
+            }
+        }
+
+        /* Move socket from the request to the subflow list */
+        list_move_tail(&mpdccp_my_sock(sk)->sk_list, &mpcb->psubflow_list);
+        mpdccp_pr_debug("Added new entry sk %p to psubflow_list @ %p\n", sk, mpdccp_my_sock(sk));
+        mpcb->cnt_subflows = (mpcb->cnt_subflows) + 1;
+        spin_unlock(&mpcb->psubflow_list_lock);
+
+        if (mpcb->sched_ops->init_subflow) {
+            rcu_read_lock ();
+            mpcb->sched_ops->init_subflow(sk);
+            rcu_read_unlock ();
+        }
+
+        mpdccp_report_new_subflow(sk);
+        mpdccp_pr_debug("client connection established successfully. There are %d subflows now.\n", mpcb->cnt_subflows);
+    }
+
+out:
+    return;
+}
+
+int mpdccp_link_cpy_set_prio(struct sock *sk, int prio)
+{
+   struct mpdccp_link_info      *link;
+
+   /* copy and get link */
+   link = mpdccp_ctrl_getcpylink (sk);
+   if (!link) return -EINVAL;
+   /* change prio */
+   mpdccp_link_change_mpdccp_prio (link, prio);
+   mpdccp_link_change_name(link, sk);
+   /* release link */
+   mpdccp_link_put (link);
+   return 0;
+}
+
+int mpdccp_get_prio(struct sock *sk)
+{
+	struct mpdccp_link_info *link;
+	int prio;
+
+	link = mpdccp_ctrl_getlink (sk);
+	if (!link)
+		return -EINVAL;
+	prio = link->mpdccp_prio;
+	mpdccp_link_put (link);
+	return prio;
+}
+
+void mpdccp_init_announce_prio(struct sock *sk)
+{
+    mpdccp_my_sock(sk)->announce_prio = mpdccp_get_prio(sk) + 1;        //adding +1 so we can check also for sending zero
+    dccp_send_keepalive(sk);
+}
+
+int mpdccp_generate_key(struct mpdccp_key *key, int key_type)
+{
+	switch (key_type) {
+		case DCCPK_PLAIN:
+			key->type = key_type;
+			key->size = MPDCCP_PLAIN_KEY_SIZE;
+			get_random_bytes(&key->value, MPDCCP_PLAIN_KEY_SIZE);
+			break;
+		case DCCPK_C25519_SHA256:
+		case DCCPK_C25519_SHA512:
+			/* TODO: add support */
+		default:
+			key->size = 0;
+			key->type = DCCPK_INVALID;
+			mpdccp_pr_debug("cannot generate key of type %d", key_type);
+			return -1;
+	}
+	mpdccp_pr_debug("generated key %llx type %d", be64_to_cpu(*((__be64 *)key->value)), key->type);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_generate_key);
+
+/* General initialization of MPDCCP */
+int mpdccp_ctrl_init(void)
+{
+    INIT_LIST_HEAD(&pconnection_list);
+    spin_lock_init(&pconnection_list_lock);
+
+    mpdccp_mysock_cache = kmem_cache_create("mpdccp_mysock", sizeof(struct my_sock),
+                       0, SLAB_TYPESAFE_BY_RCU|SLAB_HWCACHE_ALIGN,
+                       NULL);
+    if (!mpdccp_mysock_cache) {
+        mpdccp_pr_debug("Failed to create mysock slab cache.\n");
+        return -ENOMEM;
+    }
+
+    mpdccp_cb_cache = kmem_cache_create("mpdccp_cb", sizeof(struct mpdccp_cb),
+                       0, SLAB_TYPESAFE_BY_RCU|SLAB_HWCACHE_ALIGN,
+                       NULL);
+    if (!mpdccp_cb_cache) {
+        mpdccp_pr_debug("Failed to create mpcb slab cache.\n");
+    	kmem_cache_destroy(mpdccp_mysock_cache);
+        return -ENOMEM;
+    }
+
+    /*
+     * The number of active work items is limited by the number of
+     * connections, so leave @max_active at default.
+     */
+    mpdccp_wq = alloc_workqueue("mpdccp_wq", WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
+    if (!mpdccp_wq) {
+        mpdccp_pr_debug("Failed to register sysctl.\n");
+    	kmem_cache_destroy(mpdccp_mysock_cache);
+    	kmem_cache_destroy(mpdccp_cb_cache);
+	return -1;
+    }
+
+    /* Allocate crypto contexts */
+    tfm_hash = crypto_alloc_shash("sha256", 0, 0);
+    if (IS_ERR(tfm_hash)) {
+        mpdccp_pr_debug("Failed to alloc tfm_hash, err %ld\n", PTR_ERR(tfm_hash));
+        return -1;
+    }
+
+    tfm_hmac = crypto_alloc_shash("hmac(sha256)", 0, 0);
+    if (IS_ERR(tfm_hmac)) {
+        mpdccp_pr_debug("Failed to alloc tfm_hmac, err %ld\n", PTR_ERR(tfm_hmac));
+        crypto_free_shash(tfm_hash);
+        return -1;
+    }
+
+    return 0;
+}
+
+void mpdccp_ctrl_finish(void)
+{
+    if (mpdccp_wq) {
+        mpdccp_wq_flush();
+        destroy_workqueue(mpdccp_wq);
+        mpdccp_wq = NULL;
+    }
+
+    if (tfm_hash)
+        crypto_free_shash(tfm_hash);
+
+    if (tfm_hmac)
+        crypto_free_shash(tfm_hmac);
+
+    kmem_cache_destroy(mpdccp_mysock_cache);
+    kmem_cache_destroy(mpdccp_cb_cache);
+
+#if 0
+    /* sk_free (and __sk_free) requires wmem_alloc to be 1.
+     * All the rest is set to 0 thanks to __GFP_ZERO above.
+     */
+    atomic_set(&master_sk->sk_wmem_alloc, 1);
+    sk_free(master_sk);
+#endif
+}
+
+int mpdccp_hmac_sha256(const u8 *key, u8 keylen, const u8 *msg, u8 msglen, u8 *output)
+{
+        SHASH_DESC_ON_STACK(desc, tfm_hmac);
+        u8 buf[32];
+        int ret;
+
+        ret = crypto_shash_setkey(tfm_hmac, key, keylen);
+        if (ret) {
+            mpdccp_pr_debug("Failed crypto_shash_setkey, err %d", ret);
+            return -1;
+        }
+
+        desc->tfm = tfm_hmac;
+        //desc->flags = 0;
+
+        ret = crypto_shash_digest(desc, msg, msglen, buf);
+        if (ret) {
+            mpdccp_pr_debug("Failed crypto_shash_digest, err %d", ret);
+            return -1;
+        }
+        memcpy(output, buf, MPDCCP_HMAC_SIZE);
+
+        return 0;
+}
diff --git a/net/dccp/mpdccp_link.c b/net/dccp/mpdccp_link.c
new file mode 100644
index 0000000000000..5959edd9b58c2
--- /dev/null
+++ b/net/dccp/mpdccp_link.c
@@ -0,0 +1,1168 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/bitops.h>
+#include <linux/capability.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/netdevice.h>
+#include <linux/notifier.h>
+#include <linux/skbuff.h>
+#include <net/net_namespace.h>
+#include <linux/rtnetlink.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/rcupdate.h>
+#include <linux/inetdevice.h>
+#include <net/if_inet6.h>
+#include <generated/autoconf.h>
+#include <linux/list.h>
+#include <net/netns/generic.h>
+#include <linux/interrupt.h>
+#include <asm/processor.h>
+#include <asm/uaccess.h>
+
+#include <net/mpdccp_link.h>
+#include "mpdccp_link_sysfs.h"
+
+static void mpdccp_link_free (struct mpdccp_link_info*);
+static void mlk_free (struct mpdccp_link_info*);
+static struct mpdccp_link_info *mlk_alloc (void);
+static void mpdccp_link_release_nolock (struct mpdccp_link_info*);
+
+#define mpdccp_pr_debug(format, a...) do { printk (KERN_DEBUG format, ##a); } while (0)
+#define mpdccp_pr_info(format, a...) do { printk (KERN_INFO format, ##a); } while (0)
+#define mpdccp_pr_error(format, a...) do { printk (KERN_ERR format, ##a); } while (0)
+
+/* ***********************
+ * find / get functions
+ * ***********************/
+
+static LIST_HEAD(link_list);
+int mpdccp_link_net_id = -1;
+
+#if 0
+
+static DEFINE_MUTEX(mpdccp_link_mutex);
+#define mlk_lock		mutex_lock (&mpdccp_link_mutex)
+#define mlk_unlock	mutex_unlock (&mpdccp_link_mutex)
+#define mlk_init
+
+#else
+
+static spinlock_t		mlk_mutex;
+static unsigned long	mlk_lf = 0;
+
+static
+inline
+void
+mlk_doinit(void)
+{
+	spin_lock_init (&mlk_mutex);
+}
+
+static
+inline
+void
+mlk_dolock(void)
+{
+	unsigned long	lf=0;
+
+	if (in_interrupt()) {
+		spin_lock_irqsave (&mlk_mutex, lf);
+		mlk_lf = lf;
+	} else {
+		spin_lock (&mlk_mutex);
+	}
+}
+
+static
+inline
+void
+mlk_dounlock(void)
+{
+	unsigned long	lf;
+
+	if (in_interrupt()) {
+		lf = mlk_lf;
+		spin_unlock_irqrestore (&mlk_mutex, lf);
+	} else {
+		spin_unlock (&mlk_mutex);
+	}
+}
+
+#define mlk_init mlk_doinit()
+#define mlk_lock mlk_dolock()
+#define mlk_unlock mlk_dounlock()
+
+#endif
+
+
+
+struct mpdccp_link_info*
+mpdccp_link_find_by_name (
+	struct net	*net,
+	const char	*name)
+{
+	struct mpdccp_link_info	*link;
+
+	mlk_lock;
+	list_for_each_entry (link, &link_list, link_list) {
+		if (MPDCCP_LINK_TO_NET(link) != net) continue;
+		if (!strcmp (link->name, name)) goto found;
+	}
+	link=NULL;
+found:
+	//mpdccp_link_get (link);
+	mlk_unlock;
+	return link;
+}
+EXPORT_SYMBOL(mpdccp_link_find_by_name);
+
+int mpdccp_link_change_name (struct mpdccp_link_info *link, struct sock *sk) {
+	char newname[64];
+
+	snprintf(newname, 64, "%s_%pI4:%u", link->ndev_name,
+				&sk->__sk_common.skc_daddr, sk->__sk_common.skc_dport);
+	strcpy(link->ndev_name, "mp_prio");
+	return mpdccp_link_sysfs_changename(link, newname);
+}
+EXPORT_SYMBOL(mpdccp_link_change_name);
+
+struct mpdccp_link_info*
+mpdccp_link_find_by_dev (
+	struct net_device	*dev)
+{
+	struct mpdccp_link_info	*link;
+
+	if (!dev) return NULL;
+	mlk_lock;
+	list_for_each_entry (link, &link_list, link_list) {
+		if (MPDCCP_LINK_TO_DEV(link) == dev) goto found;
+	}
+	link = NULL;
+found:
+	//mpdccp_link_get (link);
+	mlk_unlock;
+	return link;
+}
+EXPORT_SYMBOL(mpdccp_link_find_by_dev);
+
+struct mpdccp_link_info*
+mpdccp_link_find_by_skb (
+	struct net				*net,
+	const struct sk_buff	*skb)
+{
+	struct mpdccp_link_info	*link;
+	struct net_device			*ndev;
+	
+	if (!skb) return NULL;
+	if (skb->mark) {
+		link = mpdccp_link_find_mark (net, skb->mark);
+		if (link) {
+			ndev = dev_get_by_index (net, skb->skb_iif);
+			mpdccp_pr_debug ("mpdccp_link_find_by_skb(): has found link (%s) by mark "
+							"(%x/%x) - skb->iif = %s\n", MPDCCP_LINK_NAME(link),
+							link->mpdccp_match_mark, link->mpdccp_match_mask,
+							(ndev?ndev->name:"<none>"));
+			
+			return link;
+		}
+	}
+#if 0		/* this is unsafe - do always use skb_iif instead */
+	if (skb->dev) {
+		rcu_read_lock();
+		ndev = skb->dev;
+		if (ndev) dev_hold (ndev);
+		rcu_read_unlock();
+	} else {
+#endif
+		ndev = dev_get_by_index (net, skb->skb_iif);
+#if 0
+	}
+#endif
+	if (ndev) {
+		link = mpdccp_link_find_by_dev (ndev);
+		if (!link) {
+			mpdccp_pr_debug ("mpdccp_link_find_by_skb(): cannot find by device (%s)\n",
+							ndev->name);
+		} else {
+			mpdccp_pr_debug ("%s(): found link (%s) by device (%s)\n", __func__,
+							MPDCCP_LINK_NAME(link), ndev->name);
+		}
+		mpdccp_link_get (link);
+		dev_put (ndev);
+		if (link) return link;
+	} else {
+		mpdccp_pr_debug ("mpdccp_link_find_by_skb(): skb has no input device\n");
+	}
+	/* yet, we don't match for ip's in skb, use the sk in 
+		calling function to get ip's
+	 */
+	return NULL;
+}
+EXPORT_SYMBOL(mpdccp_link_find_by_skb);
+
+struct mpdccp_link_info*
+mpdccp_link_find_mark (
+	struct net	*net,
+	u32			mark)
+{
+	struct mpdccp_link_info	*link;
+
+	if (!mark) return NULL;
+	mlk_lock;
+	list_for_each_entry (link, &link_list, link_list) {
+		if (MPDCCP_LINK_TO_NET(link) != net) continue;
+		if (!link->mpdccp_match_mark) continue;
+		if ((link->mpdccp_match_mark & link->mpdccp_match_mask) == 
+				(mark & link->mpdccp_match_mask)) goto found;
+	}
+	link=NULL;
+found:
+	mpdccp_link_get (link);
+	mlk_unlock;
+	return link;
+}
+EXPORT_SYMBOL(mpdccp_link_find_mark);
+
+
+struct mpdccp_link_info*
+mpdccp_link_find_ip4 (
+	struct net		*net,
+	struct in_addr	*saddr,
+	struct in_addr	*daddr)
+{	
+	struct in_ifaddr			*ia;
+	struct mpdccp_link_info	*link;
+
+	mlk_lock;
+	list_for_each_entry (link, &link_list, link_list) {
+		if (!MPDCCP_LINK_ISDEV_VALID(link)) continue;
+		if (MPDCCP_LINK_TO_NET(link) != net) continue;
+		for (ia = link->ndev->ip_ptr->ifa_list; ia; ia = ia->ifa_next) {
+			if (saddr && ia->ifa_local == saddr->s_addr) goto found;
+			/* does this make sense??? */
+			if (saddr && ia->ifa_address == saddr->s_addr) goto found;
+			/* works for PtP-devices only */
+			if (daddr && ia->ifa_address == daddr->s_addr) goto found;
+		}
+	}
+	link=NULL;
+found:
+	mpdccp_link_get (link);
+	mlk_unlock;
+	return link;
+}
+EXPORT_SYMBOL(mpdccp_link_find_ip4);
+
+
+#if IS_ENABLED(CONFIG_IPV6)
+#define ipv6cmp(a,b) (memcmp ((a).s6_addr, (b).s6_addr, 16))
+#define ipv6eq(a,b) (!ipv6cmp(a,b))
+const static u8	v4pref[] = {0,0,0,0,0,0,0,0,0,0,0xff,0xff};
+#define ipv6isv4(a) (!memcmp ((a),v4pref,12))
+struct mpdccp_link_info*
+mpdccp_link_find_ip6 (
+	struct net			*net,
+	struct in6_addr	*saddr,
+	struct in6_addr	*daddr)
+{
+	struct inet6_ifaddr		*ifa;
+	struct mpdccp_link_info	*link;
+
+	if (saddr && daddr && ipv6isv4(saddr->s6_addr) && ipv6isv4(daddr->s6_addr)) {
+		return mpdccp_link_find_ip4 (net, (struct in_addr*)&(saddr->s6_addr[12]), 
+													(struct in_addr*)&(daddr->s6_addr[12]));
+	} else if (saddr && !daddr && ipv6isv4(saddr->s6_addr)) {
+		return mpdccp_link_find_ip4 (net, (struct in_addr*)&(saddr->s6_addr[12]),
+													NULL);
+	}
+	mlk_lock;
+	list_for_each_entry (link, &link_list, link_list) {
+		if (!MPDCCP_LINK_ISDEV_VALID(link)) continue;
+		if (MPDCCP_LINK_TO_NET(link) != net) continue;
+		list_for_each_entry(ifa, &(link->ndev->ip6_ptr->addr_list), if_list) {
+			if (saddr && ipv6eq (*saddr, ifa->addr)) goto found;
+			if (daddr && ipv6eq (*daddr, ifa->peer_addr)) goto found;
+		}
+	}
+	link=NULL;
+found:
+	mpdccp_link_get (link);
+	mlk_unlock;
+	return link;
+}
+EXPORT_SYMBOL(mpdccp_link_find_ip6);
+#endif /* IS_ENABLED(CONFIG_IPV6) */
+
+
+
+
+struct mpdccp_link_info*
+mpdccp_getfallbacklink (struct net	*net)
+{
+	struct mpdccp_link_net_data	*linkdata;
+
+	if (!net) net = &init_net;
+	if (mpdccp_link_net_id < 0) return NULL;
+	linkdata = net_generic (net, mpdccp_link_net_id);
+	mpdccp_link_get (linkdata->fallback);
+	return linkdata->fallback;
+}
+EXPORT_SYMBOL(mpdccp_getfallbacklink);
+
+static
+int
+link_get_next_counter (
+	struct net	*net)
+{
+	struct mpdccp_link_net_data	*linkdata;
+
+	if (!net || mpdccp_link_net_id < 0) return 0;
+	linkdata = net_generic (net, mpdccp_link_net_id);
+	return atomic_fetch_add (1, &linkdata->counter);
+}
+
+
+
+
+
+
+/* ***************************
+ * link change functions
+ * ***************************/
+
+/**
+ *	mpdccp_link_change_mpdccp_prio - change prio for mpdccp connections
+ *	@dev: device
+ *	@mpdccp_prio: prio to set
+ *
+ *	Change settings of mpdccp priority.
+ */
+int mpdccp_link_change_mpdccp_prio(struct mpdccp_link_info *link, u32 mpdccp_prio)
+{
+	if (!link) return 0;
+	if (link->mpdccp_prio == mpdccp_prio) return 0;
+	link->mpdccp_prio = mpdccp_prio;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_PRIO, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_prio);
+
+/**
+ *	mpdccp_link_change_mpdccp_maxbuf - change maxbuf for mpdccp subflows
+ *	@dev: device
+ *	@mpdccp_maxbuf: maxbuf to set
+ *
+ *	Change settings of the max. buffer size of mpdccp subflows.
+ */
+int mpdccp_link_change_mpdccp_maxbuf(struct mpdccp_link_info *link, u64 mpdccp_maxbuf)
+{
+	if (link->mpdccp_maxbuf == mpdccp_maxbuf) return 0;
+	link->mpdccp_maxbuf = mpdccp_maxbuf;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_MAXBUF, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_maxbuf);
+
+/**
+ *	mpdccp_link_change_mpdccp_T_delay - change T_delay for mpdccp subflows
+ *	@dev: device
+ *	@mpdccp_T_delay: T_delay to set
+ *
+ *	Change settings of the delay on peek start.
+ */
+int mpdccp_link_change_mpdccp_T_delay(struct mpdccp_link_info *link, u32 mpdccp_T_delay)
+{
+	if (link->mpdccp_T_delay == mpdccp_T_delay) return 0;
+	link->mpdccp_T_delay = mpdccp_T_delay;
+	link->mpdccp_T_delay_j = mpdccp_T_delay * HZ / 1000;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_DELAY, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_T_delay);
+
+/**
+ *	mpdccp_link_change_mpdccp_T_start_delay - change T_start_delay for mpdccp subflows
+ *	@dev: device
+ *	@mpdccp_T_start_delay: T_delay to set
+ *
+ *	Change settings of the delay on stream start.
+ */
+int mpdccp_link_change_mpdccp_T_start_delay(struct mpdccp_link_info *link, u32 mpdccp_T_delay)
+{
+	if (link->mpdccp_T_start_delay == mpdccp_T_delay) return 0;
+	link->mpdccp_T_start_delay = mpdccp_T_delay;
+	link->mpdccp_T_start_delay_j = mpdccp_T_delay * HZ / 1000;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_DELAY, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_T_start_delay);
+
+/**
+ *	mpdccp_link_change_mpdccp_T_lpu - change T_lpu for mpdccp subflows
+ *	@dev: device
+ *	@mpdccp_T_lpu: T_lpu to set
+ *
+ *	Change settings of the last path usage (interval between two peeks)
+ */
+int mpdccp_link_change_mpdccp_T_lpu(struct mpdccp_link_info *link, u32 mpdccp_T_lpu)
+{
+	if (link->mpdccp_T_lpu == mpdccp_T_lpu) return 0;
+	link->mpdccp_T_lpu = mpdccp_T_lpu;
+	link->mpdccp_T_lpu_j = mpdccp_T_lpu * HZ / 1000;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_LPU, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_T_lpu);
+
+/**
+ *	mpdccp_link_change_mpdccp_T_lpu_min - change T_lpu_min for mpdccp subflows
+ *	@dev: device
+ *	@mpdccp_T_lpu: T_lpu to set
+ *
+ *	Change settings of the last path usage (interval between two peeks)
+ */
+int mpdccp_link_change_mpdccp_T_lpu_min(struct mpdccp_link_info *link, u32 mpdccp_T_lpu)
+{
+	if (link->mpdccp_T_lpu_min == mpdccp_T_lpu) return 0;
+	link->mpdccp_T_lpu_min = mpdccp_T_lpu;
+	link->mpdccp_T_lpu_min_j = mpdccp_T_lpu * HZ / 1000;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_LPU, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_T_lpu_min);
+
+/**
+ *	mpdccp_link_change_mpdccp_lpu_cnt - change lpu_cnt for mpdccp subflows
+ *	@dev: device
+ *	@mpdccp_lpu_cnt: lpu_cnt to set
+ *
+ *	Change settings of the last path usage (interval between two peeks)
+ */
+int mpdccp_link_change_mpdccp_lpu_cnt(struct mpdccp_link_info *link, u32 mpdccp_lpu_cnt)
+{
+	if (link->mpdccp_lpu_cnt == mpdccp_lpu_cnt) return 0;
+	link->mpdccp_lpu_cnt = mpdccp_lpu_cnt;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_LPU, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_lpu_cnt);
+
+/**
+ *	mpdccp_link_change_mpdccp_ignthrottle - ignores throttling if set
+ *	@dev: device
+ *	@mpdccp_ignthrottle: bool to set
+ *
+ *	Change settings of mpdccp ignthrottle
+ */
+int mpdccp_link_change_mpdccp_ignthrottle(struct mpdccp_link_info *link, unsigned int mpdccp_ignthrottle)
+{
+	if (link->mpdccp_ignthrottle == mpdccp_ignthrottle) return 0;
+	link->mpdccp_ignthrottle = mpdccp_ignthrottle;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_THROTTLE, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_ignthrottle);
+
+/**
+ *	mpdccp_link_change_mpdccp_match_mark - change fw mark against to which match incomming connection
+ *	@dev: device
+ *	@mpdccp_match_mark: fw mark to match
+ *
+ *	Change fw mark against to which match incomming connection
+ */
+int mpdccp_link_change_mpdccp_match_mark(struct mpdccp_link_info *link, u32 mpdccp_match_mark)
+{
+	if (link->mpdccp_match_mark == mpdccp_match_mark) return 0;
+	link->mpdccp_match_mark = mpdccp_match_mark;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_MARK, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_match_mark);
+
+/**
+ *	mpdccp_link_change_mpdccp_match_mask - change fw mask against to which match incomming connection
+ *	@dev: device
+ *	@mpdccp_match_mask: fw mask used for matching
+ *
+ *	Change fw mask against to which match incomming connection
+ */
+int mpdccp_link_change_mpdccp_match_mask(struct mpdccp_link_info *link, u32 mpdccp_match_mask)
+{
+	if (link->mpdccp_match_mask == mpdccp_match_mask) return 0;
+	link->mpdccp_match_mask = mpdccp_match_mask;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_MARK, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_match_mask);
+
+/**
+ *	mpdccp_link_change_mpdccp_send_mark - fw mark set in outgoing traffic
+ *	@dev: device
+ *	@mpdccp_send_mark: fw mark set in outgoing traffic
+ *
+ *	fw mark set in outgoing traffic
+ */
+int mpdccp_link_change_mpdccp_send_mark(struct mpdccp_link_info *link, u32 mpdccp_send_mark)
+{
+	if (link->mpdccp_send_mark == mpdccp_send_mark) return 0;
+	link->mpdccp_send_mark = mpdccp_send_mark;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_MARK, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_send_mark);
+
+/**
+ *	mpdccp_link_change_mpdccp_cgstalg - changes the congestion algorithmn for subflow
+ *	@dev: device
+ *	@buf: the new value
+ *	@len: strlen of buf
+ *
+ *	Change settings of the congestion algorithmn used for mpdccp subflows over
+ *	this device.
+ */
+int mpdccp_link_change_mpdccp_cgstalg(struct mpdccp_link_info *link, const char *buf, size_t len)
+{
+	if (buf && len == 0) len = strlen (buf);
+	if (!buf || (len == 0) || (len == 7 && !strncasecmp (buf, "default", 7))) {
+		link->mpdccp_cgstalg[0] = 0;
+	} else if (len >= sizeof (link->mpdccp_cgstalg)) {
+		mpdccp_pr_debug ("mpdccp_link_change_mpdccp_cgstalg(): cgstalg name >>%.*s<< too long (>%d)\n",
+					(int)len, buf, (int)sizeof (link->mpdccp_cgstalg)-1);
+		return -E2BIG;
+	} else {
+		if (!strncasecmp (link->mpdccp_cgstalg, buf, len)) {
+			return 0;
+		}
+		strncpy (link->mpdccp_cgstalg, buf, len);
+		link->mpdccp_cgstalg[len]=0;
+	}
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_CGSTCTRL, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_cgstalg);
+
+/**
+ *	mpdccp_link_change_mpdccp_path_type - path type (lte, wifi, ...)
+ *	@dev: device
+ *	@mpdccp_path_type: path type (lte, wifi, ...)
+ *
+ *	fw mark set in outgoing traffic
+ */
+int mpdccp_link_change_mpdccp_path_type(struct mpdccp_link_info *link, u32 mpdccp_path_type)
+{
+	if (link->mpdccp_path_type == mpdccp_path_type) return 0;
+	link->mpdccp_path_type = mpdccp_path_type;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_PATHTYPE, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_path_type);
+
+/**
+ *	mpdccp_link_change_mpdccp_match_pathtype - path type (lte, wifi, ...)
+ *	@dev: device
+ *	@mpdccp_match_pathtype: path type (lte, wifi, ...)
+ *
+ *	fw mark set in outgoing traffic
+ */
+int mpdccp_link_change_mpdccp_match_pathtype(struct mpdccp_link_info *link, u32 mpdccp_match_pathtype)
+{
+	if (link->mpdccp_match_pathtype == mpdccp_match_pathtype) return 0;
+	link->mpdccp_match_pathtype = mpdccp_match_pathtype;
+	link->config_cnt ++;
+	call_mpdccp_link_notifiers(MPDCCP_LINK_CHANGE_MATCH_PATHTYPE, link);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_match_pathtype);
+
+/**
+ *	mpdccp_link_change_mpdccp_resetstat 
+ *	@dev: device
+ *	@mpdccp_newbuf: bool to set
+ *
+ *	reset mpdccp statistics for device
+ */
+int mpdccp_link_change_mpdccp_resetstat(struct mpdccp_link_info *link)
+{
+#ifdef CONFIG_MPDCCP_STATS
+	link->mpdccp_noavail_hard = 0;
+	link->mpdccp_noavail_hard_state = 0;
+	link->mpdccp_noavail_hard_pre = 0;
+	link->mpdccp_noavail_hard_pf = 0;
+	link->mpdccp_noavail_hard_loss = 0;
+	link->mpdccp_noavail_nocwnd = 0;
+	link->mpdccp_noavail_nospace_maxbuf = 0;
+	link->mpdccp_noavail_nospace = 0;
+	link->mpdccp_noavail_zerownd = 0;
+	link->mpdccp_noavail_nobuf = 0;
+	link->mpdccp_noavail_delay = 0;
+	link->mpdccp_noavail_start_delay = 0;
+	link->mpdccp_noavail_dontreinject = 0;
+	link->mpdccp_selected_delayed = 0;
+	link->mpdccp_selected_onlypath = 0;
+	link->mpdccp_selected_shutdown = 0;
+	link->mpdccp_selected_backup = 0;
+	link->mpdccp_selected_good = 0;
+	link->mpdccp_selected_best = 0;
+	link->mpdccp_selected_fallback = 0;
+	link->mpdccp_selected = 0;
+#endif
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_change_mpdccp_resetstat);
+
+
+
+
+/* ***********************
+ * add / del functions
+ * ***********************/
+
+static void link_ref_release (struct kref *ref)
+{	
+	struct mpdccp_link_info	*link = container_of (ref, struct mpdccp_link_info, kref);
+	mpdccp_link_free (link);
+}
+
+int
+mpdccp_link_add (
+	struct mpdccp_link_info	**link_info,
+	struct net					*net,
+	struct net_device			*ndev,
+	const char					*name)
+{
+	struct mpdccp_link_info	*link = NULL;
+	int							ret;
+
+	if (name && !*name) {
+		mpdccp_pr_error ("mpdccp_link_add(): invalid empty link name\n");
+		return -EINVAL;
+	}
+	if (name && strlen (name) >= sizeof (link->name)) {
+		mpdccp_pr_error ("mpdccp_link_add(): name >>%s<< too long (max %d)\n", name, 
+					(int)sizeof (link->name)-1);
+		return -ERANGE;
+	}
+	if (!net && !ndev) return -EINVAL;
+	if (!net) net = read_pnet(&(ndev->nd_net));
+	if (!net) net = &init_net;
+	if (name && mpdccp_link_find_by_name (net, name)) {
+		mpdccp_pr_error ("mpdccp_link_add(): link with name >>%s<< already exists\n", name);
+		return -EEXIST;
+	}
+	if (ndev) {
+		mpdccp_pr_info ("mpdccp_link:: create new device link (dev=%s)\n", ndev->name);
+	} else if (name) {
+		mpdccp_pr_info ("mpdccp_link:: create new named link (%s)\n", name);
+	} else {
+		mpdccp_pr_info ("mpdccp_link:: create new unnamed link\n");
+	}
+	link = mlk_alloc ();
+	if (!link) return -ENOMEM;
+	*link = (struct mpdccp_link_info) { .ndev = ndev, .net = net };
+	if (name) strcpy (link->name, name);
+	if (ndev) {
+		link->is_devlink = 1;
+		strcpy (link->ndev_name, MPDCCP_LINK_TO_DEV(link)->name);
+	}
+	if (!ndev && !name) {
+		link->is_released = 1;
+	}
+	link->id = link_get_next_counter (net);
+	link->mpdccp_prio = 3;
+	ret = mpdccp_link_sysfs_add (link);
+	if (ret < 0) {
+		mpdccp_pr_error ("mpdccp_link_add(): error adding sysfs entry\n");
+		mlk_free (link);
+		return ret;
+	}
+	kref_init (&link->kref);
+	mlk_lock;
+	list_add (&link->link_list, &link_list);
+	link->is_linked = 1;
+	mlk_unlock;
+	if (link_info) *link_info = link;
+	mpdccp_pr_info ("mpdccp_link_add: link %d successfully created", link->id);
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_add);
+
+int
+mpdccp_link_copy (
+	struct mpdccp_link_info	**new_link,
+	struct mpdccp_link_info	*old_link)
+{
+	int	ret;
+
+	if (!new_link || !old_link) return -EINVAL;
+	ret = mpdccp_link_add (new_link, old_link->net, NULL, NULL);
+	if (ret < 0) {
+		mpdccp_pr_error ("mpdccp_link_copy(): error creating new link: %d", ret);
+		return ret;
+	}
+	/* copy old configuration */
+	memcpy (&(*new_link)->start_config, &old_link->start_config,
+				&old_link->end_config - &old_link->start_config);
+	(*new_link)->config_cnt = old_link->config_cnt+1;
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_link_copy);
+
+void
+mpdccp_link_get (
+	struct mpdccp_link_info	*link)
+{
+	if (!link) return;
+	kref_get (&link->kref);
+#if 0
+	//mpdccp_pr_debug ("mpdccp_link:: ref counter (%s) incremented to %d\n",
+	//	MPDCCP_LINK_NAME(link), MPDCCP_LINK_REFCOUNT(link));
+	printk ("mpdccp_link:: ref counter (%s) incremented to %d\n",
+		MPDCCP_LINK_NAME(link), MPDCCP_LINK_REFCOUNT(link));
+#endif
+#ifdef CONFIG_MPDCCP_STATS
+	link->allref++;
+#endif
+}
+EXPORT_SYMBOL(mpdccp_link_get);
+
+void
+mpdccp_link_put (
+	struct mpdccp_link_info	*link)
+{
+	if (!link) return;
+#if 0
+	//mpdccp_pr_debug ("mpdccp_link:: ref counter (%s) decremented from %d\n",
+	//	MPDCCP_LINK_NAME(link), MPDCCP_LINK_REFCOUNT(link));
+	printk ("mpdccp_link:: ref counter (%s) decremented from %d\n",
+		MPDCCP_LINK_NAME(link), MPDCCP_LINK_REFCOUNT(link));
+#endif
+	kref_put (&link->kref, link_ref_release);
+}
+EXPORT_SYMBOL(mpdccp_link_put);
+
+static
+struct mpdccp_link_info *
+mlk_alloc ()
+{
+	/* we should use a kind of caching */
+	return (struct mpdccp_link_info*) kmalloc (sizeof (struct mpdccp_link_info), GFP_KERNEL);
+}
+	
+static
+void
+mlk_free (
+	struct mpdccp_link_info	*link)
+{
+	if (!link) return;
+	kfree (link);
+}
+
+static
+void
+mpdccp_link_free (
+	struct mpdccp_link_info	*link)
+{
+	if (!link) return;
+	//mlk_lock;
+	if (!link->is_released) {
+		mpdccp_pr_info ("mpdccp_link:: link not released yet (%s) ref cnt: %d",
+				MPDCCP_LINK_NAME(link), MPDCCP_LINK_REFCOUNT(link));
+		
+		if(strcmp(link->ndev_name, "mp_prio")) {
+			mpdccp_link_get (link);
+			return;
+		}
+		mpdccp_link_release_nolock(link);
+	}
+	if (MPDCCP_LINK_ISDEV(link)) {
+		mpdccp_pr_info ("mpdccp_link:: device link freed (dev=%s)\n", link->ndev_name);
+	} else {
+		mpdccp_pr_info ("mpdccp_link:: named link %s freed\n", link->name);
+	}
+	mlk_free (link);
+	//mlk_unlock;
+}	
+
+
+void
+mpdccp_link_release (
+	struct mpdccp_link_info	*link)
+{
+	mpdccp_pr_info ("mpdccp_link:: named link %s released\n", link->name);
+	if (!link) return;
+	mlk_lock;
+	mpdccp_link_release_nolock (link);
+	mlk_unlock;
+}
+
+static
+void
+mpdccp_link_release_nolock (
+	struct mpdccp_link_info	*link)
+{
+	if (!link) return;
+	if (link->is_linked) {
+		list_del (&link->link_list);
+		link->is_linked = 0;
+	}
+	if (MPDCCP_LINK_ISDEV(link)) {
+		mpdccp_pr_info ("mpdccp_link:: device link released (dev=%s)\n", link->ndev_name);
+	} else {
+		mpdccp_pr_info ("mpdccp_link:: named link %s released\n", link->name);
+	}
+	mpdccp_link_sysfs_del (link);
+	link->ndev = NULL;
+	link->is_released = 1;
+	mpdccp_link_put (link);
+}
+
+
+/* **************************
+ * react on device notifiers 
+ * **************************/
+
+static int link_ndev_event (struct notifier_block *, unsigned long, void*);
+
+static struct notifier_block mpdccp_link_netdev_notifier = {
+	.notifier_call = link_ndev_event,
+};
+
+
+static
+int
+link_ndev_event (nblk, event, ptr)
+	struct notifier_block	*nblk;
+	unsigned long		event;
+	void			*ptr;
+{
+	struct net_device	*ndev;
+
+	ndev = netdev_notifier_info_to_dev (ptr);
+	if (!ndev) return NOTIFY_DONE;
+	dev_hold (ndev);
+	
+	switch (event) {
+	case NETDEV_REGISTER:
+		mpdccp_link_add (NULL, NULL, ndev, NULL);
+		break;
+	case NETDEV_UNREGISTER:
+		mpdccp_link_release (MPDCCP_LINK_FROM_DEV(ndev));
+		break;
+	case NETDEV_CHANGENAME:
+		mpdccp_link_sysfs_changedevname (MPDCCP_LINK_FROM_DEV(ndev));
+		break;
+	}
+	dev_put (ndev);
+	return NOTIFY_DONE;
+}
+
+
+
+
+/* ************************
+ * notifier functions
+ * ************************/
+
+static RAW_NOTIFIER_HEAD(mpdccp_link_chain);
+
+
+int
+register_mpdccp_link_notifier (
+	struct notifier_block	*nb)
+{
+	int	ret;
+
+	rtnl_lock();
+	ret = raw_notifier_chain_register(&mpdccp_link_chain, nb);
+	rtnl_unlock();
+	return ret;
+}
+EXPORT_SYMBOL(register_mpdccp_link_notifier);
+
+
+int
+unregister_mpdccp_link_notifier (
+	struct notifier_block	*nb)
+{
+	int	ret;
+
+	rtnl_lock();
+	ret = raw_notifier_chain_unregister(&mpdccp_link_chain, nb);
+	rtnl_unlock();
+	return ret;
+}
+EXPORT_SYMBOL(unregister_mpdccp_link_notifier);
+
+int
+call_mpdccp_link_notifiers (
+	unsigned long				val,
+	struct mpdccp_link_info	*link)
+{
+	int	ret;
+
+	struct mpdccp_link_notifier_info info = { .link_info = link, };
+	if (!link) return 0;
+	//mlk_lock;
+	if (MPDCCP_LINK_ISDEV(link)) {
+		if (!MPDCCP_LINK_ISDEV_VALID (link)) {
+			//mlk_unlock;
+			return 0;
+		}
+		info.ndev = MPDCCP_LINK_TO_DEV (link);
+	}
+	ret = raw_notifier_call_chain(&mpdccp_link_chain, val, &info);
+	//mlk_unlock;
+	return ret;
+}
+EXPORT_SYMBOL(call_mpdccp_link_notifiers);
+
+/*
+* Connection ID implementation. 
+* Here uniqueness is guaranteed by keeping track of all assigned CIDs in memory.
+* When a new CID is generated, we loop the list to make sure it's unique.
+* There are other, more performant ways to do this.
+*/
+
+struct mpdccp_link_cidb {
+	/* List of local MPDCCP connection IDs */
+	struct list_head list;
+	u32 cid;
+};
+
+struct list_head __rcu cid_list;
+spinlock_t cid_list_lock;
+
+u32 mpdccp_link_generate_cid()
+{
+    bool unique = false;
+    struct mpdccp_link_cidb *mcid, *ptr;
+    mcid = kmalloc(sizeof(struct mpdccp_link_cidb), GFP_ATOMIC);
+    if (!mcid)
+        return 0;
+
+    spin_lock_bh(&cid_list_lock);
+    while (!unique || !mcid->cid) {
+        mcid->cid = get_random_u32();
+        unique = true;
+
+        list_for_each_entry(ptr, &cid_list, list) {
+            if (ptr->cid == mcid->cid) {
+                unique = false;
+                break;
+            }
+        }
+    }
+    
+    list_add_tail_rcu(&mcid->list, &cid_list);
+    spin_unlock_bh(&cid_list_lock);
+	mpdccp_pr_debug("generated new CID: %u", mcid->cid);
+    return mcid->cid;
+}
+EXPORT_SYMBOL(mpdccp_link_generate_cid);
+
+void mpdccp_link_free_cid(u32 cid){
+    struct mpdccp_link_cidb *ptr;
+	if(!cid) return;
+    spin_lock_bh(&cid_list_lock);
+    list_for_each_entry(ptr, &cid_list, list) {
+        if (ptr->cid == cid) {
+            list_del_rcu(&ptr->list);
+            INIT_LIST_HEAD(&ptr->list);
+            kfree(ptr);
+            break;
+        }
+    }
+    spin_unlock_bh(&cid_list_lock);
+	mpdccp_pr_debug("freed CID: %u", cid);
+}
+EXPORT_SYMBOL(mpdccp_link_free_cid);
+
+static
+int
+link_net_init (
+	struct net	*net)
+{
+	struct mpdccp_link_net_data	*linkdata;
+	int									ret;
+
+	if (!net || mpdccp_link_net_id < 0) return 0;
+	mpdccp_pr_info ("mpdccp_link:: new network namespace created\n");
+	linkdata = net_generic (net, mpdccp_link_net_id);
+	*linkdata = (struct mpdccp_link_net_data) { .net = net };
+	atomic_set (&linkdata->counter, 0);
+	mpdccp_link_sysfs_netinit (linkdata);
+	ret = mpdccp_link_add (&linkdata->fallback, net, NULL, "fallback");
+	if (ret < 0) {
+		printk ("mpdccp_link::net_init: error creating fallback link: %d\n", ret);
+	}
+	return 0;
+}
+
+static
+void
+link_del_by_net (
+	struct net	*net)
+{
+	struct mpdccp_link_info	*link, *tmp;
+
+	mlk_lock;
+	list_for_each_entry_safe (link, tmp, &link_list, link_list) {
+		if (!MPDCCP_LINK_TO_NET(link)) continue;
+		if (MPDCCP_LINK_TO_NET(link) != net) continue;
+		/* only non devices */
+		if (MPDCCP_LINK_ISDEV(link)) continue;
+		/* delete it manually to avoid dead lock */
+	   list_del (&link->link_list);
+		link->is_linked = 0;
+		mpdccp_link_release_nolock (link);
+	}
+	mlk_unlock;
+}
+
+static
+void
+link_net_invalid (
+	struct net	*net)
+{
+	struct mpdccp_link_info	*link, *tmp;
+
+	mlk_lock;
+	list_for_each_entry_safe (link, tmp, &link_list, link_list) {
+		if (!MPDCCP_LINK_TO_NET(link)) continue;
+		if (MPDCCP_LINK_TO_NET(link) != net) continue;
+		link->net = NULL;
+	}
+	mlk_unlock;
+}
+
+
+static
+void
+link_net_exit (
+	struct net	*net)
+{
+	struct mpdccp_link_net_data	*linkdata;
+
+	if (!net || mpdccp_link_net_id < 0) return;
+	mpdccp_pr_info ("mpdccp_link:: network namespace will be destroyed\n");
+	link_del_by_net (net);
+	link_net_invalid (net);
+	linkdata = net_generic (net, mpdccp_link_net_id);
+	mpdccp_link_sysfs_netexit (linkdata);
+	*linkdata = (struct mpdccp_link_net_data) { .fallback = NULL };
+}
+
+
+static struct pernet_operations link_net_ops = {
+	.id = &mpdccp_link_net_id,
+	.init = link_net_init,
+	.exit = link_net_exit,
+	.size = sizeof (struct mpdccp_link_net_data),
+};
+
+
+
+static
+int
+__init
+mpdccp_link_module_init (void)
+{
+	int	ret;
+	mpdccp_pr_info ("mpdccp_link_module_init()\n");
+	mlk_init;
+	ret = mpdccp_link_sysfs_init ();
+	if (ret < 0) {
+		mpdccp_pr_error ("mpdccp_link: error in mpdccp_link_sysfs_init(): %d\n", ret);
+		return ret;
+	}
+	ret = register_pernet_subsys (&link_net_ops);
+	if (ret < 0) {
+		mpdccp_pr_error ("mpdccp_link: error registering pernet subsystem: %d\n", ret);
+		return ret;
+	}
+	printk ("mpdccp_link_net_id = %d\n", mpdccp_link_net_id);
+	ret = register_netdevice_notifier (&mpdccp_link_netdev_notifier);
+	if (ret < 0) {
+		mpdccp_pr_error ("mpdccp_link: error in register_netdevice_notifier(): %d\n", ret);
+		return ret;
+	}
+
+    INIT_LIST_HEAD(&cid_list);
+    spin_lock_init(&cid_list_lock);
+
+	mpdccp_pr_info ("mpdccp_link_module_init() - done\n");
+	return 0;
+}
+
+static
+void
+__exit
+mpdccp_link_module_exit (void)
+{
+	unregister_netdevice_notifier (&mpdccp_link_netdev_notifier);
+	unregister_pernet_subsys (&link_net_ops);
+	mpdccp_link_sysfs_exit ();
+}
+
+
+
+
+module_init (mpdccp_link_module_init);
+module_exit (mpdccp_link_module_exit);
+
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Frank Reker <frank@reker.net>");
+MODULE_VERSION("3");
+MODULE_DESCRIPTION("Link Information for Scheduling");
+
+
+
+
+
+/*
+ * Overrides for XEmacs and vim so that we get a uniform tabbing style.
+ * XEmacs/vim will notice this stuff at the end of the file and automatically
+ * adjust the settings for this buffer only.  This must remain at the end
+ * of the file.
+ * ---------------------------------------------------------------------------
+ * Local variables:
+ * c-indent-level: 3
+ * c-basic-offset: 3
+ * tab-width: 3
+ * End:
+ * vim:tw=0:ts=3:wm=0:
+ */
diff --git a/net/dccp/mpdccp_link_sysfs.c b/net/dccp/mpdccp_link_sysfs.c
new file mode 100644
index 0000000000000..90332fb04da52
--- /dev/null
+++ b/net/dccp/mpdccp_link_sysfs.c
@@ -0,0 +1,739 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <net/mpdccp_link.h>
+#include <linux/rtnetlink.h>
+#include <net/netns/generic.h>
+#include <linux/sched/signal.h>
+#include "mpdccp_link_sysfs.h"
+
+
+static const char fmt_hex[] = "%#x\n";
+static const char fmt_dec[] = "%d\n";
+static const char fmt_ulong[] = "%lu\n";
+static const char fmt_int[] = "%d\n";
+static const char fmt_uint[] = "%u\n";
+static const char fmt_u32[] = "%u\n";
+static const char fmt_u64[] = "%llu\n";
+static const char fmt_str[] = "%s\n";
+
+#define mpdccp_pr_debug(format, a...) do { printk (KERN_DEBUG format, ##a); } while (0)
+#define mpdccp_pr_info(format, a...) do { printk (KERN_INFO format, ##a); } while (0)
+#define mpdccp_pr_error(format, a...) do { printk (KERN_ERR format, ##a); } while (0)
+
+static void null_kobj_release(struct kobject *kobj)
+{
+	/* realy nothing to be done, because refcounter is not used */
+}
+
+static
+ssize_t
+link_sys_store (
+	struct kobject				*kobj,
+	struct kobj_attribute	*attr,
+	const char					*buf,
+	size_t						len,
+	int							(*set)(struct mpdccp_link_info *, const char *, size_t))
+{
+	struct net					*net;
+	struct mpdccp_link_info	*link;
+	int							ret = 0;
+
+	link = container_of (kobj, struct mpdccp_link_info, kobj);
+	net = MPDCCP_LINK_TO_NET (link);
+	if (!net) return -EINVAL;
+
+	if (!ns_capable(net->user_ns, CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (!rtnl_trylock())
+		return restart_syscall();
+
+	ret = (*set)(link, buf, len);
+
+	rtnl_unlock();
+	return (ret==0) ? len : ret;
+}
+
+
+#define SHOW_ATTR(name,fmt) \
+	static \
+	ssize_t \
+	link_sys_show_attr_##name ( \
+		struct kobject				*kobj, \
+		struct kobj_attribute	*attr, \
+		char							*buf) \
+	{ \
+		struct mpdccp_link_info	*link; \
+		link = container_of (kobj, struct mpdccp_link_info, kobj); \
+		return sprintf (buf, fmt, link->name); \
+	}
+
+#define STORE_ATTR(name,setfunc) \
+	static \
+	ssize_t \
+	link_sys_store_attr_##name ( \
+		struct kobject				*kobj, \
+		struct kobj_attribute	*attr, \
+		const char					*buf, \
+		size_t						count) \
+	{ \
+		return link_sys_store (kobj, attr, buf, count, setfunc); \
+	}
+
+#define STORE_ATTR_SCAN(name,fmt,typ,setfunc) \
+	static \
+	int \
+	link_sys_store_scan_attr_##name ( \
+		struct mpdccp_link_info	*link, \
+		const char					*buf, \
+		size_t						count) \
+	{ \
+		typ	val; \
+		sscanf (buf, fmt, &val); \
+		return setfunc (link, val); \
+	} \
+	STORE_ATTR(name,link_sys_store_scan_attr_##name);
+	
+
+
+#define MKATTR_RO(name) \
+	static struct kobj_attribute name##_attribute = \
+			__ATTR(name, 0444, link_sys_show_attr_##name, NULL);
+
+#define MKATTR_RW(name) \
+	static struct kobj_attribute name##_attribute = \
+			__ATTR(name, 0644, link_sys_show_attr_##name, link_sys_store_attr_##name);
+
+#define MKATTR_WO(name) \
+	static struct kobj_attribute name##_attribute = \
+			__ATTR(name, 0200, NULL, link_sys_store_attr_##name);
+
+
+#define MLATTR_RO(name,fmt) \
+	SHOW_ATTR(name,fmt)\
+	MKATTR_RO(name)
+#define MLATTR_RW(name,fmt,typ) \
+	SHOW_ATTR(name,fmt)\
+	STORE_ATTR_SCAN(name,fmt,typ,mpdccp_link_change_##name)\
+	MKATTR_RW(name)
+
+MLATTR_RO(id,fmt_int)
+MLATTR_RO(config_cnt,fmt_u64)
+#ifdef CONFIG_MPDCCP_STATS
+MLATTR_RO(allref,fmt_u64)
+#endif
+MLATTR_RW(mpdccp_prio,fmt_u32,u32)
+MLATTR_RW(mpdccp_maxbuf,fmt_u64,u64)
+MLATTR_RW(mpdccp_T_delay,fmt_u32,u32)
+MLATTR_RW(mpdccp_T_start_delay,fmt_u32,u32)
+MLATTR_RW(mpdccp_T_lpu,fmt_u32,u32)
+MLATTR_RW(mpdccp_T_lpu_min,fmt_u32,u32)
+MLATTR_RW(mpdccp_lpu_cnt,fmt_u32,u32)
+MLATTR_RW(mpdccp_ignthrottle,fmt_uint,unsigned int)
+MLATTR_RW(mpdccp_match_mark,fmt_u32,u32)
+MLATTR_RW(mpdccp_match_mask,fmt_u32,u32)
+MLATTR_RW(mpdccp_send_mark,fmt_u32,u32)
+MLATTR_RW(mpdccp_path_type,fmt_u32,u32)
+MLATTR_RW(mpdccp_match_pathtype,fmt_u32,u32)
+MLATTR_RO(mpdccp_rx_packets,fmt_u64)
+MLATTR_RO(mpdccp_rx_bytes,fmt_u64)
+MLATTR_RO(mpdccp_tx_packets,fmt_u64)
+MLATTR_RO(mpdccp_tx_bytes,fmt_u64)
+
+
+static
+int
+link_sys_store_scan_attr_mpdccp_cgstalg (
+	struct mpdccp_link_info	*link,
+	const char					*buf,
+	size_t						len)
+{
+	/* ignore trailing newline */
+	if (len >  0 && buf[len - 1] == '\n')
+		--len;
+	return mpdccp_link_change_mpdccp_cgstalg(link, buf, len);
+}
+STORE_ATTR(mpdccp_cgstalg,link_sys_store_scan_attr_mpdccp_cgstalg);
+
+static
+ssize_t
+link_sys_show_attr_mpdccp_cgstalg (
+	struct kobject				*kobj,
+	struct kobj_attribute	*attr,
+	char							*buf)
+{
+	struct mpdccp_link_info	*link;
+	ssize_t						ret;
+
+	link = container_of (kobj, struct mpdccp_link_info, kobj);
+	if (!rtnl_trylock())
+		return restart_syscall();
+	if (*link->mpdccp_cgstalg) {
+		ret = sprintf(buf, "%s\n", link->mpdccp_cgstalg);
+	} else {
+		ret = sprintf(buf, "default\n");
+	}
+	rtnl_unlock();
+	return ret;
+}
+MKATTR_RW(mpdccp_cgstalg)
+
+static
+ssize_t
+link_sys_show_attr_refcount (
+	struct kobject				*kobj,
+	struct kobj_attribute	*attr,
+	char							*buf)
+{
+	struct mpdccp_link_info	*link;
+	link = container_of (kobj, struct mpdccp_link_info, kobj);
+	return sprintf (buf, "%u\n", (unsigned)MPDCCP_LINK_REFCOUNT(link));
+}
+MKATTR_RO(refcount)
+
+static
+ssize_t
+link_sys_show_attr_link_name (
+	struct kobject				*kobj,
+	struct kobj_attribute	*attr,
+	char							*buf)
+{
+	struct mpdccp_link_info	*link;
+	link = container_of (kobj, struct mpdccp_link_info, kobj);
+	return sprintf (buf, "%s\n", MPDCCP_LINK_NAME(link));
+}
+MKATTR_RO(link_name)
+
+static
+ssize_t
+link_sys_show_attr_is_dev_link (
+	struct kobject				*kobj,
+	struct kobj_attribute	*attr,
+	char							*buf)
+{
+	struct mpdccp_link_info	*link;
+	link = container_of (kobj, struct mpdccp_link_info, kobj);
+	return sprintf (buf, "%d\n", MPDCCP_LINK_ISDEV(link) ? 1 : 0);
+}
+MKATTR_RO(is_dev_link)
+
+static struct attribute *mpdccp_link_sys_attrs[] = {
+	&id_attribute.attr,
+	&config_cnt_attribute.attr,
+	&refcount_attribute.attr,
+#ifdef CONFIG_MPDCCP_STATS
+	&allref_attribute.attr,
+#endif
+	&link_name_attribute.attr,
+	&is_dev_link_attribute.attr,
+	&mpdccp_prio_attribute.attr,
+	&mpdccp_maxbuf_attribute.attr,
+	&mpdccp_T_delay_attribute.attr,
+	&mpdccp_T_start_delay_attribute.attr,
+	&mpdccp_T_lpu_attribute.attr,
+	&mpdccp_T_lpu_min_attribute.attr,
+	&mpdccp_lpu_cnt_attribute.attr,
+	&mpdccp_ignthrottle_attribute.attr,
+	&mpdccp_match_mark_attribute.attr,
+	&mpdccp_match_mask_attribute.attr,
+	&mpdccp_send_mark_attribute.attr,
+	&mpdccp_cgstalg_attribute.attr,
+	&mpdccp_path_type_attribute.attr,
+	&mpdccp_match_pathtype_attribute.attr,
+	&mpdccp_rx_packets_attribute.attr,
+	&mpdccp_rx_bytes_attribute.attr,
+	&mpdccp_tx_packets_attribute.attr,
+	&mpdccp_tx_bytes_attribute.attr,
+	NULL,
+};
+
+/* extra attributes */
+static
+int
+link_sys_store_scan_attr_name (
+	struct mpdccp_link_info	*link,
+	const char					*buf,
+	size_t						count)
+{
+	char		name[sizeof(link->name)];
+	int		ret;
+	size_t	len = count;
+
+	if (len >  0 && buf[len - 1] == '\n') --len;
+	if (len > sizeof (name)-1) return -EINVAL;
+	strncpy (name, buf, len);
+	name[len]=0;
+	mpdccp_pr_debug ("mpdccp_link_sysfs_changename(): %s -> %s(%d)\n", link->name, name, (int)len);
+	ret = mpdccp_link_sysfs_changename (link, name);
+	if (ret < 0) return ret;
+	return count;
+}
+STORE_ATTR(name,link_sys_store_scan_attr_name);
+SHOW_ATTR(name,fmt_str);
+MKATTR_RW(name);
+MLATTR_RO(ndev_name, fmt_str);
+static struct attribute *mpdccp_link_sys_name_attrs[] = {
+	&name_attribute.attr,
+	NULL,
+};
+static struct attribute *mpdccp_link_sys_devname_attrs[] = {
+	&ndev_name_attribute.attr,
+	NULL,
+};
+static struct attribute_group mpdccp_link_sys_name_attr_group = {
+	.attrs = mpdccp_link_sys_name_attrs,
+};
+static struct attribute_group mpdccp_link_sys_devname_attr_group = {
+	.attrs = mpdccp_link_sys_devname_attrs,
+};
+
+
+
+#ifdef CONFIG_MPDCCP_STATS
+MLATTR_RO(mpdccp_noavail_hard, fmt_u64)
+MLATTR_RO(mpdccp_noavail_hard_state, fmt_u64)
+MLATTR_RO(mpdccp_noavail_hard_pre, fmt_u64)
+MLATTR_RO(mpdccp_noavail_hard_pf, fmt_u64)
+MLATTR_RO(mpdccp_noavail_hard_loss, fmt_u64)
+MLATTR_RO(mpdccp_noavail_nocwnd, fmt_u64)
+MLATTR_RO(mpdccp_noavail_nospace_maxbuf, fmt_u64)
+MLATTR_RO(mpdccp_noavail_nospace, fmt_u64)
+MLATTR_RO(mpdccp_noavail_zerownd, fmt_u64)
+MLATTR_RO(mpdccp_noavail_nobuf, fmt_u64)
+MLATTR_RO(mpdccp_noavail_delay, fmt_u64)
+MLATTR_RO(mpdccp_noavail_start_delay, fmt_u64)
+MLATTR_RO(mpdccp_noavail_dontreinject, fmt_u64)
+MLATTR_RO(mpdccp_selected_delayed, fmt_u64)
+MLATTR_RO(mpdccp_selected_onlypath, fmt_u64)
+MLATTR_RO(mpdccp_selected_shutdown, fmt_u64)
+MLATTR_RO(mpdccp_selected_backup, fmt_u64)
+MLATTR_RO(mpdccp_selected_good, fmt_u64)
+MLATTR_RO(mpdccp_selected_best, fmt_u64)
+MLATTR_RO(mpdccp_selected_fallback, fmt_u64)
+MLATTR_RO(mpdccp_selected, fmt_u64)
+
+
+static
+ssize_t
+link_sys_store_attr_mpdccp_resetstat (
+	struct kobject				*kobj,
+	struct kobj_attribute	*attr,
+	const char					*buf,
+	size_t						count)
+{
+	struct mpdccp_link_info	*link = container_of (kobj, struct mpdccp_link_info, kobj);
+	unsigned int				ret;
+	sscanf (buf, "%u", &ret);
+	if (ret != 0) {
+		mpdccp_link_change_mpdccp_resetstat (link);
+	}
+	return count;
+}
+MKATTR_WO(mpdccp_resetstat)
+
+static struct attribute *mpdccp_link_stat_sys_attrs[] = {
+	&mpdccp_resetstat_attribute.attr,
+	&mpdccp_noavail_hard_attribute.attr,
+	&mpdccp_noavail_hard_state_attribute.attr,
+	&mpdccp_noavail_hard_pre_attribute.attr,
+	&mpdccp_noavail_hard_pf_attribute.attr,
+	&mpdccp_noavail_hard_loss_attribute.attr,
+	&mpdccp_noavail_nocwnd_attribute.attr,
+	&mpdccp_noavail_nospace_maxbuf_attribute.attr,
+	&mpdccp_noavail_nospace_attribute.attr,
+	&mpdccp_noavail_zerownd_attribute.attr,
+	&mpdccp_noavail_nobuf_attribute.attr,
+	&mpdccp_noavail_delay_attribute.attr,
+	&mpdccp_noavail_start_delay_attribute.attr,
+	&mpdccp_noavail_dontreinject_attribute.attr,
+	&mpdccp_selected_delayed_attribute.attr,
+	&mpdccp_selected_onlypath_attribute.attr,
+	&mpdccp_selected_shutdown_attribute.attr,
+	&mpdccp_selected_backup_attribute.attr,
+	&mpdccp_selected_good_attribute.attr,
+	&mpdccp_selected_best_attribute.attr,
+	&mpdccp_selected_fallback_attribute.attr,
+	&mpdccp_selected_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group mpdccp_link_stat_sys_attr_group = {
+	.name = "statistics",
+	.attrs = mpdccp_link_stat_sys_attrs,
+};
+
+#endif	/* CONFIG_MPDCCP_STATS */
+
+
+static
+const
+void *
+link_namespace (
+	struct kobject	*kobj)
+{
+	struct mpdccp_link_info	*link = container_of (kobj, struct mpdccp_link_info, kobj);
+	return MPDCCP_LINK_TO_NET (link);
+}
+
+
+
+
+static struct kobject *mpdccp_link_sys_base_kobj = NULL;
+
+#if 0
+static void link_kobj_release(struct kobject *kobj)
+{
+	struct mpdccp_link_info	*link = container_of (kobj, struct mpdccp_link_info, kobj);
+	mpdccp_link_release (link);
+}
+#endif
+
+static struct kobj_type link_kobj_ktype = {
+	//.release   = link_kobj_release,
+	.release   = null_kobj_release,	/* ref counter is not used, so it is ok */
+	.sysfs_ops = &kobj_sysfs_ops,
+	.namespace = link_namespace,
+	.default_attrs = mpdccp_link_sys_attrs,
+};
+
+
+int
+mpdccp_link_sysfs_add (
+	struct mpdccp_link_info	*link)
+{
+	struct kobject					*devkobj;
+	int								ret;
+	struct mpdccp_link_net_data	*ld = NULL;
+
+	if (!link) return -EINVAL;
+	ret = kobject_init_and_add (&link->kobj, &link_kobj_ktype, mpdccp_link_sys_base_kobj, "%d", link->id);
+	if (ret < 0) {
+		mpdccp_pr_error ("error in kobject_add(): %d\n", ret);
+		return ret;
+	}
+	if (MPDCCP_LINK_ISDEV(link)) {
+		sysfs_create_group(&link->kobj, &mpdccp_link_sys_devname_attr_group);	/* ignore error */
+	} else {
+		sysfs_create_group(&link->kobj, &mpdccp_link_sys_name_attr_group);	/* ignore error */
+	}
+#ifdef CONFIG_MPDCCP_STATS
+	ret = sysfs_create_group(&link->kobj, &mpdccp_link_stat_sys_attr_group);
+	if (ret < 0) {
+		mpdccp_pr_error ("error in kobject_add() for stat: %d\n", ret);
+		kobject_put (&link->kobj);
+		return ret;
+	}
+#endif
+	if (mpdccp_link_net_id >= 0 && MPDCCP_LINK_TO_NET(link)) {
+		ld = net_generic (MPDCCP_LINK_TO_NET(link), mpdccp_link_net_id);
+	}
+
+	if (*link->name && ld) {
+		sysfs_create_link_nowarn (&ld->name, &link->kobj, link->name);
+	}
+	if (MPDCCP_LINK_ISDEV(link)) {
+		/* create symlinks */
+		//strcpy (link->ndev_name, MPDCCP_LINK_TO_DEV(link)->name);
+		if (ld) sysfs_create_link (&ld->dev, &link->kobj, link->ndev_name);
+		devkobj = &MPDCCP_LINK_TO_DEV(link)->dev.kobj;
+		sysfs_create_link (&link->kobj, devkobj, "dev");
+		sysfs_create_link (devkobj, &link->kobj, "mpdccp_link");
+	}
+	link->sysfs_to_del = 1;
+	return 0;
+}
+
+int
+mpdccp_link_sysfs_changename (
+	struct mpdccp_link_info	*link,
+	const char					*name)
+{
+	struct mpdccp_link_net_data	*ld = NULL;
+
+	if (!link) return -EINVAL;
+	if (!name || strlen (name) > sizeof (link->name)-1) return -EINVAL;
+	if (mpdccp_link_net_id >= 0 && MPDCCP_LINK_TO_NET(link)) {
+		ld = net_generic (MPDCCP_LINK_TO_NET(link), mpdccp_link_net_id);
+	}
+
+	/* we have to change symbolic link */
+	if (ld && *link->name) sysfs_remove_link (&ld->name, link->name);
+	strcpy (link->name, name);
+	if (ld && *link->name) sysfs_create_link (&ld->name, &link->kobj, link->name);
+	return 0;
+}
+
+int
+mpdccp_link_sysfs_changedevname (
+	struct mpdccp_link_info	*link)
+{
+	struct mpdccp_link_net_data	*ld = NULL;
+	if (!link) return -EINVAL;
+	if (mpdccp_link_net_id >= 0 && MPDCCP_LINK_TO_NET(link)) {
+		ld = net_generic (MPDCCP_LINK_TO_NET(link), mpdccp_link_net_id);
+	}
+
+	/* we have to change symbolic link */
+	if (ld) sysfs_remove_link (&ld->dev, link->ndev_name);
+	strcpy (link->ndev_name, MPDCCP_LINK_TO_DEV(link)->name);
+	if (ld) sysfs_create_link (&ld->dev, &link->kobj, link->ndev_name);
+	return 0;
+}
+
+void
+mpdccp_link_sysfs_del (
+	struct mpdccp_link_info	*link)
+{
+	struct kobject					*devkobj;
+	struct mpdccp_link_net_data	*ld = NULL;
+
+	if (!link) return;
+	if (!link->sysfs_to_del) return;
+	if (mpdccp_link_net_id >= 0 && MPDCCP_LINK_TO_NET(link)) {
+		ld = net_generic (MPDCCP_LINK_TO_NET(link), mpdccp_link_net_id);
+	}
+	if (!ld) return;
+	if (MPDCCP_LINK_ISDEV(link)) {
+		sysfs_remove_link (&ld->dev, link->ndev_name);
+		if (MPDCCP_LINK_TO_DEV(link)) {
+			devkobj = &MPDCCP_LINK_TO_DEV(link)->dev.kobj;
+			sysfs_remove_link (devkobj, "mpdccp_link");
+		}
+	} else if (*link->name) {
+		sysfs_remove_link (&ld->name, link->name);
+	}
+	link->sysfs_to_del = 0;
+
+	kobject_put (&link->kobj);
+}
+
+
+/*
+ * create dev and name directories on a ns base
+ */
+
+static
+const
+void *
+linkdev_namespace (
+	struct kobject	*kobj)
+{
+	struct mpdccp_link_net_data	*ld = container_of (kobj, struct mpdccp_link_net_data, dev);
+	return ld->net;
+}
+static
+const
+void *
+linkname_namespace (
+	struct kobject	*kobj)
+{
+	struct mpdccp_link_net_data	*ld = container_of (kobj, struct mpdccp_link_net_data, name);
+	return ld->net;
+}
+
+static struct kobj_type linkdev_kobj_ktype = {
+	.release   = null_kobj_release,	/* ref counter is not used, so it is ok */
+	.sysfs_ops = &kobj_sysfs_ops,
+	.namespace = linkdev_namespace,
+};
+static struct kobj_type linkname_kobj_ktype = {
+	.release   = null_kobj_release,	/* ref counter is not used, so it is ok */
+	.sysfs_ops = &kobj_sysfs_ops,
+	.namespace = linkname_namespace,
+};
+
+int
+mpdccp_link_sysfs_netinit (
+	struct mpdccp_link_net_data	*ld)
+{
+	int	ret, ret2=0;
+
+	if (!ld) return -EINVAL;
+  	ret = kobject_init_and_add (&ld->dev, &linkdev_kobj_ktype, mpdccp_link_sys_base_kobj, "dev");
+	if (ret < 0) {
+		printk ("mpdccp_link::netinit: error creatind \"dev\" dir: %d\n", ret);
+		ret2 = ret;
+	}
+  	ret = kobject_init_and_add (&ld->name, &linkname_kobj_ktype, mpdccp_link_sys_base_kobj, "name");
+	if (ret < 0) {
+		//kobject_put (&ld->dev);
+		printk ("mpdccp_link::netinit: error creatind \"name\" dir: %d\n", ret);
+		ret2 = ret;
+	}
+	return ret2;
+}
+
+void
+mpdccp_link_sysfs_netexit (
+	struct mpdccp_link_net_data	*ld)
+{
+	if (!ld) return;
+	kobject_put (&ld->dev);
+	kobject_put (&ld->name);
+}
+
+
+/*
+ * create base dir
+ */
+
+static
+ssize_t
+link_sys_store_attr_add_link (
+	struct kobject				*kobj,
+	struct kobj_attribute	*attr,
+	const char					*buf,
+	size_t						count)
+{
+	struct net	*net = current->nsproxy->net_ns;
+	int			ret;
+	size_t		len = count;
+	char			name[IFNAMSIZ];
+
+	if (len >  0 && buf[len - 1] == '\n') --len;
+	if (!len || len >= sizeof (name)) return -EINVAL;
+	strncpy (name, buf, len);
+	name[len]=0;
+	ret = mpdccp_link_add (NULL, net, NULL, name);
+	if (ret < 0) return ret;
+	return count;
+}
+
+static
+ssize_t
+link_sys_store_attr_del_link (
+	struct kobject				*kobj,
+	struct kobj_attribute	*attr,
+	const char					*buf,
+	size_t						count)
+{
+	struct net					*net = current->nsproxy->net_ns;
+	struct mpdccp_link_info	*link;
+	char							name[sizeof(link->name)];
+	size_t						len = count;
+
+	if (len >  0 && buf[len - 1] == '\n') --len;
+	if (!len || len >= sizeof (name)) return -EINVAL;
+	strncpy (name, buf, len);
+	name[len]=0;
+	link = mpdccp_link_find_by_name (net, name);
+	if (!link) return -ENOENT;
+	if (link == mpdccp_getfallbacklink (net)) return -EPERM;
+	//mpdccp_link_sysfs_del (link);
+	mpdccp_link_release (link);
+	return count;
+}
+MKATTR_WO(add_link)
+MKATTR_WO(del_link)
+static struct attribute *mpdccp_link_base_sys_attrs[] = {
+	&add_link_attribute.attr,
+	&del_link_attribute.attr,
+	NULL
+};
+static struct attribute_group mpdccp_link_sys_base_attr_group = {
+	.attrs = mpdccp_link_base_sys_attrs,
+};
+
+
+static
+const
+struct kobj_ns_type_operations *
+link_ns_op (
+	struct kobject	*kobj)
+{
+	return &net_ns_type_operations;
+}
+
+static struct kobj_type linkbase_kobj_ktype = {
+	.release   = null_kobj_release,	/* ref counter is not used, so it is ok */
+	.sysfs_ops = &kobj_sysfs_ops,
+	.child_ns_type = link_ns_op,
+	//.default_attrs = mpdccp_link_base_sys_attrs,
+};
+
+
+int
+mpdccp_link_sysfs_init (void)
+{
+	struct kobject	*parent;
+	int				ret;
+	
+	if (mpdccp_link_sys_base_kobj) return 0;
+#ifdef MODULE
+	parent = &THIS_MODULE->mkobj.kobj;
+#else
+	parent = kset_find_obj (module_kset, KBUILD_MODNAME);
+#endif
+	mpdccp_link_sys_base_kobj = kzalloc (sizeof (struct kobject), GFP_KERNEL);
+   if (!mpdccp_link_sys_base_kobj) return -ENOMEM;
+   ret = kobject_init_and_add (mpdccp_link_sys_base_kobj, &linkbase_kobj_ktype, parent, "links");
+   if (ret < 0) {
+      kobject_put (mpdccp_link_sys_base_kobj);
+      mpdccp_link_sys_base_kobj = NULL;
+      return ret;
+   }
+	/* I don't like to put it on top level, better in links, but 
+	 * does not work due to namespace tagging
+	 */
+	ret = sysfs_create_group (parent, &mpdccp_link_sys_base_attr_group);
+   if (ret < 0) {
+      kobject_put (mpdccp_link_sys_base_kobj);
+      mpdccp_link_sys_base_kobj = NULL;
+      return ret;
+   }
+	return 0;
+}
+
+void
+mpdccp_link_sysfs_exit (void)
+{
+	if (mpdccp_link_sys_base_kobj) {
+		kobject_put (mpdccp_link_sys_base_kobj);
+		mpdccp_link_sys_base_kobj = NULL;
+	}
+}
+
+
+
+
+
+
+
+
+
+
+/*
+ * Overrides for XEmacs and vim so that we get a uniform tabbing style.
+ * XEmacs/vim will notice this stuff at the end of the file and automatically
+ * adjust the settings for this buffer only.  This must remain at the end
+ * of the file.
+ * ---------------------------------------------------------------------------
+ * Local variables:
+ * c-indent-level: 3
+ * c-basic-offset: 3
+ * tab-width: 3
+ * End:
+ * vim:tw=0:ts=3:wm=0:
+ */
diff --git a/net/dccp/mpdccp_link_sysfs.h b/net/dccp/mpdccp_link_sysfs.h
new file mode 100644
index 0000000000000..404cffec8cdc9
--- /dev/null
+++ b/net/dccp/mpdccp_link_sysfs.h
@@ -0,0 +1,48 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#ifndef _LINUX_MPDCCP_LINK_SYSFS_H
+#define _LINUX_MPDCCP_LINK_SYSFS_H
+
+#include <net/mpdccp_link.h>
+
+void mpdccp_link_release (struct mpdccp_link_info*);
+
+#ifdef CONFIG_SYSFS
+int mpdccp_link_sysfs_add (struct mpdccp_link_info *link);
+void mpdccp_link_sysfs_del (struct mpdccp_link_info *link);
+int mpdccp_link_sysfs_changedevname (struct mpdccp_link_info *link);
+int mpdccp_link_sysfs_changename (struct mpdccp_link_info*, const char *);
+int mpdccp_link_sysfs_netinit (struct mpdccp_link_net_data*);
+void mpdccp_link_sysfs_netexit (struct mpdccp_link_net_data*);
+int mpdccp_link_sysfs_init (void);
+void mpdccp_link_sysfs_exit (void);
+#else
+static inline int mpdccp_link_sysfs_add (struct mpdccp_link_info *link) { return 0; };
+static inline void mpdccp_link_sysfs_del (struct mpdccp_link_info *link) {};
+static inline int mpdccp_link_sysfs_changedevname (struct mpdccp_link_info *link) { return 0; };
+static inline int mpdccp_link_sysfs_changename (struct mpdccp_link_info link, const char *name) { return 0; };
+static inline int mpdccp_link_sysfs_netinit (struct mpdccp_link_net_data *ld) { return 0; };
+static inline void mpdccp_link_sysfs_netexit (struct mpdccp_link_net_data *ld) {};
+static inline int mpdccp_link_sysfs_init (void) { return 0; };
+static inline void mpdccp_link_sysfs_exit (void) {};
+#endif
+
+
+
+#endif	/* _LINUX_MPDCCP_LINK_SYSFS_H */
diff --git a/net/dccp/mpdccp_mod.c b/net/dccp/mpdccp_mod.c
new file mode 100644
index 0000000000000..b2d45632221b9
--- /dev/null
+++ b/net/dccp/mpdccp_mod.c
@@ -0,0 +1,152 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2017 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2017 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ * Copyright (C) 2021 by Romeo Cane, Deutsche Telekom AG
+ *
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/kernel.h>
+#include <linux/workqueue.h>
+#include <linux/dccp.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/skbuff.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/inet.h>
+#include <linux/kthread.h>
+#include <uapi/linux/net.h>
+
+#include <net/inet_common.h>
+#include <net/inet_sock.h>
+#include <net/protocol.h>
+#include <net/sock.h>
+#include <net/tcp_states.h>
+#include <linux/inetdevice.h>
+#include <net/mpdccp_link.h>
+#include <net/mpdccp.h>
+
+#include "mpdccp.h"
+#include "mpdccp_scheduler.h"
+#include "mpdccp_reordering.h"
+#include "mpdccp_pm.h"
+
+
+int mpdccp_enabled = 1;
+module_param(mpdccp_enabled, int, 0644);
+MODULE_PARM_DESC(mpdccp_enabled, "Enable MPDCCP");
+EXPORT_SYMBOL(mpdccp_enabled);
+
+bool mpdccp_debug;
+module_param(mpdccp_debug, bool, 0644);
+MODULE_PARM_DESC(mpdccp_debug, "Enable debug messages");
+EXPORT_SYMBOL(mpdccp_debug);
+
+bool mpdccp_accept_prio;
+module_param(mpdccp_accept_prio, bool, 0644);
+MODULE_PARM_DESC(mpdccp_accept_prio, "Accept priority from incoming mp_prio options");
+EXPORT_SYMBOL(mpdccp_accept_prio);
+
+int mpdccp_sysctl_init (void);
+int mpdccp_ctrl_init (void);
+void mpdccp_sysctl_finish(void);
+void mpdccp_ctrl_finish(void);
+
+
+/* General initialization of MPDCCP */
+static int __init mpdccp_register(void)
+{
+	int ret = 0;
+	
+	mpdccp_pr_debug ("register MPDCCP\n");
+	ret = mpdccp_init_funcs ();
+	if (ret) {
+		mpdccp_pr_error ("Failed to initialize protocol functions.\n");
+		goto err_ctrl_init;
+	}
+	
+	ret = mpdccp_ctrl_init ();
+	if (ret) {
+		mpdccp_pr_error("Failed to initialize ctrl structures.\n");
+		goto err_ctrl_init;
+	}
+
+	ret = mpdccp_pm_setup ();
+	if (ret) {
+		mpdccp_pr_error("Failed to setup path manager\n");
+		goto err_set_pm;
+	}	
+
+	/* Initialize scheduler */
+	ret = mpdccp_scheduler_setup ();
+	if (ret < 0) 
+		goto err_scheduler;
+
+	ret = mpdccp_reordering_setup ();
+	if (ret < 0)
+		goto err_register_reorder;
+
+
+	/* Initialize /proc interface for controlling the solution*/
+	ret = mpdccp_sysctl_init ();
+	if (ret) {
+		mpdccp_pr_error ("Failed to register sysctl.\n");
+		goto err_register_sysctl;
+	}
+	
+	pr_info("MPDCCP: %s release %s", MPDCCP_RELEASE_TYPE, MPDCCP_VERSION);
+
+out:
+	return ret;
+
+err_register_sysctl:
+err_register_reorder:
+	mpdccp_reordering_finish();
+err_set_pm:
+	mpdccp_pm_finish();
+err_scheduler:
+	mpdccp_ctrl_finish();
+err_ctrl_init:
+	mpdccp_pr_error ("Failed to initialize MPDCCP\n");
+goto out;
+}
+
+static void mpdccp_unregister(void)
+{
+	/* TODO: Tear down connections */
+	
+	mpdccp_sysctl_finish();
+	mpdccp_pm_finish();
+	mpdccp_reordering_finish();
+	mpdccp_ctrl_finish();
+	mpdccp_deinit_funcs ();
+}
+
+module_init(mpdccp_register);
+module_exit(mpdccp_unregister);
+
+MODULE_AUTHOR("Andreas Ph. Matz");
+MODULE_AUTHOR("Frank Reker");
+MODULE_AUTHOR("Romeo Cane");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Multipath DCCP");
+MODULE_VERSION(MPDCCP_VERSION);
diff --git a/net/dccp/mpdccp_pm.c b/net/dccp/mpdccp_pm.c
new file mode 100644
index 0000000000000..c470e448d0925
--- /dev/null
+++ b/net/dccp/mpdccp_pm.c
@@ -0,0 +1,197 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2018 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2018 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ *
+ * MPDCCP - Path manager architecture
+ *
+ * A flexible architecture to load arbitrary path managers. 
+ *
+ * The code in this file is partly derived from the MPTCP project's 
+ * mptcp_pm.c and mptcp_fullmesh.c. Derived code is Copyright (C) 
+ * the original authors Christoph Paasch et al.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 3 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/module.h>
+#include <linux/inetdevice.h>
+#include <linux/netdevice.h>
+#include <linux/proc_fs.h>
+#include <net/addrconf.h>
+#include <net/net_namespace.h>
+#include <net/netns/mpdccp.h>
+
+#include "ccids/ccid2.h"
+#include "dccp.h"
+#include <net/mpdccp_link.h>
+#include <net/mpdccp.h>
+#include "mpdccp.h"
+#include "mpdccp_pm.h"
+
+
+static DEFINE_SPINLOCK(mpdccp_pm_list_lock);
+static LIST_HEAD(mpdccp_pm_list);
+
+int mpdccp_pm_setup (void)
+{
+	int	ret;
+
+	ret = mpdccp_pm_default_register();
+	if (ret) {
+		mpdccp_pr_error("Failed to register default path manager.\n");
+		return ret;
+	}
+	ret = mpdccp_set_default_path_manager(CONFIG_DEFAULT_MPDCCP_PM);
+	if (ret) {
+		mpdccp_pr_error("Failed to set default path manager \"%s\".\n", CONFIG_DEFAULT_MPDCCP_PM);
+		return ret;
+	}
+	
+	return 0;
+}
+EXPORT_SYMBOL(mpdccp_pm_setup);
+
+void mpdccp_pm_finish (void)
+{
+	mpdccp_pm_default_unregister();
+}
+EXPORT_SYMBOL(mpdccp_pm_finish);
+
+
+/* Dynamic interface management */
+
+
+struct mpdccp_pm_ops *mpdccp_pm_find(const char *name)
+{
+	struct mpdccp_pm_ops *e;
+
+	list_for_each_entry_rcu(e, &mpdccp_pm_list, list) {
+		if (strcmp(e->name, name) == 0)
+			return e;
+	}
+
+	return NULL;
+}
+EXPORT_SYMBOL(mpdccp_pm_find);
+
+int mpdccp_register_path_manager(struct mpdccp_pm_ops *pm)
+{
+	int ret = 0;
+
+	/* TODO: Add the path manager to the global list of pm net name spaces 
+	PROBLEM: HOW DO WE GET THE NET HERE IN ORDER TO WRITE IT? */
+	spin_lock(&mpdccp_pm_list_lock);
+	if (mpdccp_pm_find(pm->name)) {
+		pr_notice("%s path manager already registered\n", pm->name);
+		ret = -EEXIST;
+	} else {
+		list_add_tail_rcu(&pm->list, &mpdccp_pm_list);
+		pr_info("%s path manager registered\n", pm->name);
+	}
+	spin_unlock(&mpdccp_pm_list_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(mpdccp_register_path_manager);
+
+void mpdccp_unregister_path_manager(struct mpdccp_pm_ops *pm)
+{
+	spin_lock(&mpdccp_pm_list_lock);
+	list_del_rcu(&pm->list);
+	spin_unlock(&mpdccp_pm_list_lock);
+
+	/* Wait for outstanding readers to complete before the
+	 * module gets removed entirely.
+	 *
+	 * A try_module_get() should fail by now as our module is
+	 * in "going" state since no refs are held anymore and
+	 * module_exit() handler being called.
+	 */
+	synchronize_rcu();
+}
+EXPORT_SYMBOL(mpdccp_unregister_path_manager);
+
+/* Get/set the active path manager */
+void mpdccp_get_default_path_manager(char *name)
+{
+	struct mpdccp_pm_ops *pm;
+
+	BUG_ON(list_empty(&mpdccp_pm_list));
+
+	rcu_read_lock();
+	pm = list_entry(mpdccp_pm_list.next, struct mpdccp_pm_ops, list);
+	strncpy(name, pm->name, MPDCCP_PM_NAME_MAX);
+	rcu_read_unlock();
+}
+EXPORT_SYMBOL(mpdccp_get_default_path_manager);
+
+int mpdccp_set_default_path_manager(const char *name)
+{
+	struct mpdccp_pm_ops *pm;
+	int ret = -ENOENT;
+
+	spin_lock(&mpdccp_pm_list_lock);
+	pm = mpdccp_pm_find(name);
+#ifdef CONFIG_MODULES
+	if (!pm && capable(CAP_NET_ADMIN)) {
+		spin_unlock(&mpdccp_pm_list_lock);
+
+		request_module("mpdccp_%s", name);
+		spin_lock(&mpdccp_pm_list_lock);
+		pm = mpdccp_pm_find(name);
+	}
+#endif
+
+	if (pm) {
+		list_move(&pm->list, &mpdccp_pm_list);
+		ret = 0;
+	} else {
+		pr_info("%s is not available\n", name);
+	}
+	spin_unlock(&mpdccp_pm_list_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(mpdccp_set_default_path_manager);
+
+/* Manage refcounts on socket close. */
+void mpdccp_cleanup_path_manager(struct mpdccp_cb *mpcb)
+{
+	if (!mpcb->pm_ops) return;
+	module_put(mpcb->pm_ops->owner);
+}
+EXPORT_SYMBOL(mpdccp_cleanup_path_manager);
+
+void mpdccp_init_path_manager(struct mpdccp_cb *mpcb)
+{
+	struct mpdccp_pm_ops *pm;
+
+	rcu_read_lock();
+
+	list_for_each_entry_rcu(pm, &mpdccp_pm_list, list) {
+		if (try_module_get(pm->owner)) {
+			mpcb->pm_ops = pm;
+			break;
+		}
+	}
+	rcu_read_unlock();
+}
+EXPORT_SYMBOL(mpdccp_init_path_manager);
+
+
+
+
+
diff --git a/net/dccp/mpdccp_pm.h b/net/dccp/mpdccp_pm.h
new file mode 100644
index 0000000000000..6f06ca9d07a46
--- /dev/null
+++ b/net/dccp/mpdccp_pm.h
@@ -0,0 +1,115 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2018 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2018 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ *
+ * MPDCCP - Path manager architecture
+ *
+ * A flexible architecture to load arbitrary path managers. 
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 3 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#ifndef _MPDCCP_PM_H
+#define _MPDCCP_PM_H
+
+#include <net/net_namespace.h>
+#include <linux/netpoll.h>
+#include <linux/rculist.h>
+#include <linux/ktime.h>
+#include "dccp.h"
+#include "mpdccp.h"
+#include "mpdccp_version.h"
+
+/* Maximum lengths for module names */
+#define MPDCCP_PM_NAME_MAX          16
+
+#define pm_jiffies32	((u32)jiffies)
+/*
+ * Namespace related functionality
+ */
+
+/* Path manager namespace */
+struct mpdccp_pm_ns {
+	/* TODO: List of path manager name spaces */
+	void			*next_pm;
+	
+	/* List of local network interfaces */
+	struct list_head	plocal_addr_list;
+	spinlock_t		plocal_lock;
+	
+	struct list_head	retransmit;
+	struct delayed_work	retransmit_worker;
+
+	struct list_head	events;
+	struct delayed_work	address_worker;
+
+	u64 loc4_bits;
+
+	struct net		*net;
+};
+
+/* mpdccp_pm_ops - MPDCCP path manager operations. 
+ * (*new_session)   Create a new connection to a specific target host
+ * (*init)          Initialize path manager and per-subflow data for all connections
+ * name[]           The name of this algorithm
+ * *owner           Useful for memleak detection
+ */
+struct mpdccp_pm_ops {
+	struct list_head	list;
+	
+	int			(*add_init_server_conn) (struct mpdccp_cb*, int);
+	int			(*add_init_client_conn) (struct mpdccp_cb*, struct sockaddr*, int);
+	int			(*claim_local_addr)     (struct mpdccp_cb*, sa_family_t, union inet_addr*);
+	void		(*del_addr)       		(struct mpdccp_cb*, u8, bool, bool);
+	void		(*add_addr)             (struct mpdccp_cb*, sa_family_t, u8, union inet_addr*, u16, bool);
+	int			(*get_id_from_ip)       (struct mpdccp_cb*, union inet_addr*, sa_family_t, bool);
+	int 		(*get_hmac)				(struct mpdccp_cb*, u8, sa_family_t, union inet_addr*, u16, bool, u8*);
+	void		(*rcv_removeaddr_opt)   (struct mpdccp_cb*, u8);
+	void 		(*rcv_prio_opt)			(struct sock*, u8, u64);
+
+	int 		(*rcv_confirm_opt)		(struct mpdccp_cb*, u8*, u8);
+	void 		(*store_confirm_opt)	(struct sock*, u8*, u8, u8, u8);
+	void		(*del_retrans)			(struct net*, struct sock*);
+
+	char			name[MPDCCP_PM_NAME_MAX];
+	struct module		*owner;
+};
+
+
+
+/*
+ * Path management functions
+ */
+
+/* Generic path management functions */
+int mpdccp_pm_setup (void);
+void mpdccp_pm_finish (void);
+
+int mpdccp_register_path_manager(struct mpdccp_pm_ops *pm);
+void mpdccp_unregister_path_manager(struct mpdccp_pm_ops *pm);
+void mpdccp_init_path_manager(struct mpdccp_cb *mpcb);
+void mpdccp_cleanup_path_manager(struct mpdccp_cb *mpcb);
+void mpdccp_get_default_path_manager(char *name);
+int mpdccp_set_default_path_manager(const char *name);
+struct mpdccp_pm_ops *mpdccp_pm_find(const char *name);
+
+
+
+int mpdccp_pm_default_register(void);
+void mpdccp_pm_default_unregister (void);
+
+#endif /* _MPDCCP_PM_H */
+
diff --git a/net/dccp/mpdccp_proto.c b/net/dccp/mpdccp_proto.c
new file mode 100644
index 0000000000000..89680667c595a
--- /dev/null
+++ b/net/dccp/mpdccp_proto.c
@@ -0,0 +1,1077 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ * 
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ * Copyright (C) 2021 by Romeo Cane, Deutsche Telekom AG
+ *
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+
+#include <linux/kernel.h>
+#include <linux/workqueue.h>
+#include <linux/dccp.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/skbuff.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/inet.h>
+#include <linux/kthread.h>
+#include <linux/string.h>
+#include <uapi/linux/net.h>
+
+#include <net/inet_common.h>
+#include <net/inet_sock.h>
+#include <net/protocol.h>
+#include <net/sock.h>
+#include <net/tcp_states.h>
+#include <linux/inetdevice.h>
+#include <net/mpdccp_link.h>
+#include <asm/unaligned.h>
+#include "feat.h"
+#include "mpdccp.h"
+#include "mpdccp_scheduler.h"
+#include "mpdccp_reordering.h"
+#include "mpdccp_pm.h"
+
+static int do_mpdccp_write_xmit (struct sock*, struct sk_buff*);
+int mpdccp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval, unsigned int optlen);
+int mpdccp_getsockopt(struct sock *sk, int level, int optname, char __user *optval, int __user *optlen);
+
+
+static
+int
+_mpdccp_mk_meta_sk (
+	struct sock	*sk)
+{
+	struct dccp_sock		*dp = dccp_sk(sk);
+
+	if (!sk) return -EINVAL;
+	if (mpdccp_is_meta (sk)) return 0;
+	try_module_get (THIS_MODULE);
+	dp->mpdccp = (struct mpdccp_meta_cb) {
+			.magic = MPDCCP_MAGIC,
+			.is_meta = 1 };
+	dp->mpdccp.mpcb = mpdccp_alloc_mpcb ();
+	if (!dp->mpdccp.mpcb) {
+		module_put (THIS_MODULE);
+		return -ENOBUFS;
+	}
+	sk->sk_prot->setsockopt = mpdccp_setsockopt;
+	sk->sk_prot->getsockopt = mpdccp_getsockopt;
+	dp->mpdccp.mpcb->meta_sk = sk;
+	mpdccp_pr_debug ("meta socket created\n");
+
+	return 0;
+}
+
+static
+int
+_mpdccp_activate (
+	struct sock	*sk,
+	int		on)
+{
+	if (!sk) return -EINVAL;
+
+	if (!mpdccp_enabled) {
+		mpdccp_pr_debug ("mpdccp_enabled: %u, interrupting socket creation", mpdccp_enabled);
+		return -EINVAL;
+	}
+
+	if (on) {
+		mpdccp_pr_debug("activate mpdccp on socket %p\n", sk);
+		dccp_sk(sk)->mpdccp = (struct mpdccp_meta_cb) {
+								.magic = MPDCCP_MAGIC,
+								.mpcb = NULL,
+								.is_meta = 0};
+	} else {
+		mpdccp_pr_debug("deactivate mpdccp on socket %p\n", sk);
+		dccp_sk(sk)->mpdccp = (struct mpdccp_meta_cb) {
+								.magic = 0,
+								.mpcb = NULL,
+								.is_meta = 0};
+	}
+	dccp_sk(sk)->multipath_active = on;
+	return 0;
+}
+
+static
+int
+_mpdccp_isactive (
+	const struct sock	*sk)
+{
+	return dccp_sk(sk)->multipath_active;
+}
+
+
+static
+int
+_mpdccp_xmit_skb (
+	struct sock	*sk,
+	struct sk_buff	*skb)
+{
+	int	ret;
+	long	timeo;
+
+	if (!sk || !skb) return -EINVAL;
+	if (!mpdccp_is_meta (sk)) return -EINVAL;
+	lock_sock(sk);
+	if (dccp_qpolicy_full(sk)) {
+		release_sock(sk);
+		return -EAGAIN;
+	}
+
+	/* Wait for a connection to finish. */
+	timeo = sock_sndtimeo(sk, 1);
+	if ((1 << sk->sk_state) & ~(DCCPF_OPEN | DCCPF_PARTOPEN)) {
+		if ((ret = sk_stream_wait_connect(sk, &timeo)) != 0) {
+			release_sock(sk);
+			return -EAGAIN;
+		}
+	}
+
+	skb_set_owner_w(skb, sk);
+	dccp_qpolicy_push(sk, skb);
+	//if (!timer_pending(&dccp_sk(sk)->dccps_xmit_timer)) {
+		mpdccp_write_xmit(sk);
+	//}
+	release_sock(sk);
+	return 0;
+}
+
+static
+int
+_mpdccp_write_xmit (
+	struct sock	*meta_sk)
+{
+	struct sk_buff		*skb;
+	int			ret2=0, ret;
+
+	if (!mpdccp_is_meta(meta_sk)) return -EINVAL;
+	while ((skb = dccp_qpolicy_top(meta_sk))) {
+		ret = do_mpdccp_write_xmit (meta_sk, skb);
+		if (ret == -EAGAIN) {
+			sk_reset_timer(meta_sk, &dccp_sk(meta_sk)->dccps_xmit_timer,
+				       jiffies + 1);
+			return ret;
+		} else if (ret < 0) {
+			dccp_qpolicy_drop (meta_sk, skb);
+			//mpdccp_pr_debug ("packet drop due to error in xmit: %d", ret);
+			printk ("packet drop due to error in xmit: %d", ret);
+			ret2 = ret;
+		}
+	}
+	return ret2;
+}
+
+static
+int
+do_mpdccp_write_xmit (
+	struct sock	*meta_sk,
+	struct sk_buff	*skb)
+{
+	struct mpdccp_cb	*mpcb;
+	struct sock		*sk;
+	int			ret;
+
+	if (!skb) return -EINVAL;
+	mpcb = MPDCCP_CB(meta_sk);
+	if (!mpcb) return -EINVAL;
+
+	rcu_read_lock ();
+	sk = mpcb->sched_ops->get_subflow(mpcb);
+	DCCP_SKB_CB(skb)->dccpd_mpseq = mpcb->mp_oall_seqno;
+	rcu_read_unlock();
+	if (!sk) {
+		return -EAGAIN;
+	}
+	ret = mpdccp_xmit_to_sk (sk, skb);
+	rcu_read_lock ();
+	if (!strcasecmp (mpcb->sched_ops->name, "redundant")){
+		dccp_inc_seqno(&mpcb->mp_oall_seqno);
+	}
+	rcu_read_unlock();
+	return ret;
+}
+
+static int do_mpdccp_setsockopt(struct sock *sk, int level, int optname,
+		sockptr_t optval, unsigned int optlen)
+{
+	int				err = 0;
+	struct sock			*subsk;
+	struct mpdccp_cb		*mpcb;
+	struct mpdccp_sched_ops		*sched;
+	char				*val;
+	u8					*intval;
+	struct mpdccp_reorder_ops	*reorder;
+
+	lock_sock(sk);
+	mpcb = MPDCCP_CB(sk);
+	mpdccp_pr_debug ("setsockopt: %d\n", optname );
+	if (level == SOL_DCCP) {
+		/* handle multipath socket options */
+		switch (optname) {
+		case DCCP_SOCKOPT_MP_REORDER:
+			val = memdup_sockptr(optval, optlen);
+			if (IS_ERR(val)) {
+				err = PTR_ERR(val);
+				goto out_release;
+			}
+			reorder = mpdccp_reorder_find(val);
+			kfree (val);
+			if(!reorder){
+				err = -ENOENT;
+				mpdccp_pr_debug("Reordering engine not found.\n");
+				goto out_release;
+			}
+			mpcb->reorder_ops = reorder;
+			mpcb->mpdccp_reorder_cb = NULL;
+			mpcb->has_own_reorder = 1;
+			mpcb->reorder_ops->init(mpcb);
+			goto out_release;
+		case DCCP_SOCKOPT_MP_SCHEDULER:
+			val = memdup_sockptr(optval, optlen);
+			if (IS_ERR(val)) {
+				err = PTR_ERR(val);
+				goto out_release;
+			}
+			sched = mpdccp_sched_find(val);
+			kfree (val);
+			if (!sched){
+				err = -ENOENT;
+				mpdccp_pr_debug("Scheduler not found.\n");
+				goto out_release;
+			}
+			mpcb->sched_ops = sched;
+			mpcb->has_own_sched = 1;
+			if (sched->init_conn)
+				sched->init_conn(mpcb);
+			goto out_release;
+		case DCCP_SOCKOPT_MP_FAST_CLOSE:
+			intval = memdup_sockptr(optval, optlen);
+			if (IS_ERR(intval)) {
+				err = PTR_ERR(intval);
+				goto out_release;
+			}
+			mpcb->close_fast = *intval;
+			mpdccp_pr_debug("Toggle fast close %u\n", mpcb->close_fast);
+			goto out_release;
+		}
+	}
+	/* pass to all subflows */
+	mpcb = MPDCCP_CB (sk);
+	mpdccp_for_each_sk (mpcb, subsk) {
+		err = dccp_setsockopt (subsk, level, optname, optval, optlen);
+		if (err) goto out_release;
+	}
+out_release:
+	release_sock (sk);
+	return err;
+}
+
+
+int mpdccp_setsockopt(struct sock *sk, int level, int optname,
+		   sockptr_t optval, unsigned int optlen)
+{
+	int	ret;
+	if(!is_mpdccp(sk) || (is_mpdccp(sk) && !mpdccp_is_meta(sk))){
+		ret = dccp_setsockopt (sk, level, optname, optval, optlen);
+		return ret;
+	}
+
+	if (level != SOL_DCCP) {
+		ret = inet_csk(sk)->icsk_af_ops->setsockopt(sk, level,
+							     optname, optval,
+							     optlen);
+		if (ret) return ret;
+	}
+	return do_mpdccp_setsockopt(sk, level, optname, optval, optlen);
+}
+EXPORT_SYMBOL(mpdccp_setsockopt);
+
+
+static int do_mpdccp_getsockopt(struct sock *sk, int level, int optname,
+		    char __user *optval, int __user *optlen)
+{
+	int			len, err=0;
+	struct mpdccp_cb	*mpcb;
+	struct sock		*subsk;
+	char			*val;
+
+	if (get_user(len, optlen))
+		return -EFAULT;
+
+	if (len < (int)sizeof(int))
+		return -EINVAL;
+
+	mpcb = MPDCCP_CB (sk);
+	if (!mpcb) return -EINVAL;
+	switch (optname) {
+	case DCCP_SOCKOPT_MP_REORDER:
+		val = mpcb->reorder_ops->name;
+		len = strlen (val);
+		if (put_user(len+1, optlen) ||
+				copy_to_user(optval, val, len+1)) {
+			return -EFAULT;
+		}
+		break;
+	case DCCP_SOCKOPT_MP_SCHEDULER:
+		val = mpcb->sched_ops->name;
+		len = strlen (val);
+		if (put_user(len+1, optlen) ||
+				copy_to_user(optval, val, len+1)) {
+			return -EFAULT;
+		}
+		break;
+	default:
+    		mpdccp_for_each_sk (mpcb, subsk) {
+			err = dccp_getsockopt (subsk, level, optname, optval, optlen);
+			/* just get the first subflow */
+			break;
+		}
+	}
+	return err;
+}
+
+int mpdccp_getsockopt(struct sock *sk, int level, int optname,
+		    char __user *optval, int __user *optlen)
+{
+	if (level != SOL_DCCP)
+		return inet_csk(sk)->icsk_af_ops->getsockopt(sk, level,
+							     optname, optval,
+							     optlen);
+	return do_mpdccp_getsockopt(sk, level, optname, optval, optlen);
+}
+EXPORT_SYMBOL(mpdccp_getsockopt);
+
+
+static
+int
+_mpdccp_connect (
+	struct sock		*meta_sk, 
+	const struct sockaddr	*addr,
+	int			addrlen)
+{
+	char			pm_name[MPDCCP_PM_NAME_MAX];
+	struct mpdccp_pm_ops	*pm;
+	struct mpdccp_cb	*mpcb;
+	int			ret;
+
+	ret = _mpdccp_mk_meta_sk(meta_sk);
+	if (ret < 0) {
+		mpdccp_pr_debug ("error creating meta\n");
+		return ret;
+	}
+
+	if (!mpdccp_is_meta(meta_sk)) return -EINVAL;
+	mpcb = MPDCCP_CB(meta_sk);
+	if (!mpcb) return -EINVAL;
+
+	mpcb->role = MPDCCP_CLIENT;
+
+	/* Save the meta socket namespace info */
+	write_pnet(&mpcb->net, sock_net(meta_sk));
+
+	mpcb->glob_lfor_seqno = GLOB_SEQNO_INIT;
+	mpcb->mp_oall_seqno = GLOB_SEQNO_INIT;
+	mpcb->mpdccp_loc_cix = mpdccp_link_generate_cid();
+	
+	mpdccp_get_default_path_manager(pm_name);
+	pm = mpdccp_pm_find(pm_name);
+	if(!pm){
+		mpdccp_pr_debug("Path manager not found.");
+		return -ENOENT;
+	}
+	ret = pm->add_init_client_conn (mpcb, (struct sockaddr*)addr, addrlen);
+	if (ret < 0) {
+		mpdccp_pr_debug("Failed to set up MPDCCP Client mpcb: %d\n", ret);
+		return ret;
+	}
+	
+	return 0;
+}
+
+static
+int
+_mpdccp_destroy_sock (
+	struct sock	*sk)
+{
+	struct mpdccp_cb	*mpcb;
+
+	if (!mpdccp_is_meta(sk)) return -EINVAL;
+	mpcb = MPDCCP_CB(sk);
+	if (mpcb) mpdccp_destroy_mpcb (mpcb);
+	unset_mpdccp(sk);
+	module_put (THIS_MODULE);
+	return 0;
+}
+
+static int _mpdccp_conn_request(struct sock *sk, struct dccp_request_sock *dreq)
+{
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct dccp_options_received *opt_recv = &dp->dccps_options_received;
+
+	u8 msg[8];
+	mpdccp_pr_debug("enter for sk %p saw_mpkey %d dmeta %p", sk, opt_recv->saw_mpkey, dreq->meta_sk);
+
+	if (opt_recv->saw_mpkey) {
+		/* MP_KEY was in the options: we are in the key exchange phase */
+		int key_type;
+		dreq->meta_sk = NULL;
+
+		/* TODO: replace with proper crypto select algo */
+		key_type = ffs(opt_recv->dccpor_mp_suppkeys & MPDCCP_SUPPKEYS) - 1;
+		if (key_type < 0) {
+			mpdccp_pr_debug("no key support type match srv: %x cli: %x", MPDCCP_SUPPKEYS, opt_recv->dccpor_mp_suppkeys);
+			return -1;
+		}
+
+		/* Generate local key for handshake */
+		/* TODO: check for collisions in existing MPCBs */
+		if (mpdccp_generate_key(&dreq->mpdccp_loc_key, key_type)) {
+			mpdccp_pr_debug("error generating key of type %d", key_type);
+			return -1;
+		}
+
+		if ((opt_recv->dccpor_mp_keys[0].size == 0) || (opt_recv->dccpor_mp_keys[0].size > MPDCCP_MAX_KEY_SIZE)) {
+			mpdccp_pr_debug("received key has invalid length");
+			return -1;
+		}
+		/* Get the client key */
+		dreq->mpdccp_rem_key.type = opt_recv->dccpor_mp_keys[0].type;
+		dreq->mpdccp_rem_key.size = opt_recv->dccpor_mp_keys[0].size;
+		memcpy(dreq->mpdccp_rem_key.value, opt_recv->dccpor_mp_keys[0].value,
+			   dreq->mpdccp_rem_key.size);
+
+		dreq->mpdccp_loc_cix = mpdccp_link_generate_cid();
+		dreq->mpdccp_rem_cix = opt_recv->dccpor_mp_cix;
+	} else if (opt_recv->saw_mpjoin) {
+		/* No MP_KEY: this is a join */
+		struct sock *meta_sk = NULL;
+		struct mpdccp_cb *mpcb;
+		int ret;
+
+		if ((opt_recv->dccpor_mp_cix == 0) || (opt_recv->dccpor_mp_nonce == 0)) {
+			mpdccp_pr_debug("invalid CID or nonce received");
+			return -1;
+		}
+		dreq->mpdccp_rem_nonce = opt_recv->dccpor_mp_nonce;
+
+		/* Lookup the token in existing MPCBs */
+		mpdccp_for_each_conn(pconnection_list, mpcb) {
+			if (mpcb->mpdccp_loc_cix == opt_recv->dccpor_mp_cix) {
+				meta_sk = mpcb->meta_sk;
+				mpdccp_pr_debug("found CID in mpcb %p\n", mpcb);
+				break;
+			}
+		}
+
+		if (meta_sk == NULL) {
+			mpdccp_pr_debug("no CID found for join");
+			return -1;
+		}
+
+		mpcb = MPDCCP_CB(meta_sk);
+
+		/* Hold a reference to the meta socket (will be released in req destructor) */
+		sock_hold(meta_sk);
+		dreq->meta_sk = meta_sk;
+
+		/* Check the MP version is the same as the first subflow */
+		if (dccp_sk(meta_sk)->multipath_ver != dreq->multipath_ver) {
+			mpdccp_pr_debug("MP version mismatch in JOIN expected: %d got:%d", dccp_sk(meta_sk)->multipath_ver, dreq->multipath_ver);
+			return -1;
+		}
+
+		/* Generate local nonce */
+		get_random_bytes(&dreq->mpdccp_loc_nonce, 4);
+		mpdccp_pr_debug("generated nonce %x", dreq->mpdccp_loc_nonce);
+
+		/* Calculate HMAC */
+		put_unaligned_be32(mpcb->mpdccp_loc_cix, &msg[0]);
+		put_unaligned_be32(dreq->mpdccp_rem_nonce, &msg[4]);
+		ret = mpdccp_hmac_sha256(mpcb->dkeyB, mpcb->dkeylen, msg, 8, dreq->mpdccp_loc_hmac);
+		if (ret) {
+			mpdccp_pr_debug("error calculating HMAC, err %d", ret);
+			return -1;
+		}
+		mpdccp_pr_debug("calculated HMAC %llx", be64_to_cpu(*((u64 *)dreq->mpdccp_loc_hmac)));
+	} else {
+		mpdccp_pr_debug("no MP_KEY or MP_JOIN in DCCP REQUEST");
+	}
+
+	return 0;
+}
+
+static int _mpdccp_rcv_request_sent_state_process(struct sock *sk, const struct sk_buff *skb)
+{
+	struct mpdccp_cb *mpcb;
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct dccp_options_received *opt_recv = &dp->dccps_options_received;
+
+	mpdccp_pr_debug("enter for sk %p is_meta %d", sk, mpdccp_is_meta(sk));
+
+	if (dccp_sk(sk)->is_kex_sk) {
+		/* We are in key exchange phase: process data from MP_KEY option */
+		int i;
+		mpcb = MPDCCP_CB(sk);
+		/* Fallback to single path if mp cannot be established */
+		if (!opt_recv->saw_mpkey || (dccp_sk(sk)->multipath_ver == MPDCCP_VERS_UNDEFINED)) {
+			mpdccp_pr_debug("no MP_KEY in response, fallback to single DCCP");
+			if (mpcb) {
+				mpdccp_pr_debug("no MP_KEY in response or invalid MP version, fallback to single path DCCP\n");
+				mpcb->fallback_sp = 1;
+				return 0;
+			} else {
+				mpdccp_pr_debug("invalid MPCB\n");
+				return -1;
+			}
+		}
+
+		if ((opt_recv->dccpor_mp_keys[0].size == 0) || (opt_recv->dccpor_mp_keys[0].size > MPDCCP_MAX_KEY_SIZE)) {
+			mpdccp_pr_debug("rx key(s) have invalid length %d", opt_recv->dccpor_mp_keys[0].size);
+			return -1;
+		}
+
+		/* Pick the local key with the same type as the remote */
+		for (i=0; i < MPDCCP_MAX_KEYS; i++) {
+			if (mpcb->mpdccp_loc_keys[i].type == opt_recv->dccpor_mp_keys[0].type) {
+				mpcb->cur_key_idx = i;
+				mpdccp_pr_debug("found local matching key idx %i type %d\n", i, mpcb->mpdccp_loc_keys[i].type);
+				break;
+			}
+		}
+		if (i == MPDCCP_MAX_KEYS) {
+			mpdccp_pr_debug("no key type match srv: %x cli: %x", opt_recv->dccpor_mp_suppkeys, MPDCCP_SUPPKEYS);
+			return -1;
+		}
+
+		/* Store the remote key */
+		mpcb->mpdccp_rem_key.type = opt_recv->dccpor_mp_keys[0].type;
+		mpcb->mpdccp_rem_key.size = opt_recv->dccpor_mp_keys[0].size;
+		memcpy(mpcb->mpdccp_rem_key.value, opt_recv->dccpor_mp_keys[0].value, mpcb->mpdccp_rem_key.size);
+
+		mpcb->mpdccp_rem_cix = opt_recv->dccpor_mp_cix;
+		/* Created derived key(s) */
+		if (mpcb->mpdccp_loc_keys[i].type == DCCPK_PLAIN) {
+			memcpy(&mpcb->dkeyA[0], mpcb->mpdccp_loc_keys[i].value, MPDCCP_PLAIN_KEY_SIZE);
+			memcpy(&mpcb->dkeyA[MPDCCP_PLAIN_KEY_SIZE], mpcb->mpdccp_rem_key.value, MPDCCP_PLAIN_KEY_SIZE);
+			memcpy(&mpcb->dkeyB[0], mpcb->mpdccp_rem_key.value, MPDCCP_PLAIN_KEY_SIZE);
+			memcpy(&mpcb->dkeyB[MPDCCP_PLAIN_KEY_SIZE], mpcb->mpdccp_loc_keys[i].value, MPDCCP_PLAIN_KEY_SIZE);
+			mpcb->dkeylen = MPDCCP_PLAIN_KEY_SIZE * 2;
+		} else {
+			/* TODO */
+			mpdccp_pr_debug("unsupported key type %d", mpcb->mpdccp_loc_keys[i].type);
+			return -1;
+		}
+
+		/* On client side the key exchange is done */
+		mpcb->kex_done = 1;
+	} else {
+		/* We are in authentication phase: process data from MP_JOIN option */
+		u8 hash_mac[MPDCCP_HMAC_SIZE];
+		u8 msg[8];
+		int ret;
+
+		mpcb = MPDCCP_CB(sk);
+
+		if (!opt_recv->saw_mpjoin || mpcb->fallback_sp || (dccp_sk(sk)->multipath_ver == MPDCCP_VERS_UNDEFINED)) {
+			mpdccp_pr_debug("no MP_JOIN in response, invalid MP version or using single path DCCP fallback\n");
+			return -1;
+		}
+
+		if ((opt_recv->dccpor_mp_cix == 0) || (opt_recv->dccpor_mp_nonce == 0)) {
+			mpdccp_pr_debug("invalid CID or nonce received");
+			return -1;
+		}
+		dccp_sk(sk)->mpdccp_rem_nonce = opt_recv->dccpor_mp_nonce;
+
+		if (opt_recv->dccpor_mp_cix != mpcb->mpdccp_loc_cix){
+			mpdccp_pr_debug("error unknown CID");
+			return -1;
+		}
+
+		/* Validate HMAC from srv */
+		memcpy(dccp_sk(sk)->mpdccp_rem_hmac, opt_recv->dccpor_mp_hmac, MPDCCP_HMAC_SIZE);
+		put_unaligned_be32(mpcb->mpdccp_rem_cix, &msg[0]);
+		put_unaligned_be32(dccp_sk(sk)->mpdccp_loc_nonce, &msg[4]);
+		ret = mpdccp_hmac_sha256(mpcb->dkeyB, mpcb->dkeylen, msg, 8, hash_mac);
+		if (ret) {
+			mpdccp_pr_debug("error calculating HMAC, err %d", ret);
+			return -1;
+		}
+		mpdccp_pr_debug("calculated HMAC(B) %llx", be64_to_cpu(*((u64 *)hash_mac)));
+
+		if (memcmp(dccp_sk(sk)->mpdccp_rem_hmac, hash_mac, MPDCCP_HMAC_SIZE)) {
+			mpdccp_pr_debug("HMAC validation failed! rx: %llx exp: %llx",
+							be64_to_cpu(*(u64 *)dccp_sk(sk)->mpdccp_rem_hmac),
+							be64_to_cpu(*((u64 *)hash_mac)));
+			return -1;
+		}
+		mpdccp_pr_debug("HMAC validation OK");
+
+		/* Now calculate the HMAC from the received JOIN */
+		put_unaligned_be32(mpcb->mpdccp_rem_cix, &msg[0]);
+		put_unaligned_be32(dccp_sk(sk)->mpdccp_rem_nonce, &msg[4]);
+		ret = mpdccp_hmac_sha256(mpcb->dkeyA, mpcb->dkeylen, msg, 8, dccp_sk(sk)->mpdccp_loc_hmac);
+		if (ret) {
+			mpdccp_pr_debug("error calculating HMAC, err %d", ret);
+			return -1;
+		}
+		mpdccp_pr_debug("calculated HMAC(A) %llx", be64_to_cpu(*((u64 *)dccp_sk(sk)->mpdccp_loc_hmac)));
+	}
+	return 0;
+}
+
+static int _mpdccp_rcv_established(struct sock *sk)
+{
+	struct mpdccp_cb *mpcb = MPDCCP_CB(sk);
+
+	/* Check if the socket has been authenticated */
+	if(dccp_sk(sk)->auth_done || (mpcb && mpcb->fallback_sp)) {
+			return 0;
+	}
+	mpdccp_pr_debug("sk %p NOT authenticated \n", sk);
+	return -1;
+}
+
+static int _mpdccp_rcv_respond_partopen_state_process(struct sock *sk, int type)
+{
+	struct mpdccp_cb *mpcb = MPDCCP_CB(sk);
+	mpdccp_pr_debug("enter for sk %p role %s is_meta %d is_kex %d type %d", sk, dccp_role(sk), mpdccp_is_meta(sk), dccp_sk(sk)->is_kex_sk, type);
+
+	if (mpcb && !mpcb->fallback_sp && (type == DCCP_PKT_ACK || type == DCCP_PKT_DATAACK)) {
+		if (dccp_sk(sk)->is_kex_sk && !mpcb->kex_done) {
+			int key_idx = mpcb->cur_key_idx;
+			mpdccp_pr_debug("key exchange done for mpcb %p\n", mpcb); 
+			mpcb->kex_done = 1;
+
+			/* Created derived key(s) */
+			if (mpcb->mpdccp_loc_keys[key_idx].type == DCCPK_PLAIN) {
+				memcpy(&mpcb->dkeyA[0], mpcb->mpdccp_rem_key.value, MPDCCP_PLAIN_KEY_SIZE);
+				memcpy(&mpcb->dkeyA[MPDCCP_PLAIN_KEY_SIZE], mpcb->mpdccp_loc_keys[key_idx].value, MPDCCP_PLAIN_KEY_SIZE);
+				memcpy(&mpcb->dkeyB[0], mpcb->mpdccp_loc_keys[key_idx].value, MPDCCP_PLAIN_KEY_SIZE);
+				memcpy(&mpcb->dkeyB[MPDCCP_PLAIN_KEY_SIZE], mpcb->mpdccp_rem_key.value, MPDCCP_PLAIN_KEY_SIZE);
+				mpcb->dkeylen = MPDCCP_PLAIN_KEY_SIZE * 2;
+			} else {
+				/* TODO */
+				mpdccp_pr_debug("unsupported key type %d", mpcb->mpdccp_loc_keys[key_idx].type);
+				return -1;
+			}
+			/* No longer need to include the MP_KEY in the options */
+			dccp_sk(sk)->is_kex_sk = 0;
+		}
+
+		if (dccp_sk(sk)->dccps_role == DCCP_ROLE_CLIENT) {
+			/* Stop the ACK retry timer */
+			inet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);
+			WARN_ON(sk->sk_send_head == NULL);
+			kfree_skb(sk->sk_send_head);
+			sk->sk_send_head = NULL;
+
+			if(mpdccp_get_prio(sk) != 3)				// dont announce if prio = 3 (default value)
+				mpdccp_init_announce_prio(sk);			// announce prio values for all subflows after creation
+		}
+
+		/* Authentication complete, send an additional ACK if required */
+		dccp_sk(sk)->auth_done = 1;
+		if (dccp_sk(sk)->dccps_role == DCCP_ROLE_SERVER) {
+			struct mpdccp_link_info *link = mpdccp_ctrl_getlink (sk);
+
+			mpdccp_pr_debug("send ACK");
+			dccp_send_ack(sk);
+
+			/* we have a virtual link (created by a script or mpdccplink add_link) */
+			if(link && !link->is_devlink && link->mpdccp_prio != 3)
+				mpdccp_init_announce_prio(sk);			// (server) announce prio only for non dev links
+			else if(link && link->is_devlink)
+				mpdccp_link_cpy_set_prio(sk, 3);
+			
+			mpdccp_link_put (link);
+		}
+	}
+
+	/* Open the meta socket if necessary */
+	if ((sk->sk_state == DCCP_OPEN) && (mpcb && mpcb->meta_sk->sk_state == DCCP_RESPOND)) {
+		mpdccp_pr_debug("opening meta %p\n", mpcb->meta_sk);
+		mpcb->meta_sk->sk_state = DCCP_OPEN;
+	}
+
+	return 0;
+}
+
+static int
+create_subflow(
+	struct sock *sk,
+	struct sock *meta_sk,
+	struct sk_buff *skb,
+	struct request_sock *req,
+	int clone,
+	u8 loc_addr_id,
+	u8 rem_addr_id)
+{
+	int ret;
+	struct sock *newsk;
+	struct mpdccp_cb *mpcb = MPDCCP_CB(meta_sk);
+	struct mpdccp_link_info *link_info = NULL;
+	struct dccp_request_sock *dreq = dccp_rsk(req);
+
+	mpdccp_pr_debug("enter sk %p meta %p req %p clone %d loc_id %u rem_id %u\n", sk, meta_sk, req, clone, loc_addr_id, rem_addr_id);
+	if (clone) {
+		bool own;
+		/* Use the dccp request flow to create a clone of the meta socket */
+		newsk = dccp_v4_request_recv_sock(sk, skb, req, NULL, inet_reqsk(meta_sk), &own);
+		if (!newsk) {
+			mpdccp_pr_debug("error calling dccp_v4_request_recv_sock sk %p meta %p\n", sk, meta_sk);
+			goto err;
+		}
+		/* Activate the features on the new socket as in the request */
+		if (dccp_feat_activate_values(newsk, &dreq->dreq_featneg_mp)) {
+			mpdccp_pr_debug("error calling dccp_feat_activate_values for sk %p\n", newsk);
+			inet_csk_prepare_forced_close(newsk);
+			dccp_done(newsk);
+			goto err;
+		}
+		mpdccp_pr_debug("cloned socket sk %p meta %p newsk %p\n", sk, meta_sk, newsk);
+	} else {
+		/* For a join we don't need to clone the socket */
+		newsk = sk;
+		mpdccp_pr_debug("not cloned socket sk %p meta %p newsk %p\n", sk, meta_sk, newsk);
+	}
+
+	if (dreq->link_info) {
+		link_info = dreq->link_info;
+		mpdccp_link_get(link_info);
+	} else {
+		link_info = mpdccp_getfallbacklink(&init_net);
+	}
+
+	set_mpdccp(newsk, mpcb);
+
+	mpdccp_pr_debug("mysock new: %p\n", mpdccp_my_sock(newsk));
+
+	ret = my_sock_init(newsk, mpcb, sk->sk_bound_dev_if, MPDCCP_SERVER_SUBFLOW);
+	if (ret < 0) {
+		mpdccp_pr_debug("my_sock_init for sk %p failed with exit code %d.\n", newsk, ret);
+		if (clone) {
+			inet_csk_prepare_forced_close(newsk);
+			dccp_done(newsk);
+		}
+		goto err;
+	}
+
+	mpdccp_my_sock(newsk)->link_info = link_info;
+	mpdccp_my_sock(newsk)->link_cnt = mpdccp_link_cnt(link_info);
+	mpdccp_my_sock(newsk)->link_iscpy = 0;
+    mpdccp_my_sock(newsk)->local_addr_id = loc_addr_id;
+    mpdccp_my_sock(newsk)->remote_addr_id = rem_addr_id;
+
+	spin_lock(&mpcb->psubflow_list_lock);
+	list_add_tail_rcu(&mpdccp_my_sock(newsk)->sk_list, &mpcb->psubflow_list);
+	mpdccp_pr_debug("Added new entry to psubflow_list @ %p\n", mpdccp_my_sock(newsk));
+	mpcb->cnt_subflows = (mpcb->cnt_subflows) + 1;
+	spin_unlock(&mpcb->psubflow_list_lock);
+
+	if (mpcb->sched_ops->init_subflow) {
+		rcu_read_lock ();
+		mpcb->sched_ops->init_subflow(newsk);
+		rcu_read_unlock ();
+	}
+
+	mpdccp_pr_debug("Connection accepted. There are %d subflows now newsk. %p\n",
+					mpcb->cnt_subflows, newsk);
+
+	if (clone) {
+		bh_unlock_sock(meta_sk);
+	}
+	mpcb->master_sk = newsk;
+	return 0;
+err:
+	return -ENOBUFS;
+}
+
+static int
+_mpdccp_create_master(
+	struct sock *sk,
+	struct sock *child,
+	struct request_sock *req,
+	struct sk_buff *skb)
+{
+	int ret;
+	struct sock *meta_sk;
+	struct mpdccp_cb *mpcb;
+	struct sockaddr_in sin;
+	struct inet_sock *inet = inet_sk(child);
+	struct dccp_request_sock *dreq = dccp_rsk(req);
+	union inet_addr addr;
+
+	mpdccp_pr_debug("enter for sk %p child %p dreq %p\n", sk, child, dreq);
+
+	/* Allocate mpcb and meta socket data */
+	ret = _mpdccp_mk_meta_sk(child);
+	if (ret < 0) {
+		mpdccp_pr_debug("error creating meta for sk %p\n", child);
+		goto err_meta;
+	}
+
+	/* Populate mpcb */
+	meta_sk = child;
+	mpcb = MPDCCP_CB(meta_sk);
+	sin.sin_family = AF_INET;
+	sin.sin_port = inet->inet_sport;
+	sin.sin_addr.s_addr = inet->inet_saddr;
+	memcpy(&mpcb->mpdccp_local_addr, &sin, sizeof(struct sockaddr_in));
+	mpcb->localaddr_len = sizeof(struct sockaddr_in);
+	mpcb->has_localaddr = 1;
+	mpcb->mpdccp_loc_keys[0] = dreq->mpdccp_loc_key;
+	mpcb->mpdccp_loc_cix = dreq->mpdccp_loc_cix;
+	mpcb->mpdccp_rem_cix = dreq->mpdccp_rem_cix;
+	mpcb->cur_key_idx = 0;
+	mpcb->mpdccp_rem_key = dreq->mpdccp_rem_key;
+	mpcb->role = MPDCCP_SERVER;
+	mpcb->master_addr_id = 0;
+
+	//prevent cid entry deletion triggered by freeing dreq  
+	dreq->mpdccp_loc_cix = 0;
+
+	addr.ip = inet->inet_saddr;
+	if(mpcb->pm_ops->claim_local_addr)
+		mpcb->master_addr_id = mpcb->pm_ops->claim_local_addr(mpcb, AF_INET, &addr);
+
+	mpdccp_pr_debug("master subflow id: %u\n", mpcb->master_addr_id);
+
+	addr.ip = inet->inet_daddr;
+	if(mpcb->pm_ops->add_addr)
+		mpcb->pm_ops->add_addr(mpcb, AF_INET, 0, &addr, inet->inet_dport, true);
+
+	/* Create subflow and meta sockets */
+	ret = create_subflow(sk, meta_sk, skb, req, 1, mpcb->master_addr_id, 0);
+	if (ret < 0) {
+		mpdccp_pr_debug("error creating subflow %d\n", ret);
+		goto err_sub;
+	}
+
+	return 0;
+err_sub:
+	mpdccp_destroy_mpcb(mpcb);
+err_meta:
+	return ret;
+}
+
+
+/* We get here on the server side at final stage of the handshake to validate the client request */
+static int _mpdccp_check_req(struct sock *sk, struct sock *newsk, struct request_sock *req, struct sk_buff *skb, struct sock **master_sk)
+{
+	struct mpdccp_cb *mpcb;
+	struct dccp_sock *dp = dccp_sk(newsk);
+	struct dccp_options_received *opt_recv = &dp->dccps_options_received;
+	struct dccp_request_sock *dreq = dccp_rsk(req);
+	int ret;
+
+	mpdccp_pr_debug("enter sk %p newsk %p dmeta %p", sk, newsk, dreq->meta_sk);
+
+	if (dreq && !dreq->meta_sk && !mpdccp_is_meta(newsk)) {
+		/* This is a new session, need to create MPCB and meta */
+		int dkeylen;
+		u8 dkeyA[MPDCCP_MAX_KEY_SIZE * 2];
+		u8 dkeyB[MPDCCP_MAX_KEY_SIZE * 2];
+
+		/* Fallback to single path if mp cannot be established */
+		if (dreq->multipath_ver == MPDCCP_VERS_UNDEFINED) {
+			mpdccp_pr_debug("failed MP negotiation with client, fallback to single path DCCP\n");
+			mpdccp_activate (newsk, 0);
+			*master_sk = inet_csk_complete_hashdance(sk, newsk, req, true);
+			return 0;
+		}
+
+		dccp_sk(newsk)->multipath_ver = dreq->multipath_ver;
+
+
+		mpdccp_pr_debug("key exchange done, creating meta socket");
+		dccp_sk(newsk)->is_kex_sk = 0;
+
+		/* Created derived key(s) */
+		if (dreq->mpdccp_loc_key.type == DCCPK_PLAIN) {
+			memcpy(&dkeyA[0], dreq->mpdccp_rem_key.value, MPDCCP_PLAIN_KEY_SIZE);
+			memcpy(&dkeyA[MPDCCP_PLAIN_KEY_SIZE], dreq->mpdccp_loc_key.value, MPDCCP_PLAIN_KEY_SIZE);
+			memcpy(&dkeyB[0], dreq->mpdccp_loc_key.value, MPDCCP_PLAIN_KEY_SIZE);
+			memcpy(&dkeyB[MPDCCP_PLAIN_KEY_SIZE], dreq->mpdccp_rem_key.value, MPDCCP_PLAIN_KEY_SIZE);
+			dkeylen = MPDCCP_PLAIN_KEY_SIZE * 2;
+		} else {
+			/* TODO */
+			mpdccp_pr_debug("unsupported key type %d", dreq->mpdccp_loc_key.type);
+			return -1;
+		}
+
+
+		/* Now create the MPCB, meta & c */
+		ret = mpdccp_create_master(sk, newsk, req, skb);
+		if (ret) {
+			mpdccp_pr_debug("error mpdccp_create_master %d", ret);
+			return -1;
+		}
+		*master_sk = MPDCCP_CB(newsk)->master_sk;
+
+		/* Finally complete the request handling */
+		inet_csk_complete_hashdance(sk, newsk, req, true);
+	} else {
+		/* This is a new subflow socket */
+		u8 hash_mac[20];
+		u8 msg[8];
+		int loc_id = 0, rem_id = 0;
+		union inet_addr addr;
+
+		struct sock *meta_sk = dreq->meta_sk;
+		if (!meta_sk) {
+			mpdccp_pr_debug("%s meta_sk is null\n",__func__);
+			return -1;
+		}
+
+		/* Validate the HMAC from the client */
+		mpcb = MPDCCP_CB(meta_sk);
+		if (!mpcb) {
+			mpdccp_pr_debug("%s mpcb is null\n",__func__);
+			return -1;
+		}
+		memcpy(dreq->mpdccp_rem_hmac, opt_recv->dccpor_mp_hmac, MPDCCP_HMAC_SIZE);
+		put_unaligned_be32(mpcb->mpdccp_loc_cix, &msg[0]);
+		put_unaligned_be32(dreq->mpdccp_loc_nonce, &msg[4]);
+		ret = mpdccp_hmac_sha256(mpcb->dkeyA, mpcb->dkeylen, msg, 8, hash_mac);
+		if (ret) {
+			mpdccp_pr_debug("error calculating HMAC, err %d", ret);
+			return -1;
+		}
+
+		if (memcmp(dreq->mpdccp_rem_hmac, hash_mac, MPDCCP_HMAC_SIZE)) {
+					mpdccp_pr_debug("HMAC validation failed! rx: %llx exp: %llx\n",
+									be64_to_cpu(*(u64 *)dreq->mpdccp_rem_hmac),
+									be64_to_cpu(*((u64 *)hash_mac)));
+			return -1;
+		}
+		mpdccp_pr_debug("HMAC validation OK");
+
+		if(mpcb->pm_ops->get_id_from_ip){
+			addr.ip = ip_hdr(skb)->daddr;
+			loc_id = mpcb->pm_ops->get_id_from_ip(mpcb, &addr, AF_INET, false);
+			addr.ip = ip_hdr(skb)->saddr;
+			rem_id = mpcb->pm_ops->get_id_from_ip(mpcb, &addr, AF_INET, true);
+
+			if(loc_id < 0 || rem_id < 0){
+				mpdccp_pr_debug("cant create subflow with unknown address id");
+				return -1;
+			}
+		}
+		/* Now add the subflow to the mpcb */
+
+		ret = create_subflow(newsk, meta_sk, skb, req, 0, (u8)loc_id, (u8)rem_id);
+		if (ret) {
+			mpdccp_pr_debug("error mpdccp_create_master_sub %d", ret);
+			return -1;
+		}
+		*master_sk = mpcb->master_sk;
+
+		/* Drop the request since this is not following the accept() flow */
+		inet_csk_reqsk_queue_drop(sk, req);
+		reqsk_queue_removed(&inet_csk(sk)->icsk_accept_queue, req);
+	}
+	return 0;
+}
+
+static int _mpdccp_close_meta(struct sock *meta_sk)
+{
+	struct mpdccp_cb	*mpcb = MPDCCP_CB(meta_sk);
+	struct sock	*sk;
+	int ret = 0;
+	struct list_head *pos, *temp;
+	struct my_sock *mysk;
+
+	if (!mpcb) return -EINVAL;
+	if(mpcb->to_be_closed) return 0;
+
+	mpcb->to_be_closed = 1;
+	mpdccp_pr_debug ("enter for sk %p\n", meta_sk);
+	/* close all subflows */
+	list_for_each_safe(pos, temp, &((mpcb)->psubflow_list)) {
+		mysk = list_entry(pos, struct my_sock, sk_list);
+		if (mysk) {
+			sk = mysk->my_sk_sock;
+			mpdccp_pr_debug ("closing subflow %p\n", sk);
+			ret = mpdccp_close_subflow(mpcb, sk, mpcb->close_fast+1);
+			if (ret < 0) {
+				mpdccp_pr_debug ("error closing subflow: %d\n", ret);
+				break;
+			}
+		}
+	}
+
+	meta_sk->sk_prot->setsockopt = dccp_setsockopt;
+	meta_sk->sk_prot->getsockopt = dccp_getsockopt;
+
+	if(mpcb->pm_ops->del_addr)
+		mpcb->pm_ops->del_addr(mpcb, 0, 0, true);
+
+	return ret;
+}
+
+static int _mpdccp_try(struct sock *sk)
+{
+	if(!is_mpdccp(sk) && mpdccp_enabled == 1){
+		mpdccp_pr_debug ("mpdcc_enabled: 1, upgrading to mpdccp (%p)", sk);
+		_mpdccp_activate(sk,1);
+		return 1;
+	}
+	return 0;
+}
+
+int
+mpdccp_init_funcs (void)
+{
+	mpdccp_pr_debug ("initailize mpdccp functions\n");
+	if (mpdccp_funcs.magic == MPDCCP_MAGIC) return 0;
+	mpdccp_funcs = (struct mpdccp_funcs) {
+		.magic = MPDCCP_MAGIC,
+		.destroy_sock = _mpdccp_destroy_sock,
+		.mk_meta_sk = _mpdccp_mk_meta_sk,
+		.connect = _mpdccp_connect,
+		.write_xmit = _mpdccp_write_xmit,
+		.xmit_skb = _mpdccp_xmit_skb,
+		.activate = _mpdccp_activate,
+		.isactive = _mpdccp_isactive,
+		.conn_request = _mpdccp_conn_request,
+		.rcv_request_sent_state_process = _mpdccp_rcv_request_sent_state_process,
+		.rcv_respond_partopen_state_process = _mpdccp_rcv_respond_partopen_state_process,
+		.rcv_established = _mpdccp_rcv_established,
+		.check_req = _mpdccp_check_req,
+		.create_master = _mpdccp_create_master,
+		.close_meta = _mpdccp_close_meta,
+		.try_mpdccp = _mpdccp_try,
+	};
+	mpdccp_pr_debug ("mpdccp functions initialized (.magic=%x)\n",
+				mpdccp_funcs.magic);
+	return 0;
+}
+
+int
+mpdccp_deinit_funcs (void)
+{
+	mpdccp_pr_debug ("de-initialize mpdccp functions\n");
+	mpdccp_funcs = (struct mpdccp_funcs) { .magic = 0, };
+	return 0;
+}
+
diff --git a/net/dccp/mpdccp_proto.h b/net/dccp/mpdccp_proto.h
new file mode 100644
index 0000000000000..efe928c466c1e
--- /dev/null
+++ b/net/dccp/mpdccp_proto.h
@@ -0,0 +1,57 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ * 
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ *
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#ifndef _MPDCCP_META_SK_H
+#define _MPDCCP_META_SK_H
+
+
+
+#define MPDCCP_META_SK_MAGIC	0xc34fb2e1
+
+
+struct mpdccp_cb;
+
+struct mpdccp_meta_sk {
+	u32			magic;
+	struct mpdccp_cb	*mpcb;
+};
+
+#define mpdccp_is_meta(sk) \
+	((sk) && ((sk)->sk_user_data) && \
+		(((struct mpdccp_meta_sk*)((sk)->sk_user_data))->magic \
+			== MPDCCP_META_SK_MAGIC))
+
+#define MPDCCP_CB(sk) \
+	mpdccp_is_meta(sk) ? ((struct mpdccp_meta_sk*)(sk)->sk_user_data)->mpcb : NULL;
+
+
+
+
+
+
+
+
+
+
+#endif	/* _MPDCCP_META_SK_H */
+
diff --git a/net/dccp/mpdccp_reordering.c b/net/dccp/mpdccp_reordering.c
new file mode 100644
index 0000000000000..986eab3462e32
--- /dev/null
+++ b/net/dccp/mpdccp_reordering.c
@@ -0,0 +1,632 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ * 
+ * Copyright (C) 2018 by Maximilian Schuengel, Deutsche Telekom AG
+ * Copyright (C) 2018 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2020 by Nathalie Romo, Deutsche Telekom AG
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ *
+ * MPDCCP - Generic reordering functions.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/module.h>
+#include <linux/hrtimer.h>
+#include <linux/time.h>
+#include <linux/ktime.h>
+
+#include "ccids/ccid2.h"
+#include "mpdccp.h"
+#include "mpdccp_reordering.h"
+
+
+/* debugging */
+bool ro_err_state = 1;
+EXPORT_SYMBOL(ro_err_state);
+bool ro_info_state = 1;
+EXPORT_SYMBOL(ro_info_state);
+bool ro_warn_state = 1;
+EXPORT_SYMBOL(ro_warn_state);
+int ro_dbug_state = 0;
+EXPORT_SYMBOL(ro_dbug_state); 
+
+module_param(ro_err_state, bool, 0644);
+MODULE_PARM_DESC(ro_err_state, "Enable debug messages for reordering, ERROR-Level");
+module_param(ro_info_state, bool, 0644);
+MODULE_PARM_DESC(ro_info_state, "Enable debug messages for reordering, INFO-Level");
+module_param(ro_warn_state, bool, 0644);
+MODULE_PARM_DESC(ro_warn_state, "Enable debug messages for reordering, WARNING-Level");
+module_param(ro_dbug_state, int, 0644);
+MODULE_PARM_DESC(ro_dbug_state, "Enable debug messages for reordering, DEBUG-Level 1, 2 or 3 (3 most detailed)");
+
+static DEFINE_SPINLOCK(mpdccp_reorder_list_lock);
+static LIST_HEAD(mpdccp_reorder_list);
+static struct mpdccp_reorder_ops	*reorder_default = NULL;
+static struct mpdccp_reorder_ops	*reorder_fallback = NULL;
+
+static struct kmem_cache *mpdccp_reorder_path_cb_cache = NULL;
+static struct kmem_cache *mpdccp_reorder_rcv_buff_cache = NULL;
+
+
+static int mpdccp_reset_path_cb(struct mpdccp_reorder_path_cb *pcb);
+static void mpdccp_mrtt_estimator(struct mpdccp_reorder_path_cb *pcb, const long mrtt);
+static void mpdccp_krtt_estimator(struct mpdccp_reorder_path_cb *pcb, const long mrtt);
+static void mpdccp_drtt_estimator(struct mpdccp_reorder_path_cb *pcb, const long mrtt);
+static u32 mean(u32 *arr, int size);
+static void prepend(u32 *arr, int size, u32 val);
+static u32 get_mrtt(struct mpdccp_reorder_path_cb *pcb);
+static u32 get_krtt(struct mpdccp_reorder_path_cb *pcb);
+static u32 get_drtt(struct mpdccp_reorder_path_cb *pcb);
+static int mpdccp_allocate_rcv_buff_cache(void);
+static int mpdccp_allocate_path_cb_cache(void);
+static void mpdccp_release_path_cb_cache(void);
+
+
+/************************************************* 
+ *     Reordering (generic)
+ *************************************************/
+
+int mpdccp_reordering_setup (void)
+{
+	int	ret;
+
+	ret = mpdccp_allocate_path_cb_cache ();
+	if (ret < 0) return ret;
+	ret = mpdccp_allocate_rcv_buff_cache ();
+	if (ret < 0) return ret;
+	ret = mpdccp_reorder_default_register ();
+	if (ret < 0) return ret;
+	// The following doesn't work - must not be done during module setup!!! - tbd.
+	//ret = mpdccp_set_default_reordering(CONFIG_DEFAULT_MPDCCP_REORDER);
+	ret = mpdccp_set_default_reordering("default");
+	if (ret < 0) {
+		mpdccp_pr_error("Failed to set default reordering engine \"%s\".\n",
+			CONFIG_DEFAULT_MPDCCP_REORDER);
+		return ret;
+	}
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mpdccp_reordering_setup);
+
+void mpdccp_reordering_finish (void)
+{
+	mpdccp_release_path_cb_cache();
+	mpdccp_reorder_default_unregister();
+}
+EXPORT_SYMBOL_GPL(mpdccp_reordering_finish);
+
+/**
+ * Initialize reordering, set current reordering operation in control block. 
+ */
+void mpdccp_init_reordering (struct mpdccp_cb *mpcb)
+{
+	struct mpdccp_reorder_ops *reorder;
+
+	if (!mpcb) return;
+	rcu_read_lock();
+	reorder = reorder_default;
+	if (try_module_get(reorder->owner)) {
+		mpcb->reorder_ops = reorder;
+		mpcb->mpdccp_reorder_cb = NULL;
+		if (reorder->init)
+			reorder->init (mpcb);
+		mpdccp_pr_debug("reordering set to %s", reorder->name);
+	} else {
+		pr_info("cannet init reordering %s", reorder->name);
+	}
+	rcu_read_unlock();
+}
+
+/**
+ * Release allocated memory.
+ */
+void mpdccp_cleanup_reordering(struct mpdccp_cb *mpcb)
+{
+	/* Release module */
+	module_put(mpcb->reorder_ops->owner);
+}
+
+/**
+ * Register a given reordering engine.
+ */
+int mpdccp_register_reordering(struct mpdccp_reorder_ops *reorder)
+{
+	int	ret = SUCCESS_RO;
+
+	if (!reorder) return -EINVAL;
+	rcu_read_lock();
+	if (mpdccp_reorder_find(reorder->name)) {
+		pr_notice("%s already registered\n", reorder->name);
+		ret = -EEXIST;
+	} else {
+		spin_lock(&mpdccp_reorder_list_lock);
+		list_add_tail_rcu(&reorder->list, &mpdccp_reorder_list);
+		spin_unlock(&mpdccp_reorder_list_lock);
+		ro_info("RO-INFO: %s registered\n", reorder->name);
+	}
+
+	if (!strcasecmp (reorder->name, "default")) {
+		reorder_fallback = reorder;
+	}
+
+	rcu_read_unlock();
+	return ret;
+}
+EXPORT_SYMBOL(mpdccp_register_reordering);
+
+/** 
+ * Unregister a given reordering engine.
+ */
+void mpdccp_unregister_reordering (struct mpdccp_reorder_ops *reorder)
+{
+	struct mpdccp_cb *mpcb = NULL; 
+
+	if (!reorder) return;
+	rcu_read_lock ();
+	spin_lock(&mpdccp_reorder_list_lock);
+	list_del_rcu(&reorder->list);
+
+	if (reorder == reorder_default)
+		reorder_default = list_entry_rcu(mpdccp_reorder_list.next, struct mpdccp_reorder_ops, list);
+	if (reorder == reorder_fallback)
+		reorder_fallback = NULL;
+	if (!reorder_default) 
+		reorder_default = reorder_fallback;
+	/* reset reordering ops back to default */
+	mpdccp_for_each_conn(pconnection_list, mpcb) {
+		if (mpcb->reorder_ops == reorder) {
+			lock_sock (mpcb->meta_sk);
+			mpdccp_init_reordering (mpcb);
+			release_sock (mpcb->meta_sk);
+		}
+	}
+	spin_unlock(&mpdccp_reorder_list_lock);
+	rcu_read_unlock ();
+	return;
+}
+EXPORT_SYMBOL(mpdccp_unregister_reordering);
+
+/**
+ * Find reordering engine in 'list_for_each_entry_rcu'.
+ */
+struct mpdccp_reorder_ops *mpdccp_reorder_find(const char *name)
+{
+	struct mpdccp_reorder_ops *e;
+
+	if(!name) return NULL;
+	list_for_each_entry_rcu(e, &mpdccp_reorder_list, list) {
+		if (strcmp(e->name, name) == 0) return e;
+	}
+	return NULL;
+}
+EXPORT_SYMBOL(mpdccp_reorder_find);
+
+/**
+ * Get the default reordering engine.
+ */
+void mpdccp_get_default_reordering (char *name)
+{
+	BUG_ON(list_empty(&mpdccp_reorder_list));
+	rcu_read_lock();
+	if (reorder_default) {
+		strncpy(name, reorder_default->name, MPDCCP_REORDER_NAME_MAX-1);
+		name[MPDCCP_REORDER_NAME_MAX-1]=0;
+	} else if (reorder_fallback) {
+		strncpy(name, reorder_fallback->name, MPDCCP_REORDER_NAME_MAX-1);
+		name[MPDCCP_REORDER_NAME_MAX-1]=0;
+	} else {
+		name[0]=0;
+	}
+	rcu_read_unlock();
+	return;
+}
+
+/**
+ * Set the default reordering engine.
+ */
+int mpdccp_set_default_reordering(const char *name)
+{
+	struct mpdccp_reorder_ops *reorder;
+	int ret = -ENOENT;
+
+	spin_lock(&mpdccp_reorder_list_lock);
+	reorder = mpdccp_reorder_find(name);
+#ifdef CONFIG_MODULES
+	if (!reorder && capable(CAP_NET_ADMIN)) {
+		spin_unlock(&mpdccp_reorder_list_lock);
+		request_module("mpdccp_reorder_%s", name);
+		spin_lock(&mpdccp_reorder_list_lock);
+		reorder = mpdccp_reorder_find(name);
+	}
+#endif
+	if (reorder) {
+		reorder_default = reorder;
+		ret = 0;
+	} else {
+		ro_err("RO-ERROR: %s is not available\n", name); 
+	}
+	spin_unlock(&mpdccp_reorder_list_lock);
+	return ret;
+}
+
+
+/************************************************* 
+ *     receive buffer
+ *************************************************/
+/**
+ * Create memory pool for work data blocks.
+ */
+static int mpdccp_allocate_rcv_buff_cache(void)
+{
+	mpdccp_reorder_rcv_buff_cache = kmem_cache_create("rcv_buff", sizeof(struct rcv_buff),
+                       0, SLAB_TYPESAFE_BY_RCU|SLAB_HWCACHE_ALIGN,
+                       NULL);
+	if (!mpdccp_reorder_rcv_buff_cache) {
+		ro_err("RO-ERROR: Failed to create mpdccp_reorder_rcv_buff slab cache.\n");
+		goto out;
+	}
+	return SUCCESS_RO;
+out:   
+	return -EAGAIN;
+}
+
+/**
+ * Release memory allocated for work data.
+ */
+int mpdccp_release_rcv_buff(struct rcv_buff **rb)
+{
+	if(rb && *rb){
+		//printk(KERN_INFO "buffer released?");
+		kmem_cache_free(mpdccp_reorder_rcv_buff_cache, *rb);
+		*rb = NULL;
+		return SUCCESS_RO;
+	}
+	else{
+		ro_err("RO-ERROR: rb or *rb is NULL");
+		return FAIL_RO;
+	}
+}
+EXPORT_SYMBOL(mpdccp_release_rcv_buff);
+
+/**
+ * Initialize work data for given parameter.
+ */
+struct rcv_buff *mpdccp_init_rcv_buff(struct sock *sk, struct sk_buff *skb, struct mpdccp_cb *mpcb)
+{
+	struct dccp_sock	*dsk = NULL; 
+	struct rcv_buff		*rb = NULL;
+
+	rb = kmem_cache_zalloc(mpdccp_reorder_rcv_buff_cache, GFP_ATOMIC);
+	if(!rb){
+		ro_err("RO-ERROR: Failed to initialize w\n");
+		return NULL;
+	}
+	//INIT_DELAYED_WORK(&rb->work, mpdccp_do_work);  // legacy
+	rb->sk = sk;
+	rb->skb = skb;
+	rb->mpcb = mpcb;
+	rb->mpdccp_reorder_cb = mpcb->mpdccp_reorder_cb;
+
+	if(!sk) return rb;
+	
+	/* 
+	 * current sequence number (required
+	 * since sequence number is a option, 
+	 * hence is extracted from the socket)
+	 */
+	dsk = dccp_sk(sk);
+	rb->oall_seqno = (u64)dsk->dccps_options_received.dccpor_oall_seq;
+	//mpdccp_pr_debug("seqno %lu sk %p", (unsigned long)rb->oall_seqno, sk);
+	rb->latency = (u32)dsk->dccps_options_received.dccpor_rtt_value;	/* need to divide by two for one way delay*/
+	//mpdccp_pr_debug("delay %lu sk %p", (unsigned long)rb->latency, sk);
+	return rb;
+}
+EXPORT_SYMBOL(mpdccp_init_rcv_buff);
+
+
+
+
+/************************************************* 
+ *     Path Control Block (Combined 
+ *     receive and delay vector)
+ *************************************************/
+
+
+/**
+ * Create memory pool for delay control blocks.
+ */
+static int mpdccp_allocate_path_cb_cache(void)
+{
+	mpdccp_reorder_path_cb_cache = kmem_cache_create("mpdccp_reorder_path_cb", sizeof(struct mpdccp_reorder_path_cb),
+                       0, SLAB_TYPESAFE_BY_RCU|SLAB_HWCACHE_ALIGN,
+                       NULL);
+	if (!mpdccp_reorder_path_cb_cache) {
+		ro_err("RO-ERROR: Failed to create mpdccp_reorder_path_cb slab cache.\n");
+		goto out;
+	}
+	return SUCCESS_RO;
+    
+out:   
+	return -EAGAIN;
+}
+
+/**
+ * Allocate memory for delay cb from memory pool.
+ */
+struct mpdccp_reorder_path_cb *mpdccp_init_reorder_path_cb(struct sock *sk)
+{
+	struct mpdccp_reorder_path_cb *pcb;
+
+	pcb = kmem_cache_zalloc(mpdccp_reorder_path_cb_cache, GFP_ATOMIC);
+	if (!pcb) {
+		ro_err("RO-ERROR: Failed to initialize pcb.\n");
+		return NULL;
+	}
+	mpdccp_reset_path_cb(pcb);
+	pcb->sk = sk;
+	pcb->active = true;
+
+	return pcb;
+}
+EXPORT_SYMBOL(mpdccp_init_reorder_path_cb);
+
+/**
+ * Release allocated memory.
+ */
+void mpdccp_free_reorder_path_cb(struct mpdccp_reorder_path_cb *pcb)
+{
+	kmem_cache_free(mpdccp_reorder_path_cb_cache, pcb);
+}
+EXPORT_SYMBOL(mpdccp_free_reorder_path_cb);
+
+/**
+ * Destroy memory pool.
+ */
+static void mpdccp_release_path_cb_cache(void)
+{
+	kmem_cache_destroy(mpdccp_reorder_path_cb_cache);
+}
+
+/**
+ * Reset all attributes (exept sk, list) of given 'pcb' of struct 'mpdccp_reorder_path_cb' to 0.
+ */
+static int mpdccp_reset_path_cb(struct mpdccp_reorder_path_cb *pcb)
+{
+	int i = 0, ret;
+	if(!pcb) goto fail;
+
+	pcb->mrtt = 0; pcb->krtt = 0; pcb->drtt = 0;
+	pcb->oall_seqno = 0; pcb->path_seqno = 0; pcb->not_rcv = 0;
+	pcb->exp_path_seqno = 0;
+
+	for(i = 0; i < DWINDOW_SIZE; i++){
+		pcb->wnd[i] = 0;
+		pcb->wnd_raw[i] = 0;
+	}
+	
+	ret = SUCCESS_RO;
+	goto finished;
+
+fail:
+	ret = FAIL_RO;
+finished:
+	return ret;
+}
+
+
+
+/**
+ * Perfrom path timing estimations.
+ */
+int mpdccp_path_est(struct mpdccp_reorder_path_cb* pcb, u32 mrtt)
+{
+	int ret;
+	if(!pcb) goto fail;
+
+	mpdccp_mrtt_estimator(pcb, mrtt);
+	mpdccp_krtt_estimator(pcb, mrtt);
+	//TODO crashes eventually (only on pyhisical setup I guess)
+	mpdccp_drtt_estimator(pcb, mrtt);
+
+	ro_dbug3("RO-DEBUG: mrtt=%llu, krtt=%llu, drtt=%llu [ms]",
+			(u64)get_mrtt(pcb), (u64)get_krtt(pcb), (u64)get_drtt(pcb));
+
+	ret = SUCCESS_RO;
+	goto finished;
+
+fail:
+	ro_err("RO-ERROR: NULL pcb");
+	ret = FAIL_RO;
+finished:
+	return ret;
+}
+EXPORT_SYMBOL(mpdccp_path_est);
+
+
+
+
+/*
+ * latency functions
+ */
+
+/**
+ * Function pointer that is used to switch 
+ * between different delay estimations.
+ */
+static u32 (*__get_rtt)(struct mpdccp_reorder_path_cb *pcb) = get_mrtt;
+
+/**
+ * Function and function pointer used by active (adaptive) 
+ * reordering and delay equalization.
+ */
+u32 mpdccp_get_lat (struct mpdccp_reorder_path_cb *pcb)
+{
+	return __get_rtt(pcb) / 2;
+} 
+EXPORT_SYMBOL(mpdccp_get_lat);
+
+/**
+ * Set function pointer to corresponding getter funtion.
+ */
+void mpdccp_set_rtt_type(int type)
+{
+	switch(type) {
+	default:
+		ro_warn("RO-WARN: type = %d is invalid, reset to MRTT", type);
+		/* fall thru */
+	case 0:
+		ro_info("RO-INFO: > now using MRTT");
+		__get_rtt = get_mrtt;
+		break;
+	case 1:
+		ro_info("RO-INFO: > now using KRTT");
+		__get_rtt = get_krtt;
+		break;
+	case 2:
+		__get_rtt = get_drtt;
+		ro_info("RO-INFO: > now using DRTT");
+		break;
+	}
+	return;
+}
+EXPORT_SYMBOL(mpdccp_set_rtt_type);
+
+
+
+/**
+ * Read different rtt types from path cb.
+ */
+static u32 get_mrtt(struct mpdccp_reorder_path_cb *pcb)
+{
+	return pcb ? pcb->mrtt : 1000;
+}
+
+static u32 get_krtt(struct mpdccp_reorder_path_cb *pcb)
+{
+	return pcb ? pcb->krtt : 1000;
+}
+
+static u32 get_drtt(struct mpdccp_reorder_path_cb *pcb)
+{
+	return pcb ? pcb->drtt : 1000;
+}
+
+
+/*
+ * Latency Estimator functions
+ */
+
+/**
+ * MRTT estimator, just calculate varaince.
+ */
+static void mpdccp_mrtt_estimator(struct mpdccp_reorder_path_cb *pcb, const long mrtt)
+{
+	if(pcb) pcb->mrtt = mrtt;
+	else ro_err("RO-ERROR: NULL pcb"); 
+	return;
+}
+
+/**
+ * Delay prediction using Kalman filter according to paper by Zhang et al., see
+ * URL : [https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6026701] 
+ * NOTE : parameters are scaled by 1000 to increase presicion
+ */
+static void mpdccp_krtt_estimator(struct mpdccp_reorder_path_cb *pcb, const long mrtt)
+{
+	u32 SF = 100000; 			  // scale factor required for fixed point arithmetics
+	u32 Q = 8000, R = 100000; 	  // parameter are interchanged (scaled by 100000) according to Zhang et al. 
+	u32 K, kSF = 1000;
+
+	if(!pcb) goto fail;
+	/* measurement stage */
+	K = (pcb->P_ * kSF) / (pcb->P_ + R); 	// scaling required since K < 1 
+	pcb->x = pcb->x_ + (K * ((mrtt * SF) - pcb->x_) / kSF);	
+	pcb->P = (pcb->P_ * ((1 * kSF) - K)) / kSF;
+
+	/* time stage */
+	pcb->x_ = pcb->x;
+	pcb->P_ = pcb->P + Q;
+
+	pcb->krtt = pcb->x_ / SF;
+	goto finished;
+
+fail:
+	ro_err("RO-ERROR: NULL pcb");
+finished:
+	return;
+}
+
+/**
+ * DRTT estimator, directional filter.
+ */
+static void mpdccp_drtt_estimator(struct mpdccp_reorder_path_cb *pcb, const long mrtt)
+{
+	u32 SF = 100000;
+
+	if (!pcb) {
+		ro_err("RO-ERROR: NULL pcb");
+		return;
+	}
+	if ((pcb->wnd[0] / SF) <= mrtt)
+		pcb->drtt = mrtt;
+	else
+		pcb->drtt = (mean(pcb->wnd, DWINDOW_SIZE) + mean(pcb->wnd_raw, DWINDOW_SIZE)) / (2 * SF);
+	
+	prepend (pcb->wnd, DWINDOW_SIZE, pcb->drtt * SF);
+	prepend (pcb->wnd_raw, DWINDOW_SIZE, mrtt * SF);
+
+	return;
+}
+
+/**
+ * Calcualte mean of fixed size array, values of 0 are interpreted as empty thus mitigated.
+ */
+static u32 mean(u32 *arr, int size)
+{
+	int i = 0, cnt = 0;
+	u32 tmp = 0;
+	for(i = 0; i < size; i++){
+		if(arr[i] > 0){
+			tmp += arr[i];
+			cnt++;
+		}
+	} 
+	if(cnt)
+		return tmp / cnt;
+	else
+		return 0;
+}
+
+/**
+ * Prepend element to fixed size array.
+ */
+static void prepend(u32 *arr, int size, u32 val)
+{
+	int i = 0;
+	for(i = size-1; i > 0; i--) arr[i] = arr[i-1];
+	arr[0] = val;
+}
+
+/**
+ * Get current time as ktime.
+ */
+ktime_t mpdccp_get_now(void)
+{
+   //struct timeval now;
+   // do_gettimeofday(&now);
+   //return timeval_to_ktime(now);
+	ktime_t now = ktime_get_real();
+	return now;
+}
+EXPORT_SYMBOL(mpdccp_get_now);
diff --git a/net/dccp/mpdccp_reordering.h b/net/dccp/mpdccp_reordering.h
new file mode 100644
index 0000000000000..fcad8c565850e
--- /dev/null
+++ b/net/dccp/mpdccp_reordering.h
@@ -0,0 +1,196 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ * 
+ * Copyright (C) 2018 by Maximilian Schuengel, Deutsche Telekom AG
+ * Copyright (C) 2018 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2020 by Frank Reker, Deutsche Telekom AG
+ *
+ * MPDCCP - Generic reordering functions.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#ifndef _MPDCCP_REORDERING_H
+#define _MPDCCP_REORDERING_H
+
+#include <net/net_namespace.h>
+#include <linux/netpoll.h>
+#include <linux/rculist.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include "dccp.h"
+#include "ccids/ccid2.h"
+#include "mpdccp_version.h"
+#include <net/mpdccp_link_info.h>
+#include <net/mpdccp.h>
+
+/* Maximum lengths for module names */
+#define MPDCCP_REORDER_NAME_MAX     16
+
+/* Define reordering return values. */
+#define SUCCESS_RO                  0        // success 
+#define FAIL_RO                     -1       // unspecific fail 
+#define PENDING_RO                  -2       // packet is out-of-order
+
+/* Define macros and scaling factors for reordering purposes */
+#define ms2us(ms)                               \
+    (ms * 1000)
+#define ms2ns(ms)                               \
+    (ms * 1000000)
+
+#define DWINDOW_SIZE                5        // window size for directional filter used for reordering
+
+
+/* Debugging (for reordering) */
+extern bool ro_err_state;
+extern bool ro_info_state;
+extern bool ro_warn_state;
+extern int ro_dbug_state;
+
+#define ro_err(fmt, a...)                                                                     \
+    if(ro_err_state) pr_err(fmt, ##a)
+#define ro_info(fmt, a...)                                                                    \
+    if(ro_info_state) pr_info(fmt, ##a)
+#define ro_warn(fmt, a...)                                                                    \
+    if(ro_warn_state) pr_warn(fmt, ##a)
+#define ro_dbug1(fmt, a...)                                                                   \
+    MPDCCP_PRINTK (ro_dbug_state, KERN_DEBUG fmt, ##a)
+#define ro_dbug2(fmt, a...)                                                                   \
+    MPDCCP_PRINTK (ro_dbug_state > 1, KERN_DEBUG fmt, ##a)
+// most detailed info output, i.e. output for each packet
+#define ro_dbug3(fmt, a...)                                                                   \
+    MPDCCP_PRINTK (ro_dbug_state > 2, KERN_DEBUG fmt, ##a)
+
+
+/* Reordering delay types */
+enum {
+	MPDCCP_REORDERING_DELAY_MRTT,		// raw_rtt	= 0
+	MPDCCP_REORDERING_DELAY_MIN_RTT,	// min_rtt	= 1
+	MPDCCP_REORDERING_DELAY_MAX_RTT,	// max_rtt	= 2
+	MPDCCP_REORDERING_DELAY_SRTT,		// srtt		= 3
+    MPDCCP_REORDERING_DELAY_KRTT     
+};
+
+
+/* 
+ * Reordering structures 
+ */
+
+/* control block holding subflow specific information */
+struct mpdccp_reorder_path_cb {
+	struct sock	*sk;
+	bool		active;		// activity status of subflow
+	
+	/* delay vector */
+	/* Raw values as obtained from the CCID */
+	u32		mrtt; 		/// current mrtt value
+	
+	/* Kalman filter parameters */
+	u32		krtt;		// current krtt value
+	u32		x;
+	u32		P; 
+	u32		x_;
+	u32		P_; 
+	
+	/* Directional filter parameter */
+	u32		drtt;
+	u32		wnd[DWINDOW_SIZE];
+	u32		wnd_raw[DWINDOW_SIZE];
+	
+	/* receive vector */
+	u64		buffed_pkts;
+	u64		exp_path_seqno:48;
+	u64		oall_seqno:48;	// current received overall sequence number on socket sk
+	u64		path_seqno:48;	// current received path sequence number on socket sk
+	u8		not_rcv;	// counter to monitor inactivity of socket
+};
+
+/*
+ * Receive buffer holds information on received MPDCCP packets.
+ */
+struct rcv_buff
+{
+	struct delayed_work	dwork;
+	
+	struct sock		*sk;
+	struct sk_buff		*skb;
+	struct mpdccp_cb	*mpcb;
+	
+	u64			oall_seqno:48;
+	u32			latency;
+	void 			*mpdccp_reorder_cb;
+};
+
+
+/* mpdccp_reorder_ops - MPDCCP reordering operations. 
+ * (*init)              Initialize reordering engine
+ * (*queue_reorder)     Invoked by packet reception (in interrupt), queues ingress packet in workqueue
+ * (*do_reorder)        (legacy) Invoked by workqueue for queued work items
+ * (*update_reorder)    Update reordering engine in case of additional/deleted subflows
+ * name[]               The name of this algorithm
+ * *owner               Useful for memleak detection
+ */
+struct mpdccp_reorder_ops {
+	struct list_head	list;
+	
+	void			(*init) (struct mpdccp_cb *mpcb);
+	void			(*do_reorder) (struct rcv_buff *w);
+	void			(*update_pseq) (struct my_sock *, struct sk_buff *);
+	
+	char			name[MPDCCP_REORDER_NAME_MAX];
+	struct module		*owner;
+};
+
+
+
+/*
+ * Reordering functions
+ */
+
+/* Generic reordering functions */
+int mpdccp_reordering_setup (void);
+void mpdccp_reordering_finish (void);
+
+void mpdccp_init_reordering(struct mpdccp_cb *mpcb);
+void mpdccp_cleanup_reordering(struct mpdccp_cb *mpcb);
+int mpdccp_register_reordering(struct mpdccp_reorder_ops *reorder);
+void mpdccp_unregister_reordering(struct mpdccp_reorder_ops *reorder);
+struct mpdccp_reorder_ops *mpdccp_reorder_find(const char *name);
+void mpdccp_get_default_reordering(char *name);
+int mpdccp_set_default_reordering(const char *name);
+ktime_t mpdccp_get_now(void);
+
+
+/* Reordering work queue handling */
+int mpdccp_release_rcv_buff(struct rcv_buff **rb);
+struct rcv_buff *mpdccp_init_rcv_buff(struct sock *sk, struct sk_buff *skb, struct mpdccp_cb *mpcb);
+
+
+/* Reordering path cb handling */
+struct mpdccp_reorder_path_cb *mpdccp_init_reorder_path_cb(struct sock *sk);
+void mpdccp_free_reorder_path_cb(struct mpdccp_reorder_path_cb *pcb);
+int mpdccp_path_est(struct mpdccp_reorder_path_cb* pcb, u32 mrtt);
+
+
+u32 mpdccp_get_lat(struct mpdccp_reorder_path_cb *pcb);
+void mpdccp_set_rtt_type(int type);
+
+
+/* default reordering */
+int mpdccp_reorder_default_register(void);
+void mpdccp_reorder_default_unregister (void);
+
+
+
+#endif /* _MPDCCP_REORDERING_H */
+
diff --git a/net/dccp/mpdccp_scheduler.c b/net/dccp/mpdccp_scheduler.c
new file mode 100644
index 0000000000000..98209acc26246
--- /dev/null
+++ b/net/dccp/mpdccp_scheduler.c
@@ -0,0 +1,286 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2017 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2017 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2019 by Nathalie Romo, Deutsche Telekom AG
+ * Copyright (C) 2020-2021 by Frank Reker, Deutsche Telekom AG
+ *
+ * MPDCCP - Scheduler Framework
+ * 
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/module.h>
+#include <linux/rculist.h>
+
+#include <net/mpdccp_link.h>
+#include <net/mpdccp_link_info.h>
+#include "mpdccp.h"
+#include "mpdccp_scheduler.h"
+#include "mpdccp_version.h"
+
+static struct list_head __rcu mpdccp_sched_list;
+DEFINE_SPINLOCK(mpdccp_sched_list_lock);
+static struct mpdccp_sched_ops 	*sched_default=NULL;
+static struct mpdccp_sched_ops 	*sched_fallback=NULL;
+
+int
+mpdccp_scheduler_setup (void)
+{
+	int	ret;
+
+	rcu_read_lock();
+
+	INIT_LIST_HEAD_RCU(&mpdccp_sched_list);
+	mpdccp_sched_default_register();
+
+	/* Initialize default scheduler */
+	// The following doesn't work - must not be done during module setup!!! - tbd.
+	//ret = mpdccp_set_default_scheduler(CONFIG_DEFAULT_MPDCCP_SCHED);
+	ret = mpdccp_set_default_scheduler("default");
+
+	rcu_read_unlock();
+
+	if (ret < 0) {
+		mpdccp_pr_error("Failed to set default scheduler \"%s\".\n", CONFIG_DEFAULT_MPDCCP_SCHED);
+		return ret;
+	}
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mpdccp_scheduler_setup);
+
+
+/* Check if a flow is fully established, i.e. the handshake is complete. */
+bool mpdccp_sk_can_send(struct sock *sk)
+{
+	struct mpdccp_cb *mpcb = MPDCCP_CB(sk);
+	return ((dccp_sk(sk)->auth_done || (mpcb && mpcb->fallback_sp))
+			&& (sk->sk_state == DCCP_OPEN || sk->sk_state == DCCP_PARTOPEN));
+}
+EXPORT_SYMBOL(mpdccp_sk_can_send);
+
+bool mpdccp_packet_fits_in_cwnd(struct sock *sk)
+{
+	unsigned int space;
+	struct dccp_sock *dp;
+	struct tcp_info info;
+	struct tcp_info *infop = &info;
+	
+	/* From RFC 4341:
+	 * The sender MAY send a data packet when pipe < cwnd but
+	 * MUST NOT send a data packet when pipe >= cwnd.
+	 *
+	 * tx_cwnd is the congestion window size in data packets
+	 * tx_pipe is the senders' estimate of packets in flight */
+	
+	dp = dccp_sk(sk);
+	if(dp->dccps_hc_tx_ccid == NULL){
+		mpdccp_pr_debug("ccid not yet setup sk %p", sk);
+		return false; 
+	}
+	ccid_hc_tx_get_info(dp->dccps_hc_tx_ccid, sk, infop);
+	if (infop->tcpi_segs_out >= infop->tcpi_snd_cwnd) {
+		mpdccp_pr_debug( "Socket %p is congestion limited (hc->tx_pipe = %d,\
+				hc->tx_cwnd = %d).\n", sk, infop->tcpi_segs_out,
+				infop->tcpi_snd_cwnd);
+		return false;
+	}
+	
+	/* Check if what is already queued in the subflow socket's
+	 * send-queue already fills the cwnd before we even have a 
+	 * chance to send anything. Shamelessly adopted from TCP,
+	 * inspired by qpolicy_simple_full().
+	 */
+	
+	// Remaining free space in the congestion window
+	space = infop->tcpi_snd_cwnd - infop->tcpi_segs_out;
+	
+	if (sk->sk_write_queue.qlen >= space) {
+		mpdccp_pr_debug("Socket %p has a full cwnd.\n", sk);
+		return false;
+	}
+	
+	return true;
+}
+EXPORT_SYMBOL(mpdccp_packet_fits_in_cwnd);
+
+/* This function returns a pointer that is part of a RCU protected
+ * structure. It must be called with the rcu_read_lock() held. */
+struct sock *mpdccp_return_single_flow(struct mpdccp_cb *mpcb)
+{
+	struct my_sock  *my_sk;
+	struct sock     *sk = NULL;
+	
+	rcu_read_lock();
+	
+	my_sk = list_first_or_null_rcu(&mpcb->psubflow_list, struct my_sock, sk_list);
+	if( !my_sk ) {
+		goto out; /* No socket available */
+	}
+	
+	sk = my_sk->my_sk_sock;
+	if(!sk || !mpdccp_sk_can_send(sk) || (!mpdccp_packet_fits_in_cwnd(sk) && !dccp_ack_pending(sk))) {
+		rcu_read_unlock();
+		dccp_pr_debug("No free pipe available.\n");
+		return NULL;
+	}
+
+out:
+	rcu_read_unlock();
+	return sk;
+}
+EXPORT_SYMBOL(mpdccp_return_single_flow);
+
+
+
+struct mpdccp_sched_ops *mpdccp_sched_find(const char *name)
+{
+	struct mpdccp_sched_ops *e;
+
+	if(!name)
+		return NULL;
+	
+	rcu_read_lock();
+	list_for_each_entry_rcu(e, &mpdccp_sched_list, list) {
+		if (strcmp(e->name, name) == 0){
+			rcu_read_unlock();
+	    		return e;
+		}
+	}
+	rcu_read_unlock();
+
+	return NULL;
+}
+EXPORT_SYMBOL(mpdccp_sched_find);
+
+
+int mpdccp_register_scheduler(struct mpdccp_sched_ops *sched)
+{
+	int	ret = 0;
+
+	if (!sched || !sched->get_subflow)
+		return -EINVAL;
+
+	rcu_read_lock();
+	if (mpdccp_sched_find(sched->name)) {
+		pr_notice("%s scheduler already registered\n", sched->name);
+		ret = -EEXIST;
+		goto out;
+	}
+	spin_lock(&mpdccp_sched_list_lock);
+	list_add_tail_rcu(&sched->list, &mpdccp_sched_list);
+	spin_unlock(&mpdccp_sched_list_lock);
+	if (!strcasecmp (sched->name, "default"))
+		sched_fallback = sched;
+
+	pr_info("%s scheduler registered\n", sched->name);
+out:
+	rcu_read_unlock();
+	return ret;
+}
+EXPORT_SYMBOL(mpdccp_register_scheduler);
+
+
+void mpdccp_unregister_scheduler(struct mpdccp_sched_ops *sched)
+{
+	struct mpdccp_cb	*mpcb = NULL; 
+	struct mpdccp_sched_ops	*defsched = NULL;
+
+	rcu_read_lock();
+	spin_lock(&mpdccp_sched_list_lock);
+	list_del_rcu(&sched->list);
+	if (sched == sched_default)
+		sched_default = list_entry_rcu(mpdccp_sched_list.next, struct mpdccp_sched_ops, list);
+	defsched = sched_default ? sched_default : sched_fallback;
+
+	/* reset scheduling ops back to default - this is buggy - tbd. */
+	mpdccp_for_each_conn(pconnection_list, mpcb) {
+		if (mpcb->sched_ops == sched) {
+			lock_sock(mpcb->meta_sk);
+			mpdccp_init_scheduler (mpcb);
+			release_sock(mpcb->meta_sk);
+		}
+	}
+	spin_unlock(&mpdccp_sched_list_lock);
+	rcu_read_unlock();
+}
+EXPORT_SYMBOL(mpdccp_unregister_scheduler);
+
+void mpdccp_get_default_scheduler(char *name)
+{
+	struct mpdccp_sched_ops *sched;
+
+	BUG_ON(list_empty(&mpdccp_sched_list));
+
+	rcu_read_lock();
+	sched = sched_default ? sched_default : sched_fallback;
+	strncpy(name, sched->name, MPDCCP_SCHED_NAME_MAX);
+	rcu_read_unlock();
+}
+
+int mpdccp_set_default_scheduler(const char *name)
+{
+	struct mpdccp_sched_ops *sched;
+	int ret = -ENOENT;
+
+	if (!name) name = CONFIG_DEFAULT_MPDCCP_SCHED;
+	rcu_read_lock();
+	sched = mpdccp_sched_find(name);
+#ifdef CONFIG_MODULES
+	if (!sched && capable(CAP_NET_ADMIN)) {
+		request_module("mpdccp_sched_%s", name);
+		sched = mpdccp_sched_find(name);
+	}
+#endif
+
+	if (sched) {
+		//spin_lock(&mpdccp_sched_list_lock);
+		sched_default = sched;
+		//spin_unlock(&mpdccp_sched_list_lock);
+		ret = 0;
+	} else {
+		pr_info("%s is not available\n", name);
+	}
+	rcu_read_unlock();
+
+	return ret;
+}
+
+void mpdccp_init_scheduler(struct mpdccp_cb *mpcb)
+{
+	struct mpdccp_sched_ops *sched;
+
+	if (!mpcb) return;
+	rcu_read_lock();
+	sched = sched_default;
+	mpcb->do_incr_oallseq = true;
+	if (try_module_get(sched->owner)) {
+		mpcb->sched_ops = sched;
+		if (sched->init_conn)
+			sched->init_conn (mpcb);
+		mpdccp_pr_debug("sched set to %s", sched->name);
+	} else {
+		pr_info("cannet init scheduler %s", sched->name);
+	}
+	rcu_read_unlock();
+}
+
+/* Manage refcounts on socket close. */
+void mpdccp_cleanup_scheduler(struct mpdccp_cb *mpcb)
+{
+	module_put(mpcb->sched_ops->owner);
+}
+
+
+
diff --git a/net/dccp/mpdccp_scheduler.h b/net/dccp/mpdccp_scheduler.h
new file mode 100644
index 0000000000000..a0d915ffad790
--- /dev/null
+++ b/net/dccp/mpdccp_scheduler.h
@@ -0,0 +1,85 @@
+/*
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ * 
+ * Copyright (C) 2017 by Andreas Philipp Matz <info@andreasmatz.de>
+ * Copyright (C) 2020-2021 by Frank Reker <frank@reker.net>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#ifndef _MPDCCP_SCHEDULER_H
+#define _MPDCCP_SCHEDULER_H
+
+
+#define MPDCCP_SCHED_NAME_MAX       16
+
+
+struct mpdccp_cb;
+struct sock;
+
+
+/* mpdccp_sched_ops - MPDCCP scheduler operations. 
+ * (*get_subflow)   The scheduling function itself
+ * (*init)          Initialize scheduler and per-subflow data for all connections
+ * (*init_conn)     Initialize scheduler and per-subflow data for a specific connection
+ * name[]           The name of this algorithm
+ * *owner           Useful for memleak detection
+ */
+struct mpdccp_sched_ops {
+    struct list_head list;
+
+    struct sock *       (*get_subflow)(struct mpdccp_cb *mpcb);
+    void                (*init_conn) (struct mpdccp_cb *mpcb);
+    void                (*init_subflow) (struct sock *sk);
+
+    char                name[MPDCCP_SCHED_NAME_MAX];
+    struct module       *owner;
+};
+
+
+
+/* 
+ * Generic scheduling functions 
+ */
+
+/* Check if a flow is fully established, i.e. the handshake is complete. */
+bool mpdccp_sk_can_send(struct sock *sk);
+
+bool mpdccp_packet_fits_in_cwnd(struct sock *sk);
+
+/* This function returns a pointer that is part of a RCU protected
+ * structure. It must be called with the rcu_read_lock() held. */
+struct sock *mpdccp_return_single_flow(struct mpdccp_cb *mpcb);
+
+
+/* 
+ * Scheduler management functions
+ */
+
+int mpdccp_scheduler_setup(void);
+
+int mpdccp_register_scheduler(struct mpdccp_sched_ops *sched);
+void mpdccp_unregister_scheduler(struct mpdccp_sched_ops *sched);
+void mpdccp_init_scheduler(struct mpdccp_cb *mpcb);
+void mpdccp_cleanup_scheduler(struct mpdccp_cb *mpcb);
+void mpdccp_get_default_scheduler(char *name);
+int mpdccp_set_default_scheduler(const char *name);
+struct mpdccp_sched_ops *mpdccp_sched_find(const char *name);
+
+
+/*
+ * Special default scheduler
+ */
+int mpdccp_sched_default_register (void);
+void mpdccp_sched_default_unregister (void);
+
+
+
+#endif /* _MPDCCP_SCHEDULER_H */
+
diff --git a/net/dccp/mpdccp_subflow.c b/net/dccp/mpdccp_subflow.c
new file mode 100644
index 0000000000000..e3efaaa74166d
--- /dev/null
+++ b/net/dccp/mpdccp_subflow.c
@@ -0,0 +1,149 @@
+/*
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ * 
+ * Copyright (C) 2022 by Frank Reker <frank@reker.net>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/errno.h>
+#include <linux/notifier.h>
+#include <linux/skbuff.h>
+#include <net/net_namespace.h>
+#include <linux/rtnetlink.h>
+#include <generated/autoconf.h>
+#include <net/mpdccp.h>
+#include <net/mpdccp_link.h>
+#include <net/mpdccp_link_info.h>
+#include "mpdccp.h"
+
+static RAW_NOTIFIER_HEAD(mpdccp_subflow_chain);
+
+
+int
+register_mpdccp_subflow_notifier (
+	struct notifier_block	*nb)
+{
+	int	ret;
+
+	rtnl_lock();
+	ret = raw_notifier_chain_register(&mpdccp_subflow_chain, nb);
+	rtnl_unlock();
+	return ret;
+}
+EXPORT_SYMBOL(register_mpdccp_subflow_notifier);
+
+
+int
+unregister_mpdccp_subflow_notifier (
+	struct notifier_block	*nb)
+{
+	int	ret;
+
+	rtnl_lock();
+	ret = raw_notifier_chain_unregister(&mpdccp_subflow_chain, nb);
+	rtnl_unlock();
+	return ret;
+}
+EXPORT_SYMBOL(unregister_mpdccp_subflow_notifier);
+
+int
+call_mpdccp_subflow_notifiers (
+	unsigned long		action,
+	struct sock		*sk)
+{
+	int				ret;
+	struct sock			*meta_sk = NULL;
+	struct mpdccp_cb		*mpcb = NULL;
+	struct my_sock			*my_sk = NULL;
+	struct mpdccp_link_info		*link = NULL;
+	struct mpdccp_subflow_notifier	info;
+
+	if (!sk) return -EINVAL;
+	mpcb = get_mpcb (sk);
+	if (!mpcb) return -EINVAL;
+	switch (action) {
+	case MPDCCP_EV_ALL_SUBFLOW_DOWN:
+		if (!mpdccp_is_meta(sk)) return 0;
+		if (!mpcb->up_reported) return 0;
+		meta_sk = sk;
+		sk = NULL;
+		link = NULL;
+		break;
+	case MPDCCP_EV_SUBFLOW_CREATE:
+	case MPDCCP_EV_SUBFLOW_DESTROY:
+		if (mpdccp_is_meta(sk)) return 0;
+		meta_sk = mpcb->meta_sk;
+		if (!meta_sk) return -EINVAL;
+		my_sk = mpdccp_my_sock(sk);
+		if (!my_sk) return -EINVAL;
+		if (action == MPDCCP_EV_SUBFLOW_CREATE) {
+			my_sk->up_reported = 1;
+			mpcb->up_reported = 1;
+		} else {
+			if (!my_sk->up_reported) return 0;
+		}
+		rcu_read_lock();
+		link = my_sk->link_info;
+		mpdccp_link_get (link);
+		rcu_read_unlock();
+		break;
+	}
+	sock_hold (meta_sk);
+
+	info = (struct mpdccp_subflow_notifier) {
+		.link = link,
+		.sk = meta_sk,
+		.subsk = sk,
+		.role = mpcb->role,
+	};
+
+	ret = raw_notifier_call_chain(&mpdccp_subflow_chain, action, &info);
+	if (link) {
+		rcu_read_lock();
+		mpdccp_link_put (link);
+		rcu_read_unlock();
+	}
+	sock_put (meta_sk);
+	return ret;
+}
+EXPORT_SYMBOL(call_mpdccp_subflow_notifiers);
+
+int
+mpdccp_report_alldown (
+	struct sock	*sk)
+{
+	return call_mpdccp_subflow_notifiers (MPDCCP_EV_ALL_SUBFLOW_DOWN, sk);
+}
+EXPORT_SYMBOL(mpdccp_report_alldown);
+
+int
+mpdccp_report_destroy (
+	struct sock	*sk)
+{
+	return call_mpdccp_subflow_notifiers (MPDCCP_EV_SUBFLOW_DESTROY, sk);
+}
+EXPORT_SYMBOL(mpdccp_report_destroy);
+
+
+int
+mpdccp_report_new_subflow (
+	struct sock	*sk)
+{
+	return call_mpdccp_subflow_notifiers (MPDCCP_EV_SUBFLOW_CREATE, sk);
+}
+EXPORT_SYMBOL(mpdccp_report_new_subflow);
+
+
+
+
diff --git a/net/dccp/mpdccp_sysctl.c b/net/dccp/mpdccp_sysctl.c
new file mode 100644
index 0000000000000..cc8c38ed70bae
--- /dev/null
+++ b/net/dccp/mpdccp_sysctl.c
@@ -0,0 +1,310 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2017 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2017 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2020-2021 by Frank Reker, Deutsche Telekom AG
+ *
+ * MPDCCP - DCCP bundling kernel module
+ *
+ * This module implements a bundling mechanism that aggregates
+ * multiple paths using the DCCP protocol.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/kernel.h>
+#include <linux/workqueue.h>
+#include <linux/dccp.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/skbuff.h>
+#include <linux/socket.h>
+#include <linux/in.h>
+#include <linux/inet.h>
+#include <linux/kthread.h>
+#include <uapi/linux/net.h>
+
+#include <net/inet_common.h>
+#include <net/inet_sock.h>
+#include <net/protocol.h>
+#include <net/sock.h>
+#include <net/tcp_states.h>
+#include <linux/inetdevice.h>
+#include <net/mpdccp_link.h>
+
+#include "mpdccp.h"
+#include "mpdccp_scheduler.h"
+#include "mpdccp_reordering.h"
+#include "mpdccp_pm.h"
+
+/* Sysctl variables */
+struct  ctl_table_header *mpdccp_sysctl;
+/* Controls whether the client sends SRTT or MRTT */
+int sysctl_mpdccp_delay_config __read_mostly    = 0;
+int sysctl_mpdccp_accept_prio __read_mostly    = 0;
+
+
+static int proc_mpdccp_path_manager(struct ctl_table *ctl, int write,
+                void __user *buffer, size_t *lenp,
+                loff_t *ppos)
+{
+	int			ret;
+	struct mpdccp_pm_ops	*pm;
+	struct mpdccp_cb	*mpcb;
+	char			val[MPDCCP_PM_NAME_MAX];
+	
+	struct ctl_table tbl = {
+		.data = val,
+		.maxlen = MPDCCP_PM_NAME_MAX,
+	};
+	
+	mpdccp_get_default_path_manager(val);
+	
+	ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+	if (write && ret == 0){
+	    
+		ret = mpdccp_set_default_path_manager(val);
+		if(ret < 0){
+			mpdccp_pr_debug("Path manager not found (%d).\n", ret);
+			return ret;
+		}
+		
+		rcu_read_lock();
+		//Find pm struct corresponding to pm name
+		pm = mpdccp_pm_find(val);
+		if(!pm) {
+			rcu_read_unlock();
+			mpdccp_pr_debug("Path manager not found.\n");
+			return ret;
+		}
+		
+		//Assign pm to all existing connections
+		mpdccp_for_each_conn(pconnection_list, mpcb) {
+			mpcb->pm_ops = pm;
+		}
+		
+		rcu_read_unlock();
+	}
+	
+	return ret;
+}
+
+static int proc_mpdccp_scheduler(struct ctl_table *ctl, int write,
+                void __user *buffer, size_t *lenp,
+                loff_t *ppos)
+{
+	int			ret;
+	struct mpdccp_sched_ops	*sched;
+	struct mpdccp_cb	*mpcb;
+	char			val[MPDCCP_SCHED_NAME_MAX];
+	
+	struct ctl_table tbl = {
+		.data = val,
+		.maxlen = MPDCCP_SCHED_NAME_MAX,
+	};
+	
+	mpdccp_get_default_scheduler(val);
+	
+	ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+	if (write && ret == 0){
+	    
+		ret = mpdccp_set_default_scheduler(val);
+		if(ret < 0){
+			mpdccp_pr_debug("Scheduler not found (%d).\n", ret);
+			return ret;
+		}
+		
+		rcu_read_lock();
+		//Find sched struct corresponding to sched name
+		sched = mpdccp_sched_find(val);
+		if(!sched){
+			rcu_read_unlock();
+			mpdccp_pr_debug("Scheduler not found.\n");
+			return ret;
+		}
+
+        //Assign sched to all existing connections
+	mpdccp_for_each_conn(pconnection_list, mpcb) {
+		if (!mpcb->has_own_sched) {
+			lock_sock(mpcb->meta_sk);
+			mpdccp_init_scheduler (mpcb);
+			release_sock(mpcb->meta_sk);
+		}
+	}
+
+        rcu_read_unlock();
+    }
+
+    return ret;
+}
+
+static int proc_mpdccp_reordering(struct ctl_table *ctl, int write,
+                void __user *buffer, size_t *lenp,
+                loff_t *ppos)
+{
+	int ret;
+	struct mpdccp_reorder_ops *reorder;
+	struct mpdccp_cb *mpcb;
+	char val[MPDCCP_REORDER_NAME_MAX];
+	
+	struct ctl_table tbl = {
+		.data = val,
+		.maxlen = MPDCCP_REORDER_NAME_MAX,
+	};
+	
+	mpdccp_get_default_reordering(val);
+	
+	ret = proc_dostring(&tbl, write, buffer, lenp, ppos);
+	if (write && ret == 0){
+	    
+		ret = mpdccp_set_default_reordering(val);
+		if(ret < 0){
+			mpdccp_pr_debug("Reordering engine not found (%d)\n", ret);
+			return ret;
+		}
+		
+		rcu_read_lock();
+		
+		//Find reorder struct corresponding to reorder name
+		reorder = mpdccp_reorder_find(val);
+		if(!reorder){
+			mpdccp_pr_debug("Reordering engine not found.\n");
+			return ret;
+		}
+		
+		//Assign reorder to all existing connections
+		mpdccp_for_each_conn(pconnection_list, mpcb) {
+			if (!mpcb->has_own_reorder) {
+				rcu_read_unlock();
+				lock_sock (mpcb->meta_sk);
+				mpdccp_init_reordering (mpcb);
+				release_sock (mpcb->meta_sk);
+				rcu_read_lock();
+			}
+		}
+		rcu_read_unlock();
+	}
+	return ret;
+}
+
+static int proc_mpdccp_delay_config(struct ctl_table *table, int write,
+                void __user *buffer, size_t *lenp,
+                loff_t *ppos)
+{   
+	int ret;
+	mpdccp_pr_debug("/proc triggered reordering delay config\n");
+	ret = proc_dointvec(table, write, buffer, lenp, ppos);  
+	
+	if(ret == 0){
+		switch(sysctl_mpdccp_delay_config){
+		case MPDCCP_REORDERING_DELAY_MRTT:
+			mpdccp_pr_debug("Switched to MRTT\n");
+			set_mrtt_as_delayn();
+			break;
+		case MPDCCP_REORDERING_DELAY_MIN_RTT:
+			mpdccp_pr_debug("Switched to Min RTT\n");
+			set_min_rtt_as_delayn();
+			break;
+		case MPDCCP_REORDERING_DELAY_MAX_RTT:
+			mpdccp_pr_debug("Switched to Max RTT\n");
+			set_max_rtt_as_delayn();
+			break;
+		case MPDCCP_REORDERING_DELAY_SRTT:
+			mpdccp_pr_debug("Switched to SRTT\n");
+			set_srtt_as_delayn();
+			break;
+		default:
+			mpdccp_pr_debug("Parameter %d unknown, switched to SRTT\n", sysctl_mpdccp_delay_config);
+			set_srtt_as_delayn();
+			break;
+		}
+	} else {
+		set_srtt_as_delayn();
+	}
+	return ret;
+}
+
+static int proc_mpdccp_accept_prio(struct ctl_table *table, int write,
+                void __user *buffer, size_t *lenp,
+                loff_t *ppos)
+{
+	int ret = proc_dointvec(table, write, buffer, lenp, ppos);
+
+	if(ret == 0)
+		mpdccp_set_accept_prio(sysctl_mpdccp_accept_prio);
+	return ret;
+}
+
+struct ctl_table mpdccp_table[] = {
+	{
+		.procname = "mpdccp_enabled",
+		.data = &mpdccp_enabled,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = proc_dointvec,
+	},
+	{
+		.procname = "mpdccp_path_manager",
+		.maxlen = MPDCCP_PM_NAME_MAX,
+		.mode = 0644,
+		.proc_handler = proc_mpdccp_path_manager,
+	},
+	{
+		.procname = "mpdccp_scheduler",
+		.maxlen = MPDCCP_SCHED_NAME_MAX,
+		.mode = 0644,
+		.proc_handler = proc_mpdccp_scheduler,
+	},
+	{
+		.procname = "mpdccp_reordering",
+		.maxlen = MPDCCP_REORDER_NAME_MAX,
+		.mode = 0644,
+		.proc_handler = proc_mpdccp_reordering,
+	},
+	{
+		.procname = "mpdccp_rtt_config",
+		.data = &sysctl_mpdccp_delay_config,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = proc_mpdccp_delay_config,
+	},
+	{
+		.procname = "mpdccp_accept_prio",
+		.data = &sysctl_mpdccp_accept_prio,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = proc_mpdccp_accept_prio,
+	},
+	{
+		.procname = "mpdccp_debug",
+		.data = &mpdccp_debug,
+		.maxlen = sizeof(int),
+		.mode = 0644,
+		.proc_handler = &proc_dointvec,
+	},
+	{ }
+};
+
+
+int mpdccp_sysctl_init (void)
+{
+	/* Initialize /proc interface for controlling the solution*/
+	mpdccp_sysctl = register_net_sysctl(&init_net, "net/mpdccp", mpdccp_table);
+	if (!mpdccp_sysctl) {
+		mpdccp_pr_debug("Failed to register sysctl.\n");
+		return -1;
+	}
+	return 0;
+}
+
+void mpdccp_sysctl_finish(void)
+{
+	unregister_net_sysctl_table(mpdccp_sysctl);
+}
+
+
+
+
diff --git a/net/dccp/mpdccp_version.h b/net/dccp/mpdccp_version.h
new file mode 100644
index 0000000000000..e4e3e0b667ea8
--- /dev/null
+++ b/net/dccp/mpdccp_version.h
@@ -0,0 +1,3 @@
+#define MPDCCP_NAME                 "MPDCCP"
+#define MPDCCP_RELEASE_TYPE         "Development"
+#define MPDCCP_VERSION              "0.3"
\ No newline at end of file
diff --git a/net/dccp/options.c b/net/dccp/options.c
index d24cad05001e5..c629002187879 100644
--- a/net/dccp/options.c
+++ b/net/dccp/options.c
@@ -14,10 +14,18 @@
 #include <linux/kernel.h>
 #include <linux/skbuff.h>
 
+// TODO: cleanup
+#include "ccids/ccid2.h"
+
 #include "ackvec.h"
 #include "ccid.h"
 #include "dccp.h"
 #include "feat.h"
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+#  include "mpdccp.h"
+#  include "mpdccp_pm.h"
+#  include <net/mpdccp.h>
+#endif
 
 u64 dccp_decode_value_var(const u8 *bf, const u8 len)
 {
@@ -39,6 +47,97 @@ u64 dccp_decode_value_var(const u8 *bf, const u8 len)
 	return value;
 }
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+static int mpdccp_is_link_mpcap(struct dccp_request_sock *dreq,
+		       struct sk_buff *skb)
+{
+	struct inet_request_sock *ireq;
+	struct net *net;
+	struct net_device *ndev;
+
+	ireq = &dreq->dreq_inet_rsk;
+	if (!ireq) return 0;
+	net = read_pnet(&(ireq->ireq_net));
+	if(!net) return 0;
+	ndev = dev_get_by_index(net, skb->skb_iif);
+	if(!ndev) return 0;
+	if(!(ndev->flags & IFF_MPDCCPON)){
+		dccp_pr_debug("Not accepting MP-DCCP on this interface, dropping options!");
+		return 1;
+	}
+	return 0;
+}
+
+static int mpdccp_do_hmac_chk(struct sock *sk, struct dccp_options_received *opt_recv)
+{
+	struct mpdccp_cb *mpcb = get_mpcb(sk);
+	struct my_sock  *my_sk = mpdccp_my_sock(sk);
+	u8 chk_hmac[MPDCCP_HMAC_SIZE];
+	int ret;
+
+	if (opt_recv->dccpor_addaddr[0] && mpcb->pm_ops->add_addr){
+		u8 len = opt_recv->dccpor_addaddr_len;
+		u8 id = opt_recv->dccpor_addaddr[3];
+		union inet_addr addr;
+		sa_family_t family = AF_INET;
+		u16 port = 0;
+
+		if(len == 8 || len == 10){					// ipv4
+			addr.ip = get_unaligned_be32(&opt_recv->dccpor_addaddr[4]);
+			if(len == 10){
+				port = get_unaligned_be16(&opt_recv->dccpor_addaddr[8]);
+			}
+			dccp_pr_debug("rx opt: DCCPO_MP_ADDADDR = rem_id: %u, ip4 %pI4:%u", id, &addr.ip, port);
+		} else if(len == 20 || len == 22){			//ipv6
+			family = AF_INET6;
+			memcpy(addr.ip6, &opt_recv->dccpor_addaddr[4], 16);
+			if(len == 22){
+				port = get_unaligned_be16(&opt_recv->dccpor_addaddr[20]);
+			}
+			dccp_pr_debug("rx opt: DCCPO_MP_ADDADDR = rem_id: %u, ip6 %pI6:%u", id, &addr.ip6, port);
+		}
+
+		ret = mpcb->pm_ops->get_hmac(mpcb, id, family, &addr, port, 0, chk_hmac);
+
+		if(!ret){
+			dccp_pr_debug("HMAC compare rx: %llx exp: %llx", be64_to_cpu(*(u64 *)opt_recv->dccpor_mp_hmac),
+					be64_to_cpu(*((u64 *)chk_hmac)));
+			if(memcmp(opt_recv->dccpor_mp_hmac, chk_hmac, MPDCCP_HMAC_SIZE)){
+				DCCP_CRIT("HMAC CHECK FAILED!");
+				return 0;
+			}
+
+			if(opt_recv->dccpor_oall_seq < my_sk->last_addpath_seq){
+				dccp_pr_debug("option outdated, not acting on MP_ADDADDR");
+				return 0;
+			}
+			my_sk->last_addpath_seq = opt_recv->dccpor_oall_seq;
+			mpcb->pm_ops->add_addr(mpcb, family, id, &addr, port, true);
+			return len;
+		}
+	}
+	
+	else if(opt_recv->dccpor_removeaddr[0] && mpcb->pm_ops->rcv_removeaddr_opt){
+		u8 id = opt_recv->dccpor_removeaddr[3];
+		dccp_pr_debug("%s rx opt: DCCPO_MP_REMOVEADDR id: %u", dccp_role(sk), id);
+
+		ret = mpcb->pm_ops->get_hmac(mpcb, id, 0, 0, 0, 0, chk_hmac);
+
+		if(!ret){
+			dccp_pr_debug("HMAC compare rx: %llx exp: %llx", be64_to_cpu(*(u64 *)opt_recv->dccpor_mp_hmac),
+					be64_to_cpu(*((u64 *)chk_hmac)));
+			if(memcmp(opt_recv->dccpor_mp_hmac, chk_hmac, MPDCCP_HMAC_SIZE)){
+				DCCP_CRIT("HMAC CHECK FAILED!");
+				return 0;
+			}
+			mpcb->pm_ops->rcv_removeaddr_opt(mpcb, id);
+			return 4;
+		}
+	}
+	return 0;
+}
+#endif
+
 /**
  * dccp_parse_options  -  Parse DCCP options present in @skb
  * @sk: client|server|listening dccp socket (when @dreq != NULL)
@@ -61,8 +160,10 @@ int dccp_parse_options(struct sock *sk, struct dccp_request_sock *dreq,
 	u32 elapsed_time;
 	__be32 opt_val;
 	int rc;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	u8 mp_opt = 0;
+#endif
 	int mandatory = 0;
-
 	memset(opt_recv, 0, sizeof(*opt_recv));
 
 	opt = len = 0;
@@ -73,12 +174,14 @@ int dccp_parse_options(struct sock *sk, struct dccp_request_sock *dreq,
 
 		/* Check if this isn't a single byte option */
 		if (opt > DCCPO_MAX_RESERVED) {
-			if (opt_ptr == opt_end)
+			if (opt_ptr == opt_end){
 				goto out_nonsensical_length;
+			}
 
 			len = *opt_ptr++;
-			if (len < 2)
+			if (len < 2){
 				goto out_nonsensical_length;
+			}
 			/*
 			 * Remove the type and len fields, leaving
 			 * just the value size
@@ -87,8 +190,9 @@ int dccp_parse_options(struct sock *sk, struct dccp_request_sock *dreq,
 			value	= opt_ptr;
 			opt_ptr += len;
 
-			if (opt_ptr > opt_end)
+			if (opt_ptr > opt_end){
 				goto out_nonsensical_length;
+			}
 		}
 
 		/*
@@ -211,6 +315,280 @@ int dccp_parse_options(struct sock *sk, struct dccp_request_sock *dreq,
 			dccp_pr_debug("%s rx opt: ELAPSED_TIME=%d\n",
 				      dccp_role(sk), elapsed_time);
 			break;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+		case DCCPO_MULTIPATH:
+			if (dreq && skb && mpdccp_is_link_mpcap(dreq, skb)) break;
+			if (len == 0)
+				goto out_invalid_option;
+			mp_opt = *value++;
+			len--;
+			switch(mp_opt) {
+
+			case DCCPO_MP_CONFIRM:
+				dccp_pr_debug("%s rx opt: DCCPO_MP_CONFIRM = %u len: %u", 
+						dccp_role(sk), (u8)dccp_decode_value_var(value+11, 1), len);
+				if(is_mpdccp(sk) && len > 3){
+					struct mpdccp_cb *mpcb = get_mpcb(sk);
+					u8 opt[MPDCCP_CONFIRM_SIZE];
+					u8 id;
+
+					memcpy(opt, value, len);
+					if(opt[11] == DCCPO_MP_PRIO)
+						id = mpdccp_my_sock(sk)->local_addr_id;
+					else
+						id = opt[12] ? opt[12] : mpcb->master_addr_id;
+
+					if (mpcb->pm_ops->rcv_confirm_opt)
+						mpcb->pm_ops->rcv_confirm_opt(mpcb, opt, id);
+				}
+				break;
+
+			case DCCPO_MP_JOIN:
+				if (len != 9) {
+					goto out_invalid_option;
+				}
+				opt_recv->saw_mpjoin = 1;
+				opt_recv->dccpor_join_ip_local = be32_to_cpu(ip_hdr(skb)->daddr);
+				opt_recv->dccpor_join_ip_remote = be32_to_cpu(ip_hdr(skb)->saddr);
+				opt_recv->dccpor_join_port = dccp_hdr(skb)->dccph_sport;
+				opt_recv->dccpor_join_id = dccp_decode_value_var(value, 1);
+				value += 1;
+				opt_recv->dccpor_mp_cix = get_unaligned_be32(value);
+				value += 4;
+				opt_recv->dccpor_mp_nonce = get_unaligned_be32(value);
+
+				dccp_pr_debug("%s rx opt: DCCPO_MP_JOIN = addrID %u on %pI4:%u from " \
+								"%pI4:%u, CID %u nonce %x, sk %p",
+								dccp_role(sk), opt_recv->dccpor_join_id,
+								&ip_hdr(skb)->daddr, htons(dccp_hdr(skb)->dccph_dport),
+								&ip_hdr(skb)->saddr, htons(dccp_hdr(skb)->dccph_sport),
+								opt_recv->dccpor_mp_cix,
+								opt_recv->dccpor_mp_nonce, sk);
+				break;
+
+			case DCCPO_MP_FAST_CLOSE:
+				dccp_pr_debug("%s rx opt: DCCPO_MP_FAST_CLOSE = key %llx, len %u, sk %p",
+						dccp_role(sk), be64_to_cpu(*(u64*)value), len, sk);
+				if(is_mpdccp(sk) && len > 7){
+					struct mpdccp_cb *mpcb = get_mpcb(sk);
+					struct mpdccp_key *loc_key = &mpcb->mpdccp_loc_keys[mpcb->cur_key_idx];
+					dccp_pr_debug("key: %llx", be64_to_cpu(*((__be64 *)loc_key->value)));
+
+					if(len != loc_key->size) break;
+					if(memcmp(value, loc_key->value, len)) {
+						DCCP_CRIT("MP_CLOSE keys dont match");
+						return -1;
+					}
+					mpcb->close_fast = 2;				//close_fast=2 doesn't add MP_FAST_CLOSE option to reset
+					mpdccp_close_meta(mpcb->meta_sk);
+				} else
+					goto out_invalid_option;
+				break;
+
+			case DCCPO_MP_KEY:
+				if (len < 2) {
+					goto out_invalid_option;
+				}
+				value++;
+				opt_recv->dccpor_mp_cix = get_unaligned_be32(value);
+				value += 4;
+				len -= 5;
+				opt_recv->saw_mpkey = 1;
+				if (pkt_type == DCCP_PKT_REQUEST) {
+					int i;
+					/* Request: collect key types */
+					for (i = 0; len && (i < MPDCCP_MAX_KEYS); i++) {
+						int key_len;
+						u8 key_type = *value++;
+						switch (key_type) {
+							case DCCPK_PLAIN:
+								key_len = MPDCCP_PLAIN_KEY_SIZE;
+								break;
+							/* fall through */
+							case DCCPK_C25519_SHA256:
+							case DCCPK_C25519_SHA512:
+								key_len = MPDCCP_C25519_KEY_SIZE;
+								break;
+							default:
+								dccp_pr_debug("%s rx opt: received unsupported key type %d", dccp_role(sk), key_type);
+								goto out_invalid_option;
+						}
+						len--;
+						if (len < key_len) {
+							dccp_pr_debug("%s rx opt: invalid key length %d", dccp_role(sk), len);
+							goto out_invalid_option;
+						}
+
+						dccp_pr_debug("%s rx opt: DCCPO_MP_KEY(%d) = %llx, type %d, len %d, sk %p dreq %p remlen %d",
+								dccp_role(sk), i, be64_to_cpu(*(u64*)value), key_type, key_len, sk, dreq, len);
+						memcpy(opt_recv->dccpor_mp_keys[i].value, value, key_len);
+						opt_recv->dccpor_mp_keys[i].size = key_len;
+						opt_recv->dccpor_mp_keys[i].type = key_type;
+						opt_recv->dccpor_mp_suppkeys |= (1 << key_type);
+						len -= key_len;
+						value += key_len;
+					}
+				} else if ((pkt_type == DCCP_PKT_ACK) || (pkt_type == DCCP_PKT_RESPONSE)) {
+					int i;
+					/* Response/Ack: collect key data */
+					for (i = 0; len && (i < MPDCCP_MAX_KEYS); i++) {
+						int key_len;
+						u8 key_type = *value++;
+						switch (key_type) {
+							case DCCPK_PLAIN:
+								key_len = MPDCCP_PLAIN_KEY_SIZE;
+								break;
+							/* fall through */
+							case DCCPK_C25519_SHA256:
+							case DCCPK_C25519_SHA512:
+								key_len = MPDCCP_C25519_KEY_SIZE;
+								break;
+							default:
+								dccp_pr_debug("%s rx opt: received unsupported key type %d", dccp_role(sk), key_type);
+								goto out_invalid_option;
+						}
+						len--;
+						if (len < key_len) {
+							dccp_pr_debug("%s rx opt: invalid key length %d", dccp_role(sk), len);
+							goto out_invalid_option;
+						}
+
+						dccp_pr_debug("%s rx opt: DCCPO_MP_KEY(%d) = %llx, type %d, len %d, sk %p dreq %p remlen %d",
+								dccp_role(sk), i, be64_to_cpu(*(u64*)value), key_type, key_len, sk, dreq, len);
+						memcpy(opt_recv->dccpor_mp_keys[i].value, value, key_len);
+						opt_recv->dccpor_mp_keys[i].size = key_len;
+						opt_recv->dccpor_mp_keys[i].type = key_type;
+						opt_recv->dccpor_mp_suppkeys |= (1 << key_type);
+						len -= key_len;
+						value += key_len;
+					}
+					/* Send ack if the authentication was completed before */
+					if ((pkt_type == DCCP_PKT_ACK) && dccp_sk(sk)->auth_done) {
+						dccp_send_ack(sk);
+					}
+				}
+				break;
+
+			case DCCPO_MP_SEQ:
+				//TODO reordering, use 48 bit > 8 -> 6
+				if(len != 6){ //if not 48 bit
+					goto out_invalid_option;
+				}
+				opt_recv->dccpor_oall_seq = dccp_decode_value_var(value, len);
+				dccp_pr_debug("%s rx opt: MP_SEQ = %llu on loc_id: %u, rem_id: %u",
+						dccp_role(sk), (u64)opt_recv->dccpor_oall_seq,
+						get_id(sk), mpdccp_my_sock(sk)->remote_addr_id);
+				break;
+
+			case DCCPO_MP_HMAC:
+				if (len != MPDCCP_HMAC_SIZE) {
+					goto out_invalid_option;
+				}
+				dccp_pr_debug("%s rx opt: DCCPO_MP_HMAC = %llx, sk %p dreq %p", dccp_role(sk),
+						be64_to_cpu(*(u64 *)value), sk, dreq);
+
+				memcpy(opt_recv->dccpor_mp_hmac, value, MPDCCP_HMAC_SIZE);
+				/* Send ack if the authentication was completed before */
+				if ((pkt_type == DCCP_PKT_ACK) && dccp_sk(sk)->auth_done)
+					dccp_send_ack(sk);
+
+				if(!is_mpdccp(sk)) break;
+				len = mpdccp_do_hmac_chk(sk, opt_recv);
+				if(len){
+					struct my_sock  *my_sk = mpdccp_my_sock(sk);
+					u8 buf[MPDCCP_CONFIRM_SIZE];
+					len += 9;
+					memcpy(&buf, value-(len+3), len);
+
+					if(my_sk->cnf_cache_len + len < MPDCCP_CONFIRM_SIZE){
+						memcpy(&my_sk->cnf_cache[my_sk->cnf_cache_len], &buf, len);
+						my_sk->cnf_cache_len += len;
+						dccp_send_keepalive(sk);
+					}
+				}
+
+				break;
+
+			case DCCPO_MP_RTT:
+				if (len != 9) { //if not 9 byte
+					goto out_invalid_option;
+				}
+				opt_recv->dccpor_rtt_type = dccp_decode_value_var(value, 1);
+				value += 1;
+				opt_recv->dccpor_rtt_value = get_unaligned_be32(value);
+				value += 4;
+				opt_recv->dccpor_rtt_age = get_unaligned_be32(value);
+				dccp_pr_debug("rx opt: DCCPO_MP_RTT = type %u, value %d, age %d, sk %p",
+						opt_recv->dccpor_rtt_type, opt_recv->dccpor_rtt_value,
+						opt_recv->dccpor_rtt_age, sk);
+				break;
+
+			case DCCPO_MP_ADDADDR:
+				if(len == 5 || len == 7 || len == 17 || len == 19){
+					opt_recv->dccpor_addaddr_len = len+3;
+					memcpy(&opt_recv->dccpor_addaddr[0], value-3, opt_recv->dccpor_addaddr_len);
+				} else
+					goto out_invalid_option;
+				break;
+
+			case DCCPO_MP_REMOVEADDR:
+				if(len == 1){
+					memcpy(&opt_recv->dccpor_removeaddr[0], value-3, len+3);
+				} else
+					goto out_invalid_option;
+				break;
+
+			case DCCPO_MP_PRIO:
+				if (len == 1) {
+					u8 prio = dccp_decode_value_var(value, 1);
+					dccp_pr_debug("%s rx opt: DCCPO_MP_PRIO = value: %d, seq %llu",
+							dccp_role(sk), prio, (u64)opt_recv->dccpor_oall_seq);
+
+					if(prio < 16 && is_mpdccp(sk)){
+						struct mpdccp_cb *mpcb = get_mpcb(sk);
+						struct my_sock  *my_sk = mpdccp_my_sock(sk);
+
+						if (mpcb->pm_ops->rcv_prio_opt)
+							mpcb->pm_ops->rcv_prio_opt(sk, prio, opt_recv->dccpor_oall_seq);
+
+						len += 12;
+						if(my_sk->cnf_cache_len + len < MPDCCP_CONFIRM_SIZE){
+							memcpy(&my_sk->cnf_cache[my_sk->cnf_cache_len], value-12, len);
+							my_sk->cnf_cache_len += len;
+							dccp_send_keepalive(sk);
+						}
+					}
+				} else
+					goto out_invalid_option;
+				break;
+
+			case DCCPO_MP_CLOSE:
+				dccp_pr_debug("%s rx opt: DCCPO_MP_CLOSE = key %llx, len %u, sk %p",
+						dccp_role(sk), be64_to_cpu(*(u64*)value), len, sk);
+				if(is_mpdccp(sk) && len > 7){
+					struct mpdccp_cb *mpcb = get_mpcb(sk);
+					struct mpdccp_key *loc_key = &mpcb->mpdccp_loc_keys[mpcb->cur_key_idx];
+
+					mpdccp_my_sock(sk)->saw_mp_close = 1;
+					dccp_pr_debug("key: %llx", be64_to_cpu(*((__be64 *)loc_key->value)));
+
+					if(len != loc_key->size) break;
+					if(memcmp(value, loc_key->value, len)) {
+						DCCP_CRIT("MP_CLOSE keys dont match");
+						return -1;
+					}
+					mpdccp_close_meta(mpcb->meta_sk);
+				} else
+					goto out_invalid_option;
+				break;
+
+			default:
+				DCCP_CRIT("DCCP(%p): mp option %d(len=%d) not "
+					  "implemented, ignoring", sk, mp_opt, len);
+				break;
+			}
+
+#endif
 		case DCCPO_MIN_RX_CCID_SPECIFIC ... DCCPO_MAX_RX_CCID_SPECIFIC:
 			if (ccid_hc_rx_parse_options(dp->dccps_hc_rx_ccid, sk,
 						     pkt_type, opt, value, len))
@@ -546,15 +924,284 @@ static void dccp_insert_option_padding(struct sk_buff *skb)
 	}
 }
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+static int dccp_insert_option_multipath(struct sk_buff *skb, const unsigned char mp_option,
+		       const void *value, const unsigned char len)
+{
+	unsigned char *to;
+
+	if (DCCP_SKB_CB(skb)->dccpd_opt_len + len + 3 > DCCP_MAX_OPT_LEN)
+		return -1;
+
+	DCCP_SKB_CB(skb)->dccpd_opt_len += len + 3;
+
+	to    = skb_push(skb, len + 3);
+	*to++ = DCCPO_MULTIPATH;
+	*to++ = len + 3;
+	*to++ = mp_option;
+
+	memcpy(to, value, len);
+	return 0;
+}
+
+static int dccp_insert_option_mp_confirm(struct sk_buff *skb, u8 *buf, u8 len)
+{
+	return dccp_insert_option_multipath(skb, DCCPO_MP_CONFIRM, buf, len);
+}
+
+static int dccp_insert_option_mp_join(struct sk_buff *skb, u8 addr_id, u32 cix, u32 nonce)
+{
+	u8 buf[9];
+	buf[0] = addr_id;
+	put_unaligned_be32(cix, &buf[1]);
+	put_unaligned_be32(nonce, &buf[5]);
+	return dccp_insert_option_multipath(skb, DCCPO_MP_JOIN, &buf, sizeof(buf));
+}
+
+static int dccp_insert_option_mp_fast_close(struct sk_buff *skb, struct mpdccp_key *key)
+{
+	return dccp_insert_option_multipath(skb, DCCPO_MP_FAST_CLOSE, &key->value, key->size);
+}
+
+static int dccp_insert_option_mp_key(struct sk_buff *skb, struct mpdccp_cb *mpcb, struct dccp_request_sock *dreq)
+{
+	u8 buf[MPDCCP_MAX_KEYS*(MPDCCP_MAX_KEY_SIZE + 1) + 5];
+	int ret, i;
+	int optlen = 5;
+
+	buf[0] = 0;
+	switch (DCCP_SKB_CB(skb)->dccpd_type) {
+		case DCCP_PKT_REQUEST:
+			put_unaligned_be32(mpcb->mpdccp_loc_cix, &buf[1]);
+			for (i=0; i < MPDCCP_MAX_KEYS; i++) {
+				struct mpdccp_key *key = &mpcb->mpdccp_loc_keys[i];
+				if (mpcb && mpdccp_is_validkey(key)) {
+					buf[optlen] = key->type;
+					memcpy(&buf[optlen + 1], key->value, key->size);
+					optlen += key->size + 1;
+				}
+			}
+			break;
+		case DCCP_PKT_RESPONSE:
+			if (dreq && mpdccp_is_validkey(&dreq->mpdccp_loc_key)) {
+				put_unaligned_be32(dreq->mpdccp_loc_cix, &buf[1]);
+				struct mpdccp_key *key = &dreq->mpdccp_loc_key;
+				buf[optlen] = key->type;
+				memcpy(&buf[optlen + 1], key->value, key->size);
+				optlen += key->size + 1;
+			} else {
+				DCCP_WARN("MP_KEY: invalid input key for DCCP_PKT_RESPONSE\n");
+				ret = -1;
+			}
+			break;
+		default:
+			DCCP_WARN("MP_KEY: unsupported packet type %d\n", DCCP_SKB_CB(skb)->dccpd_type);
+			ret = -1;
+			break;
+	}
+	if (optlen > 5) {
+		ret = dccp_insert_option_multipath(skb, DCCPO_MP_KEY, &buf, optlen);
+	}
+	return ret;
+}
+
+/* Insert overall sequence number option */
+static int dccp_insert_option_mp_seq(struct sk_buff *skb, u64 *mp_oall_seq, bool do_incr_oallseq)
+{	
+	__be64 be_oall_seq;
+
+	if (do_incr_oallseq){
+		be_oall_seq = cpu_to_be64((*mp_oall_seq << 16));
+		dccp_inc_seqno(mp_oall_seq); // increment overall sequence number
+		//printk(KERN_INFO "insrt skb %p mp_seq %lu", skb, *mp_oall_seq);
+	}
+	else {
+		be_oall_seq = cpu_to_be64((DCCP_SKB_CB(skb)->dccpd_mpseq)<<16);
+		//printk(KERN_INFO "insrt_red skb %p mp_seq %lu", skb, DCCP_SKB_CB(skb)->dccpd_mpseq);
+	}
+	return dccp_insert_option_multipath(skb, DCCPO_MP_SEQ, &be_oall_seq, 6);
+}
+
+static int dccp_insert_option_mp_hmac(struct sk_buff *skb, u8 *hmac)
+{
+	return dccp_insert_option_multipath(skb, DCCPO_MP_HMAC, hmac, MPDCCP_HMAC_SIZE);
+}
+
+static int dccp_insert_option_mp_rtt(struct sk_buff *skb, u8 mp_rtt_type, u32 mp_rtt_value, u32 mp_rtt_age)
+{
+    u8 buf[9];
+	buf[0] = mp_rtt_type;
+	put_unaligned_be32(mp_rtt_value, &buf[1]);
+	put_unaligned_be32(mp_rtt_age, &buf[5]);
+	return dccp_insert_option_multipath(skb, DCCPO_MP_RTT, &buf, 9);
+}
+
+static int dccp_insert_option_mp_addaddr(struct sk_buff *skb, u8 *buf, struct sock *sk)
+{
+	get_mpcb(sk)->pm_ops->store_confirm_opt(sk, &buf[1], buf[1], DCCPO_MP_ADDADDR, buf[0]);
+	return dccp_insert_option_multipath(skb, DCCPO_MP_ADDADDR, &buf[1], buf[0]);
+}
+
+static int dccp_insert_option_mp_removeaddr(struct sk_buff *skb, u8 id, struct sock *sk)
+{
+	get_mpcb(sk)->pm_ops->store_confirm_opt(sk, &id, id, DCCPO_MP_REMOVEADDR, 1);
+	return dccp_insert_option_multipath(skb, DCCPO_MP_REMOVEADDR, &id, 1);
+}
+
+static int dccp_insert_option_mp_prio(struct sk_buff *skb, u8 prio, struct sock *sk)
+{
+	get_mpcb(sk)->pm_ops->store_confirm_opt(sk, &prio, get_id(sk), DCCPO_MP_PRIO, 1);
+	return dccp_insert_option_multipath(skb, DCCPO_MP_PRIO, &prio, 1);
+}
+
+static int dccp_insert_option_mp_close(struct sk_buff *skb, struct mpdccp_key *key)
+{
+	return dccp_insert_option_multipath(skb, DCCPO_MP_CLOSE, &key->value, key->size);
+}
+
+static int dccp_reinsert_option_mp(struct sk_buff *skb, u8 *buf)
+{
+	return dccp_insert_option_multipath(skb, buf[2], &buf[3], buf[1] - 3);
+}
+
+void dccp_insert_options_mp(struct sock *sk, struct sk_buff *skb)
+{
+	struct mpdccp_cb *mpcb = get_mpcb(sk);
+	struct my_sock  *my_sk = mpdccp_my_sock(sk);
+	u8 mp_addr_id = 0;
+
+	/* nothing to do here if my_sock is not available in sk_user_data */
+	if (my_sk == NULL)
+		return;
+
+	/* Skip if fallback to sp DCCP */
+	if (mpcb && mpcb->fallback_sp)
+		return;
+
+	mp_addr_id = get_id(sk);
+
+	/* Insert delay value (sub-flow specific) as DCCP option */
+	switch(DCCP_SKB_CB(skb)->dccpd_type){
+	case DCCP_PKT_DATAACK:
+		if(!(mpcb->mp_oall_seqno % 1)){				//try sending mp_rtt with every x dataack	
+			struct tcp_info info;
+			u8 rtt_type;
+			u32 rtt_value = get_delay_valn(sk, &info, &rtt_type);
+			u32 rtt_age = jiffies_to_msecs(info.tcpi_last_ack_recv);
+
+			if(rtt_value){
+				dccp_insert_option_mp_rtt(skb, rtt_type, rtt_value, rtt_age);
+				dccp_pr_debug("RTT: %u type: %u age: %u on socket (0x%p) loc_id: %u rem_id: %u",
+						rtt_value, rtt_type, rtt_age, sk, mp_addr_id, my_sk->remote_addr_id);
+			}
+		}
+		dccp_insert_option_mp_seq(skb, &mpcb->mp_oall_seqno, mpcb->do_incr_oallseq);
+		break;
+	case DCCP_PKT_DATA:
+		if(my_sk->delpath_id && mpcb->pm_ops->get_hmac){
+			u8 del_id = chk_id(my_sk->delpath_id, mpcb->master_addr_id);
+			/* dont send over path that is about to be removed */
+			if(del_id != mp_addr_id){
+				u8 delpath_hmac[MPDCCP_HMAC_SIZE];
+				mpcb->pm_ops->get_hmac(mpcb, del_id, 0, 0, 0, 1, delpath_hmac);
+				dccp_pr_debug("(%s) DATA insert opt MP_HMAC %llx", dccp_role(sk), be64_to_cpu(*((u64 *)delpath_hmac)));
+				dccp_insert_option_mp_hmac(skb, delpath_hmac);
+
+				dccp_pr_debug("(%s) DATA insert opt MP_REMOVEADDR, id: %u", dccp_role(sk), del_id);
+				dccp_insert_option_mp_removeaddr(skb, del_id, sk);
+				my_sk->delpath_id = 0;
+			}
+		}
+
+		else if(my_sk->addpath[0] && mpcb->pm_ops->get_hmac){
+			dccp_pr_debug("(%s) DATA insert opt MP_HMAC %llx", dccp_role(sk),
+					be64_to_cpu(*((u64 *)my_sk->addpath_hmac)));
+			dccp_insert_option_mp_hmac(skb, my_sk->addpath_hmac);
+
+			dccp_pr_debug("(%s) DATA insert opt MP_ADDADDR, id: %u, len: %u",
+					dccp_role(sk), my_sk->addpath[1], my_sk->addpath[0]);
+			dccp_insert_option_mp_addaddr(skb, my_sk->addpath, sk);
+			memset(my_sk->addpath, 0, MPDCCP_ADDADDR_SIZE);
+		}
+
+		else if(my_sk->announce_prio){
+			dccp_pr_debug("(%s) DATA insert opt MP_PRIO, prio: %u, sk: %p",
+					dccp_role(sk), my_sk->announce_prio - 1, sk);
+
+			dccp_insert_option_mp_prio(skb, my_sk->announce_prio - 1, sk);
+			my_sk->announce_prio = 0;
+		}
+
+		else if(my_sk->cnf_cache_len > 3){
+			dccp_pr_debug("(%s) DATA insert opt MP_CONFIRM, type: %u, len: %u",
+						dccp_role(sk), my_sk->cnf_cache[2], my_sk->cnf_cache_len);
+			dccp_insert_option_mp_confirm(skb, my_sk->cnf_cache, my_sk->cnf_cache_len);
+			my_sk->cnf_cache_len = 0;
+		}
+
+		else if(my_sk->reins_cache[0]){
+			dccp_pr_debug("(%s) REQ reinsert opt %u, addr_id: %u",
+					dccp_role(sk), my_sk->reins_cache[2], my_sk->reins_cache[3]);
+			dccp_reinsert_option_mp(skb, my_sk->reins_cache);
+		}
+
+		dccp_insert_option_mp_seq(skb, &mpcb->mp_oall_seqno, mpcb->do_incr_oallseq);
+		break;
+	case DCCP_PKT_REQUEST:
+		if (dccp_sk(sk)->is_kex_sk) {
+			dccp_pr_debug("(%s) REQ insert opt MP_KEY (supp)", dccp_role(sk));
+			/* Insert supported keys */
+			dccp_insert_option_mp_key(skb, mpcb, NULL);
+		} else {
+			/* Insert cix and nonce */
+			dccp_pr_debug("(%s) REQ insert opt MP_JOIN id:%u CI:%x nc:%x",
+							dccp_role(sk),
+							mp_addr_id,
+							mpcb->mpdccp_rem_cix,
+							dccp_sk(sk)->mpdccp_loc_nonce);
+			dccp_insert_option_mp_join(skb, mp_addr_id, mpcb->mpdccp_rem_cix, dccp_sk(sk)->mpdccp_loc_nonce);
+
+		}
+		break;
+	case DCCP_PKT_ACK:
+		if (!dccp_sk(sk)->auth_done && !dccp_sk(sk)->is_kex_sk) {
+			dccp_pr_debug("(%s) ACK insert opt MP_HMAC %llx", dccp_role(sk),be64_to_cpu(*((u64*)dccp_sk(sk)->mpdccp_loc_hmac)));
+			dccp_insert_option_mp_hmac(skb, dccp_sk(sk)->mpdccp_loc_hmac);
+		}
+		break;
+	case DCCP_PKT_CLOSE:
+		if(mpcb->to_be_closed || mpdccp_my_sock(sk)->saw_mp_close){
+			dccp_pr_debug("(%s) CLOSE insert opt MP_CLOSE", dccp_role(sk));
+			dccp_insert_option_mp_close(skb, &mpcb->mpdccp_rem_key);
+		}
+		break;
+	case DCCP_PKT_CLOSEREQ:
+		if(mpcb->to_be_closed && mpdccp_my_sock(sk)->closing == 1){
+			dccp_pr_debug("(%s) CLOSEREQ insert opt MP_CLOSE", dccp_role(sk));
+			dccp_insert_option_mp_close(skb, &mpcb->mpdccp_rem_key);
+		}
+		break;
+	case DCCP_PKT_RESET:
+		if(mpcb->to_be_closed && mpdccp_my_sock(sk)->closing == 2){
+			dccp_pr_debug("(%s) RESET insert opt MP_FAST_CLOSE", dccp_role(sk));
+			dccp_insert_option_mp_fast_close(skb, &mpcb->mpdccp_rem_key);
+		}
+		break;
+	default:
+		break;
+	}
+}
+#endif
+
 int dccp_insert_options(struct sock *sk, struct sk_buff *skb)
 {
 	struct dccp_sock *dp = dccp_sk(sk);
+	struct ccid2_hc_tx_sock *hc = NULL;
+	hc = ccid2_hc_tx_sk(sk);
 
 	DCCP_SKB_CB(skb)->dccpd_opt_len = 0;
-
 	if (dp->dccps_send_ndp_count && dccp_insert_option_ndp(sk, skb))
 		return -1;
-
 	if (DCCP_SKB_CB(skb)->dccpd_type != DCCP_PKT_DATA) {
 
 		/* Feature Negotiation */
@@ -585,6 +1232,14 @@ int dccp_insert_options(struct sock *sk, struct sk_buff *skb)
 	    dccp_insert_option_timestamp_echo(dp, NULL, skb))
 		return -1;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	/* Role dependent option required status. MPDCCP Server does not have
+	 * to add delay values since MPDCCP Client knows delays due to its
+	 * congestion control */
+	if (is_mpdccp(sk))
+		dccp_insert_options_mp(sk, skb);
+#endif
+	/* Insert padding to achieve option size as multiple of 32 bit (4 byte) */
 	dccp_insert_option_padding(skb);
 	return 0;
 }
@@ -607,3 +1262,50 @@ int dccp_insert_options_rsk(struct dccp_request_sock *dreq, struct sk_buff *skb)
 	dccp_insert_option_padding(skb);
 	return 0;
 }
+
+int dccp_insert_options_rsk_mp(const struct sock *sk, struct dccp_request_sock *dreq, struct sk_buff *skb)
+{
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	struct dccp_sock *dp = dccp_sk(sk);
+	struct dccp_options_received *opt_recv = &dp->dccps_options_received;
+
+	if (opt_recv->saw_mpjoin) {
+		union inet_addr addr;
+		struct mpdccp_cb *mpcb = get_mpcb(dreq->meta_sk);
+		int loc_id = 0;
+
+		if (!mpcb || mpcb->to_be_closed) {
+			dccp_pr_debug("(%s) invalid MPCB", dccp_role(sk));
+			return -1;
+		}
+
+		if(mpcb->pm_ops->get_id_from_ip){
+			/* Get local addr_id for response */
+			addr.ip = cpu_to_be32(opt_recv->dccpor_join_ip_local);
+			loc_id = mpcb->pm_ops->get_id_from_ip(mpcb, &addr, AF_INET, false);
+			if(loc_id < 0){
+				dccp_pr_debug("not able to figure out local address id for mp_join, continuing with id=0");
+				loc_id = 0;
+			}
+		}
+		if(mpcb->pm_ops->add_addr){
+			/* Store remote addr and id */
+			addr.ip = cpu_to_be32(opt_recv->dccpor_join_ip_remote);
+			mpcb->pm_ops->add_addr(mpcb, AF_INET, opt_recv->dccpor_join_id, &addr, opt_recv->dccpor_join_port, true);
+		}
+
+		/* Insert HMAC */
+		dccp_pr_debug("(%s) RES insert opt MP_HMAC %llx", dccp_role(sk), be64_to_cpu(*((u64 *)dreq->mpdccp_loc_hmac)));
+		dccp_insert_option_mp_hmac(skb, dreq->mpdccp_loc_hmac);
+		/* Insert cix and nonce */
+		dccp_pr_debug("(%s) RES insert opt MP_JOIN id:%u CI:%x nc:%x", dccp_role(sk), chk_id(loc_id, mpcb->master_addr_id), mpcb->mpdccp_loc_cix, dreq->mpdccp_loc_nonce);
+		dccp_insert_option_mp_join(skb, chk_id(loc_id, mpcb->master_addr_id), mpcb->mpdccp_rem_cix, dreq->mpdccp_loc_nonce);
+	} else if (opt_recv->saw_mpkey) {
+		dccp_pr_debug("(%s) RES insert opt MP_KEY %llx", dccp_role(sk), be64_to_cpu(*((__be64 *)dreq->mpdccp_loc_key.value)));
+		/* Insert local key */
+		dccp_insert_option_mp_key(skb, NULL, dreq);
+	}
+	dccp_insert_option_padding(skb);
+#endif
+	return 0;
+}
diff --git a/net/dccp/output.c b/net/dccp/output.c
index fd2eb148d24de..001a24df8706c 100644
--- a/net/dccp/output.c
+++ b/net/dccp/output.c
@@ -14,10 +14,16 @@
 
 #include <net/inet_sock.h>
 #include <net/sock.h>
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+#  include <net/mpdccp.h>
+#  include <net/mpdccp_meta.h>
+#endif
 
 #include "ackvec.h"
 #include "ccid.h"
 #include "dccp.h"
+#include "output.h"
+#include "feat.h"
 
 static inline void dccp_event_ack_sent(struct sock *sk)
 {
@@ -33,6 +39,35 @@ static struct sk_buff *dccp_skb_entail(struct sock *sk, struct sk_buff *skb)
 	return skb_clone(sk->sk_send_head, gfp_any());
 }
 
+static void dccp_update_skb_after_send(struct sock *sk, unsigned int len, u64 prior_wstamp) {
+	struct dccp_sock *dp = dccp_sk(sk);
+
+	if (sk->sk_pacing_status != SK_PACING_NONE) {
+		unsigned long rate = sk->sk_pacing_rate;
+
+		/* Original sch_fq does not pace first 10 MSS */
+		if (rate != ~0UL && rate && dp->data_segs_out >= 10) {
+			u64 len_ns = div64_u64(len * NSEC_PER_SEC, rate);
+			u64 credit = dp->dccps_wstamp_ns - prior_wstamp;
+
+			/* take into account OS jitter */
+			len_ns -= min_t(u64, len_ns / 2, credit);
+			dp->dccps_wstamp_ns += len_ns;
+		}
+	}
+}
+
+/* Refresh clocks of a DCCP socket,
+ * ensuring monotically increasing values.
+ */
+static void dccp_mstamp_refresh(struct dccp_sock *dp)
+{
+	u64 val = tcp_clock_ns();
+
+	dp->dccps_clock_cache = val;
+	dp->dccps_mstamp = div_u64(val, NSEC_PER_USEC);
+}
+
 /*
  * All SKB's seen here are completely headerless. It is our
  * job to build the DCCP header, and pass the packet down to
@@ -47,6 +82,11 @@ static int dccp_transmit_skb(struct sock *sk, struct sk_buff *skb)
 		struct dccp_sock *dp = dccp_sk(sk);
 		struct dccp_skb_cb *dcb = DCCP_SKB_CB(skb);
 		struct dccp_hdr *dh;
+		unsigned int len = skb->len;
+
+		u64 prior_wstamp = dp->dccps_wstamp_ns;
+		dp->dccps_wstamp_ns = max(dp->dccps_wstamp_ns, dp->dccps_clock_cache); // alerab: check this
+    
 		/* XXX For now we're using only 48 bits sequence numbers */
 		const u32 dccp_header_size = sizeof(*dh) +
 					     sizeof(struct dccp_hdr_ext) +
@@ -58,12 +98,14 @@ static int dccp_transmit_skb(struct sock *sk, struct sk_buff *skb)
 		 * Update GSS for real only if option processing below succeeds.
 		 */
 		dcb->dccpd_seq = ADD48(dp->dccps_gss, 1);
-
+		
 		switch (dcb->dccpd_type) {
 		case DCCP_PKT_DATA:
 			set_ack = 0;
+			dp->data_segs_out++;
 			fallthrough;
 		case DCCP_PKT_DATAACK:
+			dp->data_segs_out++;
 		case DCCP_PKT_RESET:
 			break;
 
@@ -96,6 +138,7 @@ static int dccp_transmit_skb(struct sock *sk, struct sk_buff *skb)
 		}
 
 
+
 		/* Build DCCP header and checksum it. */
 		dh = dccp_zeroed_hdr(skb, dccp_header_size);
 		dh->dccph_type	= dcb->dccpd_type;
@@ -136,10 +179,17 @@ static int dccp_transmit_skb(struct sock *sk, struct sk_buff *skb)
 		DCCP_INC_STATS(DCCP_MIB_OUTSEGS);
 
 		err = icsk->icsk_af_ops->queue_xmit(sk, skb, &inet->cork.fl);
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+		dp->dccps_lsndtime = tcp_jiffies32;
+#endif
+		if (!err) /* alerab: tcp uses a copy of skb, but we only need the len */
+			dccp_update_skb_after_send(sk, len, prior_wstamp);
+
 		return net_xmit_eval(err);
 	}
 	return -ENOBUFS;
 }
+EXPORT_SYMBOL_GPL(dccp_transmit_skb);
 
 /**
  * dccp_determine_ccmps  -  Find out about CCID-specific packet-size limits
@@ -278,6 +328,7 @@ static void dccp_xmit_packet(struct sock *sk)
 		DCCP_SKB_CB(skb)->dccpd_type = DCCP_PKT_DATA;
 	}
 
+	dccp_mstamp_refresh(dp); // alerab
 	err = dccp_transmit_skb(sk, skb);
 	if (err)
 		dccp_pr_debug("transmit_skb() returned err=%d\n", err);
@@ -351,9 +402,18 @@ void dccp_write_xmit(struct sock *sk)
 {
 	struct dccp_sock *dp = dccp_sk(sk);
 	struct sk_buff *skb;
+	int		rc;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if (mpdccp_is_meta (sk)) {
+		rc = mpdccp_write_xmit (sk);
+		if (rc < 0)
+			dccp_pr_debug("error in mpdccp_write_xmit: %d", rc);
+		return;
+	}
+#endif
 	while ((skb = dccp_qpolicy_top(sk))) {
-		int rc = ccid_hc_tx_send_packet(dp->dccps_hc_tx_ccid, sk, skb);
+		rc = ccid_hc_tx_send_packet(dp->dccps_hc_tx_ccid, sk, skb);
 
 		switch (ccid_packet_dequeue_eval(rc)) {
 		case CCID_PACKET_WILL_DEQUEUE_LATER:
@@ -371,6 +431,7 @@ void dccp_write_xmit(struct sock *sk)
 		}
 	}
 }
+EXPORT_SYMBOL_GPL(dccp_write_xmit);
 
 /**
  * dccp_retransmit_skb  -  Retransmit Request, Close, or CloseReq packets
@@ -432,6 +493,17 @@ struct sk_buff *dccp_make_response(const struct sock *sk, struct dst_entry *dst,
 	if (dccp_insert_options_rsk(dreq, skb))
 		goto response_failed;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if ((mpdccp_isactive(sk) > 0) && dreq && (dreq->multipath_ver != MPDCCP_VERS_UNDEFINED)) {
+		if (dreq->meta_sk && dreq->meta_sk->sk_state != DCCP_OPEN) {
+			dccp_pr_debug("meta socket %p not in OPEN state during response\n", dreq->meta_sk);
+			goto response_failed;
+		}
+		if (dccp_insert_options_rsk_mp(sk, dreq, skb))
+			goto response_failed;
+	}
+#endif
+
 	/* Build and checksum header */
 	dh = dccp_zeroed_hdr(skb, dccp_header_size);
 
@@ -514,6 +586,7 @@ EXPORT_SYMBOL_GPL(dccp_ctl_make_reset);
 int dccp_send_reset(struct sock *sk, enum dccp_reset_codes code)
 {
 	struct sk_buff *skb;
+  struct dccp_sock *dp = dccp_sk(sk);
 	/*
 	 * FIXME: what if rebuild_header fails?
 	 * Should we be doing a rebuild_header here?
@@ -532,6 +605,7 @@ int dccp_send_reset(struct sock *sk, enum dccp_reset_codes code)
 	DCCP_SKB_CB(skb)->dccpd_type	   = DCCP_PKT_RESET;
 	DCCP_SKB_CB(skb)->dccpd_reset_code = code;
 
+  dccp_mstamp_refresh(dp);
 	return dccp_transmit_skb(sk, skb);
 }
 
@@ -566,6 +640,7 @@ int dccp_connect(struct sock *sk)
 
 	DCCP_SKB_CB(skb)->dccpd_type = DCCP_PKT_REQUEST;
 
+  dccp_mstamp_refresh(dp);
 	dccp_transmit_skb(sk, dccp_skb_entail(sk, skb));
 	DCCP_INC_STATS(DCCP_MIB_ACTIVEOPENS);
 
@@ -580,10 +655,11 @@ EXPORT_SYMBOL_GPL(dccp_connect);
 
 void dccp_send_ack(struct sock *sk)
 {
+	struct sk_buff *skb;
+	int err;
 	/* If we have been reset, we may not send again. */
 	if (sk->sk_state != DCCP_CLOSED) {
-		struct sk_buff *skb = alloc_skb(sk->sk_prot->max_header,
-						GFP_ATOMIC);
+		skb = alloc_skb(sk->sk_prot->max_header, GFP_ATOMIC);
 
 		if (skb == NULL) {
 			inet_csk_schedule_ack(sk);
@@ -597,12 +673,76 @@ void dccp_send_ack(struct sock *sk)
 		/* Reserve space for headers */
 		skb_reserve(skb, sk->sk_prot->max_header);
 		DCCP_SKB_CB(skb)->dccpd_type = DCCP_PKT_ACK;
-		dccp_transmit_skb(sk, skb);
+		err = dccp_transmit_skb(sk, skb);
 	}
 }
 
 EXPORT_SYMBOL_GPL(dccp_send_ack);
 
+void dccp_send_ack_entail(struct sock *sk)
+{
+	struct sk_buff *skb;
+
+	/* If we have been reset, we may not send again. */
+	if (sk->sk_state != DCCP_CLOSED) {
+		skb = alloc_skb(sk->sk_prot->max_header, GFP_ATOMIC);
+
+		if (skb == NULL) {
+			inet_csk_schedule_ack(sk);
+			inet_csk(sk)->icsk_ack.ato = TCP_ATO_MIN;
+			inet_csk_reset_xmit_timer(sk, ICSK_TIME_DACK,
+						  TCP_DELACK_MAX,
+						  DCCP_RTO_MAX);
+			return;
+		}
+
+		/* Reserve space for headers */
+		skb_reserve(skb, sk->sk_prot->max_header);
+		DCCP_SKB_CB(skb)->dccpd_type = DCCP_PKT_ACK;
+
+
+		skb = dccp_skb_entail(sk, skb);
+		inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
+					  DCCP_TIMEOUT_INIT, DCCP_RTO_MAX);
+
+		dccp_transmit_skb(sk, skb);
+	}
+}
+
+EXPORT_SYMBOL_GPL(dccp_send_ack_entail);
+
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+void dccp_send_keepalive(struct sock *sk)
+{
+	struct sk_buff *skb;
+	int err;
+
+	/* If we have been reset, we may not send again. */
+	dccp_pr_debug("enter dccp_send_keepalive %p", sk);
+	if (sk->sk_state != DCCP_CLOSED) {
+		dccp_pr_debug("enter dccp_send_ack if not CLOSED %p", sk);
+		skb = alloc_skb(sk->sk_prot->max_header, GFP_ATOMIC);
+
+		if (skb == NULL) {
+			dccp_pr_debug("enter dccp_send_keepalive if skb NULL %p", sk);
+			return;
+		}
+
+		/* Reserve space for headers */
+		skb_reserve(skb, sk->sk_prot->max_header);
+		DCCP_SKB_CB(skb)->dccpd_type = DCCP_PKT_DATA;
+		dccp_pr_debug("enter dccp_send_ack just be transmit");
+    
+		dccp_mstamp_refresh(dccp_sk(sk));
+    
+		err = dccp_transmit_skb(sk, skb);
+		}
+
+}
+EXPORT_SYMBOL_GPL(dccp_send_keepalive);
+#endif
+
+
 #if 0
 /* FIXME: Is this still necessary (11.3) - currently nowhere used by DCCP. */
 void dccp_send_delayed_ack(struct sock *sk)
@@ -639,6 +779,7 @@ void dccp_send_delayed_ack(struct sock *sk)
 void dccp_send_sync(struct sock *sk, const u64 ackno,
 		    const enum dccp_pkt_type pkt_type)
 {
+  struct dccp_sock *dp = dccp_sk(sk);
 	/*
 	 * We are not putting this on the write queue, so
 	 * dccp_transmit_skb() will set the ownership to this
@@ -661,8 +802,9 @@ void dccp_send_sync(struct sock *sk, const u64 ackno,
 	 * Clear the flag in case the Sync was scheduled for out-of-band data,
 	 * such as carrying a long Ack Vector.
 	 */
-	dccp_sk(sk)->dccps_sync_scheduled = 0;
+	dp->dccps_sync_scheduled = 0;
 
+  dccp_mstamp_refresh(dp);
 	dccp_transmit_skb(sk, skb);
 }
 
diff --git a/net/dccp/output.h b/net/dccp/output.h
new file mode 100644
index 0000000000000..35f969d7255c2
--- /dev/null
+++ b/net/dccp/output.h
@@ -0,0 +1,21 @@
+/*
+ *  net/dccp/output.c
+ *
+ *  An implementation of the DCCP protocol
+ *  Arnaldo Carvalho de Melo <acme@conectiva.com.br>
+ *
+ *	This header file by
+ *	Andreas Philipp Matz <info@andreasmatz.de>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	as published by the Free Software Foundation; either version
+ *	2 of the License, or (at your option) any later version.
+ */
+
+#ifndef __DCCP_OUTPUT_H
+#define __DCCP_OUTPUT_H
+
+//static int dccp_transmit_skb(struct sock *sk, struct sk_buff *skb);
+
+#endif /* __DCCP_OUTPUT_H */
\ No newline at end of file
diff --git a/net/dccp/pm/Kconfig b/net/dccp/pm/Kconfig
new file mode 100644
index 0000000000000..346497c51cda7
--- /dev/null
+++ b/net/dccp/pm/Kconfig
@@ -0,0 +1,5 @@
+
+config DEFAULT_MPDCCP_PM
+	string
+	depends on IP_MPDCCP
+	default "default"
diff --git a/net/dccp/pm/Makefile b/net/dccp/pm/Makefile
new file mode 100644
index 0000000000000..d1558d84eeaed
--- /dev/null
+++ b/net/dccp/pm/Makefile
@@ -0,0 +1,2 @@
+# SPDX-License-Identifier: GPL-2.0
+
diff --git a/net/dccp/pm/pm_default.c b/net/dccp/pm/pm_default.c
new file mode 100644
index 0000000000000..8976f9d1b709b
--- /dev/null
+++ b/net/dccp/pm/pm_default.c
@@ -0,0 +1,1682 @@
+/*
+ * MPDCCP - Path manager architecture
+ *
+ * A flexible architecture to load arbitrary path managers. The
+ * default path manager does nothing.
+ *
+ * The code in this file is partly derived from the MPTCP project's 
+ * mptcp_pm.c and mptcp_fullmesh.c. Derived code is Copyright (C) 
+ * the original authors Christoph Paasch et al.
+ *
+ * Copyright (C) 2018 Andreas Philipp Matz <info@andreasmatz.de>
+ * Copyright (C) 2020 Frank Reker <frank@reker.net>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/module.h>
+#include <linux/inetdevice.h>
+#include <linux/netdevice.h>
+#include <linux/proc_fs.h>
+#include <net/addrconf.h>
+#include <net/net_namespace.h>
+#include <net/netns/mpdccp.h>
+#include <asm/unaligned.h>
+
+#include "../ccids/ccid2.h"
+#include "../dccp.h"
+#include <net/mpdccp_link.h>
+#include <net/mpdccp.h>
+#include "../mpdccp.h"
+#include "../mpdccp_pm.h"
+
+static struct kmem_cache *mpdccp_pm_addr_cache __read_mostly;
+static struct mpdccp_pm_ops mpdccp_pm_default;
+
+//retransmit unconfirmed options after this many millisecs
+#define MPDCCP_CONFIRM_RETRANSMIT_TIMEOUT msecs_to_jiffies(1000)
+#define MPDCCP_CONFIRM_RETRANSMIT_TRIES	5
+
+enum {
+	MPDCCP_EVENT_ADD = 1,
+	MPDCCP_EVENT_DEL,
+	MPDCCP_EVENT_MOD,
+};
+
+struct pm_local_addr_event {
+	struct list_head list;
+	unsigned short	family;
+	u8	code;
+	int	if_idx;
+	union inet_addr addr;
+};
+
+struct mpdccp_confirm_opt {
+	u8 opt[MPDCCP_CONFIRM_SIZE];
+	u8 resent_cnt;
+	u32 t_init;
+	u32 t_timeout;
+};
+
+struct pm_retransmit_event {
+	struct list_head list;
+	struct sock *sk;
+	struct mpdccp_confirm_opt *cnf_opt;
+};
+
+/* Holds a single local interface address */
+struct pm_local_addr {
+	struct list_head address_list;
+
+	/* Address family, IPv4/v6 address, Interface ID */
+	sa_family_t family;
+	union inet_addr addr;
+	int if_idx;
+	u8 id;
+
+	struct rcu_head rcu;
+};
+
+struct mpdccp_addr {
+	struct list_head address_list;
+
+	struct mpdccp_confirm_opt cnf_addaddr;
+	struct mpdccp_confirm_opt cnf_remaddr;
+	struct mpdccp_confirm_opt cnf_prio;
+
+	bool remote;
+	sa_family_t family;
+	union inet_addr addr;
+	u16 port;
+	u8 id;
+
+	struct rcu_head rcu;
+};
+
+static struct mpdccp_pm_ns *fm_get_ns(const struct net *net)
+{
+	/* TAG-0.8: Migrate to list implementation */
+	return (struct mpdccp_pm_ns *)net->mpdccp.path_managers[MPDCCP_PM_FULLMESH];
+}
+
+/* calculate hmac for mp_addaddr and mp_removeaddr */
+static int pm_get_addr_hmac(struct mpdccp_cb *mpcb,
+							u8 id, sa_family_t family,
+							union inet_addr *addr, u16 port,
+							bool send, u8 *hmac)
+{
+	u8 msg[19];		//1:id + 16:ipv6 + 2:port
+	int len = 1;
+	msg[0] = id;
+
+	if(family == AF_INET){
+		put_unaligned_be32(addr->ip, &msg[1]);
+		put_unaligned_be16(port, &msg[5]);
+		len = 7;
+	} else if(family == AF_INET6){
+		memcpy(&msg[1], addr->ip6, 16);
+		put_unaligned_be16(port, &msg[17]);
+		len = 19;
+	}
+	if((send && mpcb->role == MPDCCP_CLIENT) || (!send && mpcb->role != MPDCCP_CLIENT))
+		return mpdccp_hmac_sha256(mpcb->dkeyA, mpcb->dkeylen, msg, len, hmac);
+	else
+		return mpdccp_hmac_sha256(mpcb->dkeyB, mpcb->dkeylen, msg, len, hmac);
+}
+
+/* triggers MP_REMOVEADDR option with next packet */
+static void mpdccp_send_remove_path(struct mpdccp_cb *mpcb, u8 addr_id)
+{
+	struct sock *sk = mpdccp_select_ann_sock(mpcb, addr_id);
+	if (sk){
+		mpdccp_my_sock(sk)->delpath_id = addr_id;
+		dccp_send_keepalive(sk);
+	}
+}
+
+/* triggers MP_ADDADDR option with next packet */
+static void mpdccp_send_add_path(struct pm_local_addr *loc_addr, u16 port, struct mpdccp_cb *mpcb)
+{
+	struct sock *sk = mpdccp_select_ann_sock(mpcb, 0);
+	int len = 1;
+	u8 *buf = mpdccp_my_sock(sk)->addpath;
+	/* since MP_ADDADDR is variable in size buf[0] will hold the length of the option 
+		buf[1] -> buf[n] will hold the contents of the option */
+
+	if (!sk) return;
+
+	buf[len] = loc_addr->id;
+	len++;
+	if(loc_addr->family == AF_INET){
+		put_unaligned_be32(loc_addr->addr.ip, &buf[len]);
+		len += 4;
+	} else if(loc_addr->family == AF_INET6){
+		memcpy(&buf[len], loc_addr->addr.ip6, 16);
+		len += 16;
+	}
+
+	if(port){
+		put_unaligned_be16(port, &buf[len]);
+		len += 2;
+	}
+	buf[0] = len - 1;
+
+	pm_get_addr_hmac(mpcb, loc_addr->id, loc_addr->family, &loc_addr->addr, 
+			port, 1, mpdccp_my_sock(sk)->addpath_hmac);
+
+	dccp_send_keepalive(sk);
+}
+
+/* Use ip routing functions to figure out default source address and store address in mpcb*/
+static void mpdccp_get_mpcb_local_address(struct mpdccp_cb *mpcb, struct sockaddr_in *nexthop)
+{
+	struct sockaddr_in sin;
+	struct sock *sk = mpcb->meta_sk;
+
+	/* check if socket was bound to local ip address,
+		otherwise use route.h function for local routing default route */
+	if(sk && sk->__sk_common.skc_rcv_saddr){
+		sin.sin_addr.s_addr = mpcb->meta_sk->__sk_common.skc_rcv_saddr;
+	} else {
+		struct flowi4 *fl4;
+		struct inet_sock *inet = inet_sk(sk);
+		fl4 = &inet->cork.fl.u.ip4;
+		ip_route_connect(fl4, nexthop->sin_addr.s_addr, inet->inet_saddr, RT_CONN_FLAGS(sk),
+				sk->sk_bound_dev_if, IPPROTO_DCCP, inet->inet_sport, nexthop->sin_port, sk);
+		sin.sin_addr.s_addr = fl4->saddr;
+	}
+	memcpy(&mpcb->mpdccp_local_addr, &sin, sizeof(struct sockaddr_in));
+	mpcb->localaddr_len = sizeof(struct sockaddr_in);
+	mpcb->has_localaddr = 1;
+}
+
+/* stores address to paddress_list if not already in list */
+static void pm_add_addr(struct mpdccp_cb *mpcb, sa_family_t family, u8 id, union inet_addr *addr, u16 port, bool is_remote)
+{
+	struct mpdccp_addr *mp_addr;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(mp_addr, &mpcb->paddress_list, address_list) {
+		if (mp_addr->remote == is_remote && family == mp_addr->family) {		//allows to have ipv6 and ipv4 with the same id in memory
+			if (id == mp_addr->id ||								//does the id exist?
+				(family == AF_INET  &&
+				mp_addr->addr.in.s_addr == addr->in.s_addr) ||		//does the ipv4 exist?
+				(family == AF_INET6 &&
+				ipv6_addr_equal(&mp_addr->addr.in6, &addr->in6)))	//does the ipv6 exist?
+			{
+				mpdccp_pr_debug("already have an entry for %s address %pI4, id: %u", 
+							(is_remote ? "remote" : "local"), &addr->in.s_addr, id);
+				return;
+			}
+		}
+	}
+
+	/* not in list add new entry */
+	mp_addr = kzalloc(sizeof(*mp_addr), GFP_KERNEL);
+	mp_addr->remote = is_remote;
+	mp_addr->family = family;
+	mp_addr->id = id;
+	mp_addr->port = port;
+
+	if (family == AF_INET) {
+		mp_addr->addr.in.s_addr = addr->in.s_addr;
+		mpdccp_pr_debug("Stored new %s IP %pI4:%u with id: %u", 
+				(is_remote ? "remote" : "local"), &addr->in, htons((unsigned)port), id);
+	} else {
+		mp_addr->addr.in6 = addr->in6;
+		mpdccp_pr_debug("Stored new %s IP %pI6:%u with id: %u", 
+				(is_remote ? "remote" : "local"), &addr->in6, htons((unsigned)port), id);
+	}
+	
+	list_add_tail_rcu(&mp_addr->address_list, &mpcb->paddress_list);
+	if(is_remote) mpcb->cnt_remote_addrs++;
+	rcu_read_unlock();
+	return;
+}
+
+/* remove id from address list, flushing the entire list is also possible */
+static void pm_del_addr(struct mpdccp_cb *mpcb, u8 id, bool is_remote, bool flush)
+{
+	struct mpdccp_addr *mp_addr;
+	if(!is_remote && !id) id = mpcb->master_addr_id;
+	mpdccp_pr_debug("trying to remove address id %u from addr memory", id);
+	list_for_each_entry_rcu(mp_addr, &mpcb->paddress_list, address_list) {
+		if( flush || (mp_addr->remote == is_remote && mp_addr->id == id)) {
+			mpdccp_pr_debug("removing %s address %pI4", 
+					(mp_addr->remote ? "remote" : "local"), &mp_addr->addr.ip);
+
+			list_del_rcu(&mp_addr->address_list);
+			kfree_rcu(mp_addr, rcu);
+			if(is_remote)
+				mpcb->cnt_remote_addrs--;
+		}
+	}
+}
+
+static int pm_get_id_from_ip(struct mpdccp_cb *mpcb, union inet_addr *addr, sa_family_t family, bool is_remote)
+{
+	struct mpdccp_addr *mp_addr;
+	list_for_each_entry_rcu(mp_addr, &mpcb->paddress_list, address_list) {
+		if(!mp_addr->remote == is_remote) continue;
+		if(family == mp_addr->family){
+			if((family == AF_INET && addr->ip == mp_addr->addr.ip) || 
+						(family == AF_INET6 && addr->ip6 == mp_addr->addr.ip6))
+				return mp_addr->id;
+		}
+	}
+	return -1;
+}
+
+/*function that copies address with given ip from pathmanager namespace to mpcb address list, returns id */
+static int pm_claim_local_addr(struct mpdccp_cb *mpcb, sa_family_t family, union inet_addr *addr)
+{
+	struct mpdccp_pm_ns *pm_ns = fm_get_ns(sock_net(mpcb->meta_sk));
+	struct pm_local_addr *local_addr;
+	bool found;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(local_addr, &pm_ns->plocal_addr_list, address_list) {
+		if (local_addr && family == local_addr->family &&
+					local_addr->addr.in.s_addr == addr->in.s_addr) {
+			found = true;
+			break;
+		}
+	}
+	rcu_read_unlock();
+	
+	if(!local_addr || !found){
+		mpdccp_pr_debug("pm was unable to claim address %pI4", &addr->in.s_addr);
+		return 0;
+	}
+
+	pm_add_addr(mpcb, local_addr->family, local_addr->id, &local_addr->addr, 0, false);
+	return local_addr->id;
+}
+
+/* remove id from list and closes all subflows with remote id == id learned from MP_REMOVEADDR option*/
+static void pm_handle_rm_addr(struct mpdccp_cb *mpcb, u8 id)
+{
+	struct sock *sk;
+	rcu_read_lock();
+	/* remove all sockets with id remote_id from subflow list */
+	mpdccp_for_each_sk(mpcb, sk) {
+		if(mpdccp_my_sock(sk)->remote_addr_id == id){
+			/* when we receive MP_REMOVEADDR the subflow is already dead */
+			mpdccp_close_subflow(mpcb, sk, 2);
+			mpdccp_pr_debug("deleting path with id: %u sk %p", id, sk);
+		}
+	}
+	//remove id from list
+	pm_del_addr(mpcb, id, true, false);
+	rcu_read_unlock();
+}
+
+/* Function is called when receiving mp_confirm for mp_removeaddr.
+ * It checks if it possible to free the global address id of the removed subflow */
+static void pm_free_id(const struct sock *meta_sk, u8 id) {
+	struct mpdccp_cb *mpcb;
+	struct sock *sk;
+	struct mpdccp_pm_ns *pm_ns;
+
+	mpdccp_for_each_conn(pconnection_list, mpcb) {
+		mpdccp_for_each_sk(mpcb, sk) {
+			if(mpdccp_my_sock(sk)->local_addr_id == id){
+				mpdccp_pr_debug("socket still in use cant free id: %u", id);
+				return;
+			}
+		}
+	}
+
+	pm_ns = fm_get_ns(sock_net(meta_sk));
+	spin_lock(&pm_ns->plocal_lock);
+	mpdccp_pr_debug("loc4_bits %llu removing id: %u", pm_ns->loc4_bits, id);
+	pm_ns->loc4_bits &= ~(1 << (id-1));
+	spin_unlock(&pm_ns->plocal_lock);
+}
+
+/*  handle received mp_prio option - clone link and set prio */
+static void pm_handle_rcv_prio(struct sock *sk, u8 prio, u64 seq)
+{
+	struct mpdccp_link_info *link;
+
+	rcu_read_lock();
+	link = mpdccp_ctrl_getlink(sk);
+
+	if(!link || prio == link->mpdccp_prio || 
+			seq < mpdccp_my_sock(sk)->last_prio_seq || !mpdccp_accept_prio){
+		if(!mpdccp_accept_prio)
+			mpdccp_pr_debug("mpdccp configured to ignore incoming mp_prio options");
+		if(seq < mpdccp_my_sock(sk)->last_prio_seq)
+			mpdccp_pr_debug("outdated mp_prio option detected");
+
+		mpdccp_link_put(link);
+		rcu_read_unlock();
+		return;
+	}
+
+	mpdccp_pr_debug("assigning prio %u - old prio %u to sk (%p)",
+			prio, link->mpdccp_prio, sk);
+	mpdccp_my_sock(sk)->prio_rcvrd = true;
+
+	if(link->is_devlink)		// create copy and change prio of new copy
+		mpdccp_link_cpy_set_prio(sk, prio); 
+	else						// change prio of this (virtual) link
+		link->mpdccp_prio = prio;
+
+	mpdccp_my_sock(sk)->last_prio_seq = seq;
+	mpdccp_link_put(link);
+	rcu_read_unlock();
+}
+
+static int pm_handle_link_event(struct notifier_block *this,
+				   unsigned long event, void *ptr)
+{
+	if (event != MPDCCP_LINK_CHANGE_PRIO)
+		return NOTIFY_DONE;
+	else {
+		struct mpdccp_link_notifier_info *lni = ptr;
+		struct mpdccp_link_info *link = lni->link_info;
+		struct sock *sk;
+		struct mpdccp_cb *mpcb;
+
+		rcu_read_lock();
+		mpdccp_for_each_conn(pconnection_list, mpcb) {
+			if(mpcb->fallback_sp) continue;
+			mpdccp_for_each_sk(mpcb, sk) {
+				if(!mpdccp_my_sock(sk)->prio_rcvrd && 
+					    mpdccp_my_sock(sk)->link_info->id == link->id)
+					mpdccp_init_announce_prio(sk);
+				else
+					mpdccp_my_sock(sk)->prio_rcvrd = false;
+			}
+		}
+		rcu_read_unlock();
+		return NOTIFY_DONE;
+	}
+}
+
+static struct notifier_block mpdccp_pm_link_notifier = {
+	.notifier_call = pm_handle_link_event,
+};
+
+
+/* this function handles the retransmission queue */
+static void pm_retransmit_worker(struct work_struct *work)
+{
+	const struct delayed_work *delayed_work = container_of(work, struct delayed_work, work);
+	struct mpdccp_pm_ns *pm_ns = container_of(delayed_work, struct mpdccp_pm_ns, retransmit_worker);
+	struct pm_retransmit_event *event;
+	struct mpdccp_confirm_opt *my_opt;
+	const u32 now = pm_jiffies32;
+
+next_event:
+	event = list_first_entry_or_null(&pm_ns->retransmit, struct pm_retransmit_event, list);
+	if (!event)
+		return;
+
+	if(event->sk->sk_state != DCCPF_OPEN){
+		list_del_rcu(&event->list);
+		kfree(event);
+		return;
+	}
+
+	my_opt = event->cnf_opt;
+/* we loop through the list and check if retransmission timeout has surpassed */
+	if (now > my_opt->t_timeout){
+		u32 delay, age = now - my_opt->t_init;
+
+		/* initiate retransmission of option */
+		memcpy(&mpdccp_my_sock(event->sk)->reins_cache, &my_opt->opt[9], my_opt->opt[10]);
+		dccp_send_keepalive(event->sk);
+		my_opt->resent_cnt++;
+
+		mpdccp_pr_debug("Retransmitting (x%u) unconfirmed option after: %ums, type: %u",
+				my_opt->resent_cnt, jiffies_to_msecs(age), my_opt->opt[11]);
+
+		/* check if we still have retransmit tries. if yes, keep event in list and move to tail */
+		if (my_opt->resent_cnt < MPDCCP_CONFIRM_RETRANSMIT_TRIES){
+			/* calculate new timeout based of original timestamp(age) to remove */
+			delay = (my_opt->resent_cnt + 2) * MPDCCP_CONFIRM_RETRANSMIT_TIMEOUT - age;
+			my_opt->t_timeout = now + delay;
+
+			/* now calculate timeout for next event in list */
+			if (!list_is_singular(&pm_ns->retransmit))
+				list_move_tail(&event->list, &pm_ns->retransmit);
+		} else {
+			/* no retransmit tries left, del event */
+			list_del_rcu(&event->list);
+			kfree(event);
+		}
+		goto next_event;
+	} else
+		/* set new work queue timeout for remaining elements in list and exit */
+		queue_delayed_work(mpdccp_wq, &pm_ns->retransmit_worker, event->cnf_opt->t_timeout - now);
+}
+
+/*	this function queues a new retransmission event */
+static void pm_add_insert_rt_event(struct sock *sk, struct mpdccp_confirm_opt *my_opt){
+	struct mpdccp_cb *mpcb = get_mpcb(sk);
+	struct mpdccp_pm_ns *pm_ns = fm_get_ns(sock_net(mpcb->meta_sk));
+	struct pm_retransmit_event *event = kzalloc(sizeof(*event), GFP_KERNEL);
+
+	event->cnf_opt = my_opt;
+	event->sk = sk;
+	list_add_tail_rcu(&event->list, &pm_ns->retransmit);
+	mpdccp_pr_debug("Added retransmission event to queue");
+
+	/* Create work-queue */
+	if (!delayed_work_pending(&pm_ns->retransmit_worker))
+		queue_delayed_work(mpdccp_wq, &pm_ns->retransmit_worker, MPDCCP_CONFIRM_RETRANSMIT_TIMEOUT);
+}
+
+/*  remove option from reinsertion list. either because it was confirmed or because sk is closing
+	if my_opt is used, we only remove this specific option from list
+	if sk is used, we remove all options linked to sk from list
+	if we remove the first option on the list we also cancel the delayed work */
+static void pm_remove_rt_event(struct net *net, struct sock *sk, struct mpdccp_confirm_opt *my_opt){
+	struct mpdccp_pm_ns *pm_ns = fm_get_ns(net);
+	struct pm_retransmit_event *event;
+	bool first = true;
+
+next_event:
+
+	event = list_first_entry_or_null(&pm_ns->retransmit, struct pm_retransmit_event, list);
+	if(!event)
+		return;
+
+	if(event->cnf_opt == my_opt || event->sk == sk){
+		list_del_rcu(&event->list);
+		kfree(event);
+
+		if (first && delayed_work_pending(&pm_ns->retransmit_worker))
+			cancel_delayed_work(&pm_ns->retransmit_worker);
+
+		first = false;
+		mpdccp_pr_debug("Removed retransmission event");
+	}
+	goto next_event;
+}
+
+static void pm_del_retrans(struct net *net, struct sock *sk){
+	pm_remove_rt_event(net, sk, NULL);
+}
+
+/* returns pointer to right memory that stores info for option confirmation */
+static struct mpdccp_confirm_opt* get_cnf_mem(struct mpdccp_cb *mpcb, u8 id, u8 type)
+{
+	struct mpdccp_addr *mp_addr;
+
+	mpdccp_pr_debug("looking for address id: %u, opt_type %u", id, type);
+	list_for_each_entry_rcu(mp_addr, &mpcb->paddress_list, address_list) {
+		if (!mp_addr->remote && mp_addr->id == id) {
+			switch (type)
+			{
+				case DCCPO_MP_ADDADDR:
+					return &mp_addr->cnf_addaddr;
+				case DCCPO_MP_REMOVEADDR:
+					return &mp_addr->cnf_remaddr;
+				case DCCPO_MP_PRIO:
+					return &mp_addr->cnf_prio;
+			}
+		}
+	}
+	DCCP_CRIT("couldnt locate confirm memory for id %u", id);
+	return NULL;
+}
+
+/* Function is called when sending either mp_addaddr, mp_remoeaddr or mp_prio to store a copy */
+static void pm_store_confirm_opt(struct sock *sk, u8 *buf, u8 id, u8 type, u8 len)
+{
+	struct mpdccp_cb *mpcb = get_mpcb(sk);
+	u8 real_id = id ? id : mpcb->master_addr_id;		//if id = 0 we store master id
+	struct mpdccp_confirm_opt *new_opt = get_cnf_mem(mpcb, real_id, type);
+
+	if (new_opt) {
+		__be64 seq = cpu_to_be64((mpcb->mp_oall_seqno << 16));
+		rcu_read_lock();
+		new_opt->opt[0] = DCCPO_MULTIPATH;
+		new_opt->opt[1] = 9;
+		new_opt->opt[2] = DCCPO_MP_SEQ;
+		memcpy(&new_opt->opt[3], &seq, 6);
+		new_opt->opt[9] = DCCPO_MULTIPATH;
+		new_opt->opt[10] = len + 3;
+		new_opt->opt[11] = type;
+		memcpy(&new_opt->opt[12], buf, len);
+		
+		new_opt->t_init = pm_jiffies32;
+		new_opt->t_timeout = new_opt->t_init + MPDCCP_CONFIRM_RETRANSMIT_TIMEOUT;
+		new_opt->resent_cnt = 0;
+		rcu_read_unlock();
+
+		pm_add_insert_rt_event(sk, new_opt);
+	}
+}
+
+/*  handle received mp_confirm option */
+static int pm_rcv_confirm_opt(struct mpdccp_cb *mpcb, u8 *rcv_opt, u8 id)
+{
+	u8 len = rcv_opt[10];
+	u8 type = rcv_opt[11];
+	struct mpdccp_confirm_opt *snt_opt = get_cnf_mem(mpcb, id, type);
+
+	if(!snt_opt){
+		DCCP_CRIT("could not recover a matching sent option");
+		return 1;
+	}
+
+	rcu_read_lock();
+	if(snt_opt && len == snt_opt->opt[10] && !memcmp(rcv_opt, snt_opt->opt, len)) {
+		mpdccp_pr_debug("mp_confirm matches sent option. txpe: %u, len %u", type, len);
+
+		/* only mp_removeaddr requires action after received confirm */
+		if(type == DCCPO_MP_REMOVEADDR) {
+			pm_del_addr(mpcb, id, false, false);
+			pm_free_id(mpcb->meta_sk, id);
+		}
+		rcu_read_unlock();
+		/* option was confirmed, stop retransmitting the option */
+		pm_remove_rt_event(sock_net(mpcb->meta_sk), NULL, snt_opt);
+		return 0;
+	}
+
+	DCCP_CRIT("mp_confirm does not match any stored option");
+	rcu_read_unlock();
+	return 1;
+}
+
+/* Pathmanager namespace related functions */
+/* Find the first free index in the bitfield */
+static int mpdccp_find_free_index(u64 bitfield)
+{
+	int i;
+	/* There are anyways no free bits... */
+	if (bitfield == 0xff) return -1;
+
+	i = ffs(~bitfield) - 1;
+	/* Try from 0 on */
+	if (i >= sizeof(bitfield) * 8)
+		return mpdccp_find_free_index(bitfield);
+	return i;
+}
+
+/* The following two functions mpdccp_add_addr and mpdccp_del_addr
+ * add or delete an address to/from both the global address list and all
+ * existing connections. We assume that every operation produces a 
+ * consistent state upon completion, i.e. if an address is not
+ * in the list, it is not used in any connection. */
+static int mpdccp_add_addr(struct mpdccp_pm_ns *pm_ns,
+			      			struct pm_local_addr_event *event)
+{
+	struct pm_local_addr *local_addr;
+	struct mpdccp_cb *mpcb;
+
+	struct sockaddr 			*local;
+	struct sockaddr_in 			local_v4_address;
+	struct sockaddr_in6 		local_v6_address;
+
+	sa_family_t family 			= event->family;
+	union inet_addr *addr 		= &event->addr;
+    int if_idx 					= event->if_idx;
+	int locaddr_len, loc_id;
+	u16 port;
+
+	struct list_head *plocal_addr_list = &pm_ns->plocal_addr_list;
+
+	//rcu_read_lock_bh();
+	spin_lock(&pm_ns->plocal_lock);
+
+	/* Add the address to the list of known addresses so that
+	 * new connections can use it. If the address is known, it does not
+	 * need to be added, as all existing connections already use it. */
+	list_for_each_entry_rcu(local_addr, plocal_addr_list, address_list) {
+		if (family == local_addr->family &&
+		   (!if_idx || if_idx == local_addr->if_idx))
+		{
+			if ((family == AF_INET  && 
+				local_addr->addr.in.s_addr == addr->in.s_addr) ||
+				(family == AF_INET6 && 
+				ipv6_addr_equal(&local_addr->addr.in6, &addr->in6)))
+			{
+				spin_unlock(&pm_ns->plocal_lock);
+				//rcu_read_unlock_bh();
+
+				return false;
+			}
+		}
+	}
+
+	/* Address is unused, create a new address entry. */
+    local_addr = kmem_cache_zalloc(mpdccp_pm_addr_cache, GFP_ATOMIC);
+    if (!local_addr) {
+    	spin_unlock(&pm_ns->plocal_lock);
+	//rcu_read_unlock_bh();
+
+        mpdccp_pr_debug("Failed to allocate memory for new local address.\n");
+        
+        return false;
+    }
+
+    local_addr->family = family;
+	local_addr->if_idx = if_idx;
+
+	loc_id = mpdccp_find_free_index(pm_ns->loc4_bits);
+
+	if (loc_id < 0) {
+		mpdccp_pr_debug("Failed to find free address id index.\n");
+		return false;
+	}
+
+	local_addr->id = loc_id + 1;
+	pm_ns->loc4_bits |= (1 << loc_id);
+
+	if (family == AF_INET) {
+	    local_addr->addr.in.s_addr = addr->in.s_addr;
+
+		mpdccp_pr_debug("updated IP %pI4 on ifidx %u, id: %u loc4: %llu",
+			    &addr->in.s_addr, if_idx, local_addr->id, pm_ns->loc4_bits);
+	} else {
+		local_addr->addr.in6 = addr->in6;
+
+		mpdccp_pr_debug("updated IP %pI6 on ifidx %u, id: %u loc4: %llu",
+				&addr->in6, if_idx, local_addr->id, pm_ns->loc4_bits);
+	}
+	
+	list_add_tail_rcu(&local_addr->address_list, plocal_addr_list);
+
+	/* TODO: Is this needed? It might have been a MOD-event. */
+	//event->code = MPDCCP_EVENT_ADD;
+
+	/* Set target IPv4/v6 address correctly */
+	if (family == AF_INET) {
+		local_v4_address.sin_family			= AF_INET;
+		local_v4_address.sin_addr.s_addr 	= addr->in.s_addr;
+	} else {
+		local_v6_address.sin6_family		= AF_INET6;
+		local_v6_address.sin6_addr 			= addr->in6;
+	}
+
+	/* Iterate over all connections and create new connections via this address */
+	mpdccp_for_each_conn(pconnection_list, mpcb) {
+		if (family == AF_INET) {
+			if(mpcb->role == MPDCCP_CLIENT)
+				local_v4_address.sin_port		= 0;
+			else
+				local_v4_address.sin_port		= htons (mpcb->server_port);
+
+			port = local_v4_address.sin_port;
+			local = (struct sockaddr *) &local_v4_address;
+			locaddr_len = sizeof (struct sockaddr_in);
+		} else {
+			if(mpcb->role == MPDCCP_CLIENT)
+				local_v6_address.sin6_port		= 0;
+			else
+				local_v6_address.sin6_port		= htons (mpcb->server_port);
+
+			port = local_v6_address.sin6_port;
+			local = (struct sockaddr *) &local_v6_address;
+			locaddr_len = sizeof (struct sockaddr_in6);
+		}
+
+		switch (mpcb->role) {
+		case MPDCCP_CLIENT:
+			/* Do not add more subflows if the client in in SP mode */
+			if (!(mpcb->fallback_sp && (mpcb->cnt_subflows > 0)))
+				mpdccp_add_client_conn(mpcb, local, locaddr_len, if_idx,
+						(struct sockaddr*)&mpcb->mpdccp_remote_addr,
+						mpcb->remaddr_len);
+			break;
+		case MPDCCP_SERVER:
+			/* send mp_addaddress */
+			if (!(mpcb->fallback_sp && (mpcb->cnt_subflows > 0))){
+				//add_init_server_conn(mpcb, backlog);
+				pm_add_addr(mpcb, family, local_addr->id, &local_addr->addr, port, false);
+				mpdccp_send_add_path(local_addr, port, mpcb);
+			}
+			break;
+		default:
+			break;
+		}
+	}
+	spin_unlock(&pm_ns->plocal_lock);
+	//rcu_read_unlock_bh();
+
+	return true;
+}
+
+static bool mpdccp_del_addr(struct mpdccp_pm_ns *pm_ns,
+			      struct pm_local_addr_event *event)
+{
+
+	struct sock *sk;
+	struct mpdccp_cb *mpcb;
+	struct 	pm_local_addr *local_addr;
+
+	sa_family_t family 			= event->family;
+	union inet_addr *addr 		= &event->addr;
+    int if_idx 					= event->if_idx;
+    int addr_id;
+	bool found 					= false;
+	bool in_use					= false;
+
+	struct list_head *plocal_addr_list = &pm_ns->plocal_addr_list;
+
+	//TODO reordering
+	pr_info("RO: mpdccp_del_addr triggered...");
+
+	//rcu_read_lock_bh();
+	spin_lock_bh(&pm_ns->plocal_lock);
+
+	/* Delete the address from the list of known addresses so that
+	 * new connections stop using it. */
+	list_for_each_entry_rcu(local_addr, plocal_addr_list, address_list) {
+		/* Search for an entry with matching address family and iface ID */
+		if (family == local_addr->family &&
+		   (!if_idx || if_idx == local_addr->if_idx))
+		{
+			/* Check for a matching IPv4/v6 address */
+			if ((family == AF_INET  && 
+				local_addr->addr.in.s_addr == addr->in.s_addr) ||
+				(family == AF_INET6 && 
+				ipv6_addr_equal(&local_addr->addr.in6, &addr->in6)))
+			{
+				found = true;
+				addr_id = local_addr->id;
+				list_del_rcu(&local_addr->address_list);
+				kmem_cache_free(mpdccp_pm_addr_cache, local_addr);
+			}
+		}
+	}
+
+	/* Address is unknown, so it can not be used in any connection. */
+	if(!found) 
+	{
+		spin_unlock_bh(&pm_ns->plocal_lock);
+		//rcu_read_unlock_bh();
+		return false;
+	}
+
+	/* Iterate over all connections and remove any socket that still
+	 * uses this address */
+	mpdccp_for_each_conn(pconnection_list, mpcb) {
+		mpdccp_for_each_sk(mpcb, sk) {
+
+			/* Not yet sure if this applies to MPDCCP, too */
+			// if (sock_net(sk) != net)
+			// 	continue;
+
+			/* If the PM has changed, we are not responsible for this mpcb */
+			if (mpcb->pm_ops != &mpdccp_pm_default)
+				break;
+
+			/* Does the event family and interface ID match the socket? */
+			if (family == sk->__sk_common.skc_family &&
+			   (!if_idx || mpdccp_my_sock(sk)->if_idx  == if_idx))
+			{
+				/* Does the IP address in the event match the socket */
+				if ((family == AF_INET  && 
+					sk->__sk_common.skc_rcv_saddr == addr->in.s_addr) ||
+					(family == AF_INET6 && 
+					ipv6_addr_equal(&sk->__sk_common.skc_v6_rcv_saddr, &addr->in6)))
+				{
+					if(family == AF_INET)
+						mpdccp_pr_debug("Deleting subflow socket %p with address %pI4.\n", sk, &sk->__sk_common.skc_rcv_saddr);
+					else
+						mpdccp_pr_debug("Deleting subflow socket %p with address %pI6.\n", sk, &sk->__sk_common.skc_v6_rcv_saddr);
+
+					addr_id = mpdccp_my_sock(sk)->local_addr_id;
+					in_use = true;
+					mpdccp_my_sock(sk)->delpath_sent = true;
+					mpdccp_close_subflow (mpcb, sk, 2);
+					mpdccp_send_remove_path(mpcb, addr_id);
+					pm_free_id(mpcb->meta_sk, addr_id);
+				}
+			}
+		}
+
+		mpdccp_for_each_listen_sk(mpcb, sk) {
+			/* Not yet sure if this applies to MPDCCP, too */
+			// if (sock_net(sk) != net)
+			// 	continue;
+
+			/* If the PM has changed, we are not responsible for this mpcb */
+			if (mpcb->pm_ops != &mpdccp_pm_default)
+				break;
+
+			/* Does the event family and interface ID match the socket? */
+			if (family == sk->__sk_common.skc_family &&
+			   (!if_idx || mpdccp_my_sock(sk)->if_idx  == if_idx))
+			{
+				/* Does the IP address in the event match the socket */
+				if ((family == AF_INET  && 
+					sk->__sk_common.skc_rcv_saddr == addr->in.s_addr) ||
+					(family == AF_INET6 && 
+					ipv6_addr_equal(&sk->__sk_common.skc_v6_rcv_saddr, &addr->in6)))
+				{
+					if(family == AF_INET)
+						mpdccp_pr_debug("Deleting listening socket %p with address %pI4.\n", sk, &sk->__sk_common.skc_rcv_saddr);
+					else
+						mpdccp_pr_debug("Deleting listening socket %p with address %pI6.\n", sk, &sk->__sk_common.skc_v6_rcv_saddr);
+
+					//addr_id = mpdccp_my_sock(sk)->local_addr_id;
+					//in_use = true;
+					mpdccp_close_subflow (mpcb, sk, 1);
+				}
+			}
+		}
+	}
+
+	if(!in_use){
+		pm_ns->loc4_bits &= ~(1 << (addr_id-1));
+		mpdccp_pr_debug("loc4_bits updated: %llu, removed id: %u", pm_ns->loc4_bits, addr_id);
+	}
+
+	spin_unlock_bh(&pm_ns->plocal_lock);
+	//rcu_read_unlock_bh();
+
+	return true;
+}
+
+static void pm_local_address_worker(struct work_struct *work)
+{
+	const struct delayed_work *delayed_work = container_of(work,
+							 struct delayed_work,
+							 work);
+	struct mpdccp_pm_ns *pm_ns = container_of(delayed_work,
+						 struct mpdccp_pm_ns,
+						 address_worker);
+	struct pm_local_addr_event *event = NULL;
+
+next_event:
+	kfree(event);
+
+	/* First, let's dequeue an event from our event-list */
+	/* TODO: is _bh REALLY the right thing to do here? */
+	//rcu_read_lock_bh();
+	event = list_first_entry_or_null(&pm_ns->events,
+					 struct pm_local_addr_event, list);
+	if (!event) {
+		/* No more events to work on */
+		//rcu_read_unlock_bh();
+		return;
+	}
+
+	list_del(&event->list);
+	//mpdccp_local = rcu_dereference_bh(pm_ns->local);
+
+	if (event->code == MPDCCP_EVENT_DEL) {
+		if( !mpdccp_del_addr(pm_ns, event) ) {
+			mpdccp_pr_debug("Delete address failed: Address not found.\n");
+		}
+	} else {
+		/* TODO: Filter link local and TUN devices */
+		if( !mpdccp_add_addr(pm_ns, event) ) {
+			mpdccp_pr_debug("Add address failed: Address already in use.\n");
+		}
+	}
+	//rcu_read_unlock_bh();
+
+	goto next_event;
+}
+
+
+
+/***********************************
+* IPv4/v6 address event handling
+************************************/
+
+static struct pm_local_addr_event *lookup_similar_event(const struct net *net,
+						     const struct pm_local_addr_event *event)
+{
+	struct pm_local_addr_event *eventq;
+	struct mpdccp_pm_ns *pm_ns = fm_get_ns(net);
+
+	list_for_each_entry(eventq, &pm_ns->events, list) {
+		if (eventq->family != event->family)
+			continue;
+		if (eventq->if_idx != event->if_idx)
+			continue;
+		if (event->family == AF_INET) {
+			if (eventq->addr.in.s_addr == event->addr.in.s_addr)
+				return eventq;
+		} else {
+			if (ipv6_addr_equal(&eventq->addr.in6, &event->addr.in6))
+				return eventq;
+		}
+	}
+	return NULL;
+}
+
+/* We already hold the net-namespace MPDCCP-lock */
+static void add_pm_event(struct net *net, const struct pm_local_addr_event *event)
+{
+	struct pm_local_addr_event *eventq = lookup_similar_event(net, event);
+	struct mpdccp_pm_ns *pm_ns = fm_get_ns(net);
+	int delay = 10;
+
+	if (eventq) {
+		switch (event->code) {
+		case MPDCCP_EVENT_DEL:
+			mpdccp_pr_debug("del old_code %u\n", eventq->code);
+			list_del(&eventq->list);
+			kfree(eventq);
+			break;
+		case MPDCCP_EVENT_ADD:
+			mpdccp_pr_debug("add old_code %u\n", eventq->code);
+			eventq->code = MPDCCP_EVENT_ADD;
+			return;
+		case MPDCCP_EVENT_MOD:
+			mpdccp_pr_debug("mod old_code %u\n", eventq->code);
+			eventq->code = MPDCCP_EVENT_MOD;
+			return;
+		}
+	}
+
+	/* OK, we have to add the new address to the wait queue */
+	eventq = kmemdup(event, sizeof(struct pm_local_addr_event), GFP_ATOMIC);
+	if (!eventq)
+		return;
+
+	list_add_tail(&eventq->list, &pm_ns->events);
+
+	/* Create work-queue */
+	if (!delayed_work_pending(&pm_ns->address_worker))
+		queue_delayed_work(mpdccp_wq, &pm_ns->address_worker, msecs_to_jiffies(delay));
+}
+
+static void addr4_event_handler(const struct in_ifaddr *ifa, unsigned long event,
+				struct net *net)
+{
+	const struct net_device *netdev = ifa->ifa_dev->dev;
+	struct mpdccp_pm_ns *pm_ns = fm_get_ns(net);
+	struct pm_local_addr_event mpevent;
+
+	/* Do not create events for link-local interfaces and TUN devices */
+	if ( ifa->ifa_scope > RT_SCOPE_LINK 	||
+		 netdev->flags & IFF_POINTOPOINT	||
+	     ipv4_is_loopback(ifa->ifa_local) )
+		return;
+
+	spin_lock_bh(&pm_ns->plocal_lock);
+
+	mpevent.family = AF_INET;
+	mpevent.addr.in.s_addr = ifa->ifa_local;
+	mpevent.if_idx  = netdev->ifindex;
+
+	if (event == NETDEV_DOWN || !netif_running(netdev) || netdev->operstate == IF_OPER_DOWN ||
+	    !(netdev->flags & IFF_MPDCCPON)|| !(netdev->flags & IFF_UP))
+		mpevent.code = MPDCCP_EVENT_DEL;
+	else if (event == NETDEV_UP)
+		mpevent.code = MPDCCP_EVENT_ADD;
+	else if (event == NETDEV_CHANGE)
+		mpevent.code = MPDCCP_EVENT_MOD;
+
+	mpdccp_pr_debug("event %lu, running %d flags %u oper %x", event, netif_running(netdev), netdev->flags, netdev->operstate);
+    mpdccp_pr_debug("%s created event for %pI4, code %u idx %u\n", __func__,
+		    &ifa->ifa_local, mpevent.code, mpevent.if_idx);
+	add_pm_event(net, &mpevent);
+
+	spin_unlock_bh(&pm_ns->plocal_lock);
+	return;
+}
+
+static int mpdccp_pm_inetaddr_event(struct notifier_block *this,
+				   unsigned long event, void *ptr)
+{
+	const struct in_ifaddr *ifa = (struct in_ifaddr *)ptr;
+	struct net *net = dev_net(ifa->ifa_dev->dev);
+
+	if (!(event == NETDEV_UP || event == NETDEV_DOWN ||
+	      event == NETDEV_CHANGE))
+		return NOTIFY_DONE;
+
+	addr4_event_handler(ifa, event, net);
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block mpdccp_pm_inetaddr_notifier = {
+		.notifier_call = mpdccp_pm_inetaddr_event,
+};
+
+static int mpdccp_pm_dccp_event(struct notifier_block *this,
+				   unsigned long event, void *ptr)
+{
+	const struct sock *sk_closed = (struct sock *)ptr;
+
+	struct sock *sk;
+	struct mpdccp_cb *mpcb;
+	struct sockaddr 			*local;
+	struct sockaddr_in 			local_v4_address;
+	struct sockaddr_in6 		local_v6_address;
+	int	locaddr_len;
+	int	if_idx;
+
+	if (!sk_closed) return NOTIFY_DONE;
+
+	if(sk_closed->__sk_common.skc_family == AF_INET){
+			local_v4_address.sin_family		= AF_INET;
+			local_v4_address.sin_addr.s_addr = sk_closed->__sk_common.skc_rcv_saddr;
+			local_v4_address.sin_port		= 0;
+			local = (struct sockaddr *) &local_v4_address;
+			locaddr_len = sizeof (struct sockaddr_in);
+		} else {
+			local_v6_address.sin6_family		= AF_INET6;
+			local_v6_address.sin6_addr 		= sk_closed->__sk_common.skc_v6_rcv_saddr;
+			local_v6_address.sin6_port		= 0;
+			local = (struct sockaddr *) &local_v6_address;
+			locaddr_len = sizeof (struct sockaddr_in6);
+		}
+
+	if_idx = sk_closed->__sk_common.skc_bound_dev_if;
+	if (event == DCCP_EVENT_CLOSE){
+		mpdccp_for_each_conn(pconnection_list, mpcb) {
+			if (mpcb->to_be_closed) continue;
+			mpdccp_for_each_sk(mpcb, sk) {
+#if 0
+				/* Handle close events for both the subflow and meta sockets */
+				if (mpcb->meta_sk == sk_closed) {
+					mpdccp_close_subflow(mpcb, sk, 1);
+					mpdccp_pr_debug("close dccp sk %p", sk_closed);
+				}
+				else
+#endif
+				if(sk == sk_closed) {
+					mpdccp_reconnect_client (sk, 0, local, locaddr_len, if_idx);
+					break;
+				}
+			}
+		}
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block mpdccp_pm_dccp_notifier = {
+		.notifier_call = mpdccp_pm_dccp_event,
+};
+
+#if IS_ENABLED(CONFIG_IPV6)
+
+/* IPV6-related address/interface watchers */
+struct mpdccp_dad_data {
+	struct timer_list timer;
+	struct inet6_ifaddr *ifa;
+};
+
+static void dad_callback(struct timer_list *t);
+static int inet6_addr_event(struct notifier_block *this,
+				     unsigned long event, void *ptr);
+
+static bool ipv6_dad_finished(const struct inet6_ifaddr *ifa)
+{
+	return !(ifa->flags & IFA_F_TENTATIVE) ||
+	       ifa->state > INET6_IFADDR_STATE_DAD;
+}
+
+static void dad_init_timer(struct mpdccp_dad_data *data,
+				 struct inet6_ifaddr *ifa)
+{
+	data->ifa = ifa;
+	//data->timer.data = (unsigned long)data;
+	data->timer.function = dad_callback;
+	if (ifa->idev->cnf.rtr_solicit_delay)
+		data->timer.expires = jiffies + ifa->idev->cnf.rtr_solicit_delay;
+	else
+		data->timer.expires = jiffies + (HZ/10);
+}
+
+//static void dad_callback(unsigned long arg)
+static void dad_callback(struct timer_list *t)
+{
+	//struct mpdccp_dad_data *data = (struct mpdccp_dad_data *)arg;
+	struct mpdccp_dad_data *data = from_timer(data, t, timer);
+
+	/* DAD failed or IP brought down? */
+	if (data->ifa->state == INET6_IFADDR_STATE_ERRDAD ||
+	    data->ifa->state == INET6_IFADDR_STATE_DEAD)
+		goto exit;
+
+	if (!ipv6_dad_finished(data->ifa)) {
+		dad_init_timer(data, data->ifa);
+		add_timer(&data->timer);
+		return;
+	}
+
+	inet6_addr_event(NULL, NETDEV_UP, data->ifa);
+
+exit:
+	in6_ifa_put(data->ifa);
+	kfree(data);
+}
+
+static inline void dad_setup_timer(struct inet6_ifaddr *ifa)
+{
+	struct mpdccp_dad_data *data;
+
+	data = kmalloc(sizeof(*data), GFP_ATOMIC);
+
+	if (!data)
+		return;
+
+	//init_timer(&data->timer);
+	timer_setup(&data->timer, NULL, 0);
+	dad_init_timer(data, ifa);
+	add_timer(&data->timer);
+	in6_ifa_hold(ifa);
+}
+
+static void addr6_event_handler(const struct inet6_ifaddr *ifa, unsigned long event,
+				struct net *net)
+{
+	const struct net_device *netdev = ifa->idev->dev;
+	int addr_type = ipv6_addr_type(&ifa->addr);
+	struct mpdccp_pm_ns *pm_ns = fm_get_ns(net);
+	struct pm_local_addr_event mpevent;
+
+	if ( ifa->scope > RT_SCOPE_LINK 		||
+		 netdev->flags & IFF_POINTOPOINT	||
+	     addr_type == IPV6_ADDR_ANY 		||
+	    (addr_type & IPV6_ADDR_LOOPBACK) 	||
+	    (addr_type & IPV6_ADDR_LINKLOCAL))
+		return;
+
+	spin_lock_bh(&pm_ns->plocal_lock);
+
+	mpevent.family = AF_INET6;
+	mpevent.addr.in6 = ifa->addr;
+	mpevent.if_idx = netdev->ifindex;
+
+	if (event == NETDEV_DOWN || !netif_running(netdev) ||
+	    !(netdev->flags & IFF_MPDCCPON)|| !(netdev->flags & IFF_UP))
+		mpevent.code = MPDCCP_EVENT_DEL;
+	else if (event == NETDEV_UP)
+		mpevent.code = MPDCCP_EVENT_ADD;
+	else if (event == NETDEV_CHANGE)
+		mpevent.code = MPDCCP_EVENT_MOD;
+
+	mpdccp_pr_debug("%s created event for %pI6, code %u idx %u\n", __func__,
+		    &ifa->addr, mpevent.code, mpevent.if_idx);
+	add_pm_event(net, &mpevent);
+
+	spin_unlock_bh(&pm_ns->plocal_lock);
+	return;
+}
+
+/* React on IPv6-addr add/rem-events */
+static int mpdccp_pm_inet6addr_event(struct notifier_block *this, unsigned long event,
+			    void *ptr)
+{
+	struct inet6_ifaddr *ifa6 = (struct inet6_ifaddr *)ptr;
+	struct net *net = dev_net(ifa6->idev->dev);
+
+	if (!(event == NETDEV_UP || event == NETDEV_DOWN ||
+	      event == NETDEV_CHANGE))
+		return NOTIFY_DONE;
+
+	if (!ipv6_dad_finished(ifa6))
+		dad_setup_timer(ifa6);
+	else
+		addr6_event_handler(ifa6, event, net);
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block mpdccp_pm_inet6addr_notifier = {
+		.notifier_call = mpdccp_pm_inet6addr_event,
+};
+#endif
+
+static int mpdccp_init_net(struct net *net)
+{
+	struct mpdccp_pm_ns *pm_ns;
+
+	pm_ns = kzalloc(sizeof(*pm_ns), GFP_KERNEL);
+	if (!pm_ns)
+		return -ENOBUFS;
+
+	INIT_LIST_HEAD(&pm_ns->plocal_addr_list);
+	spin_lock_init(&pm_ns->plocal_lock);
+	INIT_LIST_HEAD(&pm_ns->events);
+	INIT_DELAYED_WORK(&pm_ns->address_worker, pm_local_address_worker);
+	INIT_LIST_HEAD(&pm_ns->retransmit);
+	INIT_DELAYED_WORK(&pm_ns->retransmit_worker, pm_retransmit_worker);
+	pm_ns->net = net;
+	net->mpdccp.path_managers[MPDCCP_PM_FULLMESH] = pm_ns;
+
+	return 0;
+}
+
+/* Wipe the local address list */
+static void dccp_free_local_addr_list(struct mpdccp_pm_ns *pm_ns)
+{
+	struct pm_local_addr *addr;
+	struct list_head *pos, *temp;
+	list_for_each_safe(pos, temp, &pm_ns->plocal_addr_list) {
+		addr = list_entry(pos, struct pm_local_addr, address_list);
+		list_del(pos);
+		kfree(addr);
+	}
+}
+
+static void mpdccp_exit_net(struct net *net)
+{
+	struct mpdccp_pm_ns *pm_ns;
+
+	pm_ns = net->mpdccp.path_managers[MPDCCP_PM_FULLMESH];
+	/* Stop the worker */
+	cancel_delayed_work_sync(&pm_ns->address_worker);
+	cancel_delayed_work_sync(&pm_ns->retransmit_worker);
+
+	/* Clean and free the list */
+	dccp_free_local_addr_list(pm_ns);
+	kfree(pm_ns);
+
+/* This is statistics stuff that is not yet supported. */
+#if 0
+    remove_proc_entry("snmp", net->mpdccp.proc_net_mpdccp);
+    remove_proc_entry("mpdccp", net->mpdccp.proc_net_mpdccp);
+    remove_proc_subtree("mpdccp_net", net->proc_net);
+    free_percpu(net->mpdccp.mpdccp_statistics);
+#endif
+}
+
+static struct pernet_operations mpdccp_pm_net_ops = {
+    .init = mpdccp_init_net,
+    .exit = mpdccp_exit_net,
+};
+
+/* React on IPv6-addr add/rem-events */
+static int inet6_addr_event(struct notifier_block *this, unsigned long event,
+			    void *ptr)
+{
+	struct inet6_ifaddr *ifa6 = (struct inet6_ifaddr *)ptr;
+	struct net *net = dev_net(ifa6->idev->dev);
+
+	if (!(event == NETDEV_UP || event == NETDEV_DOWN ||
+	      event == NETDEV_CHANGE))
+		return NOTIFY_DONE;
+
+	if (!ipv6_dad_finished(ifa6))
+		dad_setup_timer(ifa6);
+	else
+		addr6_event_handler(ifa6, event, net);
+
+	return NOTIFY_DONE;
+}
+
+/* React on ifup/down-events */
+static int netdev_event(struct notifier_block *this, unsigned long event,
+			void *ptr)
+{
+	const struct net_device *dev = netdev_notifier_info_to_dev(ptr);
+	struct in_device *in_dev;
+	const struct in_ifaddr *ifa;
+#if IS_ENABLED(CONFIG_IPV6)
+	struct inet6_dev *in6_dev;
+#endif
+
+	if (!(event == NETDEV_UP || event == NETDEV_DOWN ||
+	      event == NETDEV_CHANGE))
+		return NOTIFY_DONE;
+
+	//rcu_read_lock();
+	in_dev = __in_dev_get_rtnl(dev);
+
+	if (in_dev) {
+		//for_ifa(in_dev) {
+		in_dev_for_each_ifa_rcu(ifa, in_dev) {
+			mpdccp_pm_inetaddr_event(NULL, event, ifa);
+		//} endfor_ifa(in_dev);
+		}
+	}
+
+#if IS_ENABLED(CONFIG_IPV6)
+	in6_dev = __in6_dev_get(dev);
+
+	if (in6_dev) {
+		struct inet6_ifaddr *ifa6;
+		list_for_each_entry(ifa6, &in6_dev->addr_list, if_list)
+			inet6_addr_event(NULL, event, ifa6);
+	}
+#endif
+
+	//rcu_read_unlock();
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block mpdccp_pm_netdev_notifier = {
+		.notifier_call = netdev_event,
+};
+
+/* General initialization of MPDCCP */
+static int mpdccp_pm_init(void)
+{
+    int ret = 0;
+
+    mpdccp_pm_addr_cache = kmem_cache_create("mpdccp_pm_addr", sizeof(struct pm_local_addr),
+                       0, SLAB_TYPESAFE_BY_RCU|SLAB_HWCACHE_ALIGN,
+                       NULL);
+    if (!mpdccp_pm_addr_cache) {
+        mpdccp_pr_debug("Failed to create mpcb pm address slab cache.\n");
+        ret = -1;
+        goto out;
+    }
+
+    ret = register_pernet_subsys(&mpdccp_pm_net_ops);
+    if (ret) {
+        mpdccp_pr_debug("Failed to register pernet subsystem.\n");
+        goto err_reg_pernet_subsys;
+    }
+
+    ret = register_inetaddr_notifier(&mpdccp_pm_inetaddr_notifier);
+    if (ret) {
+        mpdccp_pr_debug("Failed to register inet address notifier.\n");
+        goto err_reg_inetaddr;
+    }
+
+    ret = register_mpdccp_link_notifier(&mpdccp_pm_link_notifier);
+    if (ret) {
+        mpdccp_pr_debug("Failed to register mpdccp_link notifier.\n");
+        goto err_reg_prio;
+    }
+
+#if IS_ENABLED(CONFIG_IPV6)
+    ret = register_inet6addr_notifier(&mpdccp_pm_inet6addr_notifier);
+    if (ret) {
+        mpdccp_pr_debug("Failed to register inet6 address notifier.\n");
+        goto err_reg_inet6addr;
+    }
+#endif
+
+    ret = register_netdevice_notifier(&mpdccp_pm_netdev_notifier);
+    if (ret) {
+        mpdccp_pr_debug("Failed to register netdevice notifier.\n");
+        goto err_reg_netdev;
+    }
+
+    ret = register_dccp_notifier(&mpdccp_pm_dccp_notifier);
+    if (ret) {
+        mpdccp_pr_debug("Failed to register dccp notifier.\n");
+        goto err_reg_dccp;
+    }
+
+out:
+    return ret;
+
+    //unregister_netdevice_notifier(&mpdccp_pm_netdev_notifier);
+err_reg_dccp: // later change position
+	unregister_dccp_notifier(&mpdccp_pm_dccp_notifier);
+goto out;
+
+err_reg_netdev:
+#if IS_ENABLED(CONFIG_IPV6)
+    unregister_inet6addr_notifier(&mpdccp_pm_inet6addr_notifier);
+err_reg_inet6addr:
+#endif
+    unregister_inetaddr_notifier(&mpdccp_pm_inetaddr_notifier);
+err_reg_inetaddr:
+    unregister_pernet_subsys(&mpdccp_pm_net_ops);
+err_reg_prio:
+	unregister_mpdccp_link_notifier(&mpdccp_pm_link_notifier);
+err_reg_pernet_subsys:
+	kmem_cache_destroy(mpdccp_pm_addr_cache);
+goto out;
+}
+
+static void mpdccp_pm_exit(void)
+{
+    unregister_dccp_notifier(&mpdccp_pm_dccp_notifier);
+
+    /* TODO: Tear down connections */
+    unregister_netdevice_notifier(&mpdccp_pm_netdev_notifier);
+#if IS_ENABLED(CONFIG_IPV6)
+    unregister_inet6addr_notifier(&mpdccp_pm_inet6addr_notifier);
+#endif
+    unregister_mpdccp_link_notifier(&mpdccp_pm_link_notifier);
+    unregister_inetaddr_notifier(&mpdccp_pm_inetaddr_notifier);
+    unregister_pernet_subsys(&mpdccp_pm_net_ops);
+
+    /* TODO: we need to free all mpcb's (slab cache) on exit. */
+    kmem_cache_destroy(mpdccp_pm_addr_cache);
+#if 0
+    /* sk_free (and __sk_free) requires wmem_alloc to be 1.
+     * All the rest is set to 0 thanks to __GFP_ZERO above.
+     */
+    atomic_set(&master_sk->sk_wmem_alloc, 1);
+    sk_free(master_sk);
+#endif
+}
+
+static
+int
+add_init_client_conn (
+	struct mpdccp_cb		*mpcb,
+	struct sockaddr			*remote_address,
+	int				socklen)
+{
+	struct pm_local_addr 	*local;
+	struct sockaddr 		*local_address;
+	struct sockaddr_in		*meta_v4_address;
+	struct sockaddr_in 		local_v4_address;
+	struct sockaddr_in6 		local_v6_address;
+	union inet_addr rema;
+	int				locaddr_len;
+	int				ret=0, num=0, port=0;
+	struct mpdccp_pm_ns		*pm_ns;
+	int local_if_idx;
+	
+	if (!mpcb || !remote_address) return -EINVAL;
+	if (mpcb->role != MPDCCP_CLIENT) return -EPERM;
+	
+	//pm_ns = fm_get_ns (current->nsproxy->net_ns);
+	pm_ns = fm_get_ns (read_pnet (&mpcb->net));
+	
+	memcpy(&mpcb->mpdccp_remote_addr, remote_address, socklen);
+	mpcb->remaddr_len = socklen;
+
+	if(remote_address->sa_family == AF_INET){
+		struct sockaddr_in *ad4 = (struct sockaddr_in*)remote_address;
+		rema.in = ad4->sin_addr;
+		port = ad4->sin_port;
+	} else if(remote_address->sa_family == AF_INET6) {
+		struct sockaddr_in6 *ad6 = (struct sockaddr_in6*)remote_address;
+		rema.in6 = ad6->sin6_addr;
+		port = ad6->sin6_port;
+	}
+	pm_add_addr(mpcb, remote_address->sa_family, 0, &rema, port, true);
+
+	if(mpcb->has_localaddr == 0)
+		mpdccp_get_mpcb_local_address(mpcb, (struct sockaddr_in *)remote_address);
+
+	meta_v4_address = (struct sockaddr_in *)&mpcb->mpdccp_local_addr;
+	mpdccp_pr_debug ("MPDCCP bound to saddr %pI4", &meta_v4_address->sin_addr.s_addr);
+	
+	//rcu_read_lock_bh();
+	/*first create subflow on default path*/
+	list_for_each_entry_rcu(local, &pm_ns->plocal_addr_list, address_list) {
+		if (local->family == AF_INET && local->addr.in.s_addr == meta_v4_address->sin_addr.s_addr) {
+			local_v4_address.sin_family		= AF_INET;
+			local_v4_address.sin_addr.s_addr 	= local->addr.in.s_addr;
+			local_v4_address.sin_port		= 0;
+			local_address = (struct sockaddr *) &local_v4_address;
+			ret = mpdccp_add_client_conn (mpcb, local_address, sizeof(struct sockaddr_in),
+				local->if_idx, remote_address, socklen);
+			if ((ret < 0) && (ret != -EINPROGRESS) ) {
+				mpdccp_pr_debug ("error in mpdccp_add_client_conn() for master subflow: %d\n", ret);
+				goto out;
+			} else {
+				num++;
+				if (mpcb && mpcb->fallback_sp) {
+					mpdccp_pr_debug ("fallback to single path DCCP, don't create more subflows");
+					goto out;
+				}
+				break;
+			}
+		}
+	}
+
+	/* Create subflows with all other local addresses */
+	list_for_each_entry_rcu(local, &pm_ns->plocal_addr_list, address_list) {
+		/* Set target IPv4/v6 address correctly */
+		if (local->family == AF_INET) {
+			if(local->addr.in.s_addr == meta_v4_address->sin_addr.s_addr) continue;
+			local_v4_address.sin_family		= AF_INET;
+			local_v4_address.sin_addr.s_addr 	= local->addr.in.s_addr;
+			local_v4_address.sin_port		= 0;
+			local_address = (struct sockaddr *) &local_v4_address;
+			locaddr_len = sizeof (struct sockaddr_in);
+		} else {
+			local_v6_address.sin6_family		= AF_INET6;
+			local_v6_address.sin6_addr 		= local->addr.in6;
+			local_v6_address.sin6_port		= 0;
+			local_address = (struct sockaddr *) &local_v6_address;
+			locaddr_len = sizeof (struct sockaddr_in6);
+		}
+		local_if_idx = local->if_idx;
+		/* unlock since mpdccp_add_client_conn() can sleep (data from the list entry are now copied locally) */
+		//rcu_read_unlock_bh();
+		ret = mpdccp_add_client_conn (	mpcb, local_address, locaddr_len,
+						local_if_idx, remote_address, socklen);
+		if ((ret < 0) && (ret != -EINPROGRESS) ) {
+			mpdccp_pr_debug ("error in mpdccp_add_client_conn() for "
+					"subflow %d: %d\n", num, ret);
+		} else {
+			num++;
+		}
+		/* lock again to continue scanning the list */
+		//rcu_read_lock_bh();
+	}
+	//rcu_read_unlock_bh();
+
+out:
+	if (num == 0) {
+		mpdccp_pr_debug ("no connection could be established\n");
+		return ret == 0 ? -ENOTCONN : ret;
+	}
+
+	mpdccp_pr_debug("%d client connections added successfully. There are "
+			"%d subflows now.\n", num, mpcb->cnt_subflows);
+
+	return num;
+}
+
+static
+int
+add_init_server_conn (
+    struct mpdccp_cb		*mpcb,
+    int				backlog)
+{
+    struct pm_local_addr 	*local;
+    struct sockaddr 		*local_address;
+    struct sockaddr_in 		local_v4_address;
+    struct sockaddr_in6 	local_v6_address;
+    int				locaddr_len;
+    int				ret;
+    struct mpdccp_pm_ns		*pm_ns;
+    int				server_port = 0;
+
+
+    if (!mpcb) return -EINVAL;
+    if (mpcb->role != MPDCCP_SERVER) return -EPERM;
+    if (!mpcb->has_localaddr) return -EINVAL;
+
+    /* get server port from local addr - to be changed - use full local address instead!! */
+    local_address = (struct sockaddr*) &mpcb->mpdccp_local_addr;
+    if (local_address->sa_family == AF_INET) {
+	server_port = ((struct sockaddr_in*)local_address)->sin_port;
+    } else if (local_address->sa_family == AF_INET6) {
+	server_port = ((struct sockaddr_in6*)local_address)->sin6_port;
+    }
+    if (server_port == 0) return -EINVAL;
+
+    //pm_ns = fm_get_ns (current->nsproxy->net_ns);
+    pm_ns = fm_get_ns (read_pnet (&mpcb->net));
+
+    mpcb->server_port = server_port;
+    mpcb->backlog = backlog;
+
+    /* Create subflows on all local interfaces */
+    //rcu_read_lock_bh();
+    list_for_each_entry_rcu(local, &pm_ns->plocal_addr_list, address_list) {
+    
+	/* Set target IPv4/v6 address correctly */
+	if (local->family == AF_INET) {
+	    local_v4_address.sin_family		= AF_INET;
+	    local_v4_address.sin_addr.s_addr 	= local->addr.in.s_addr;
+	    local_v4_address.sin_port		= server_port;
+	    local_address = (struct sockaddr *) &local_v4_address;
+	    locaddr_len = sizeof (struct sockaddr_in);
+	} else {
+	    local_v6_address.sin6_family	= AF_INET6;
+	    local_v6_address.sin6_addr 		= local->addr.in6;
+	    local_v6_address.sin6_port		= server_port;
+	    local_address = (struct sockaddr *) &local_v6_address;
+	    locaddr_len = sizeof (struct sockaddr_in6);
+	}
+	mpdccp_pr_debug("add listen socket: %pISc:%u\n", local_address, htons((unsigned)server_port));
+	ret = mpdccp_add_listen_sock (	mpcb, local_address, locaddr_len,
+					local->if_idx);
+	if (ret < 0) {
+	    mpdccp_pr_debug ("error in mpdccp_add_listen_sock(): %d\n", ret);
+    	    //rcu_read_unlock_bh();
+	    return ret;
+	}
+    }
+    //rcu_read_unlock_bh();
+
+    mpdccp_pr_debug("all server sockets added successfully. There are %d "
+			"listening sockets now.\n", mpcb->cnt_subflows);
+
+    return 0;
+}
+
+static struct mpdccp_pm_ops mpdccp_pm_default = {
+	.add_init_server_conn	= add_init_server_conn,
+	.add_init_client_conn	= add_init_client_conn,
+
+	.claim_local_addr = pm_claim_local_addr,
+	.get_id_from_ip = pm_get_id_from_ip,
+	.del_addr = pm_del_addr,
+	.add_addr = pm_add_addr,
+
+	.rcv_removeaddr_opt	= pm_handle_rm_addr,
+	.get_hmac = pm_get_addr_hmac,
+	.rcv_prio_opt = pm_handle_rcv_prio,
+
+	.rcv_confirm_opt = pm_rcv_confirm_opt,
+	.store_confirm_opt = pm_store_confirm_opt,
+	.del_retrans = pm_del_retrans,
+
+	.name 			= "default",
+	.owner 			= THIS_MODULE,
+};
+
+
+int mpdccp_pm_default_register(void)
+{
+	int	ret;
+
+	ret = mpdccp_register_path_manager(&mpdccp_pm_default);
+	if (ret < 0) {
+		mpdccp_pr_error("Failed to register deault path manager\n");
+		return ret;
+	}
+	/* Register notifier chains for dynamic interface management */
+	ret = mpdccp_pm_init();
+	if (ret) {
+		mpdccp_pr_error("Failed to init default path manager.\n");
+		return ret;
+	}
+	return 0;
+}
+
+void mpdccp_pm_default_unregister (void)
+{
+	mpdccp_pm_exit();
+	mpdccp_unregister_path_manager(&mpdccp_pm_default);
+}
+
+
+
diff --git a/net/dccp/probe.c b/net/dccp/probe.c
new file mode 100644
index 0000000000000..3d3fda05b32d7
--- /dev/null
+++ b/net/dccp/probe.c
@@ -0,0 +1,203 @@
+/*
+ * dccp_probe - Observe the DCCP flow with kprobes.
+ *
+ * The idea for this came from Werner Almesberger's umlsim
+ * Copyright (C) 2004, Stephen Hemminger <shemminger@osdl.org>
+ *
+ * Modified for DCCP from Stephen Hemminger's code
+ * Copyright (C) 2006, Ian McDonald <ian.mcdonald@jandi.co.nz>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/kernel.h>
+#include <linux/kprobes.h>
+#include <linux/socket.h>
+#include <linux/dccp.h>
+#include <linux/proc_fs.h>
+#include <linux/module.h>
+#include <linux/kfifo.h>
+#include <linux/vmalloc.h>
+#include <linux/time64.h>
+#include <linux/gfp.h>
+#include <net/net_namespace.h>
+
+#include "dccp.h"
+#include "ccid.h"
+#include "ccids/ccid3.h"
+
+static int port;
+
+static int bufsize = 64 * 1024;
+
+static const char procname[] = "dccpprobe";
+
+static struct {
+	struct kfifo	  fifo;
+	spinlock_t	  lock;
+	wait_queue_head_t wait;
+	struct timespec64 tstart;
+} dccpw;
+
+static void printl(const char *fmt, ...)
+{
+	va_list args;
+	int len;
+	struct timespec64 now;
+	char tbuf[256];
+
+	va_start(args, fmt);
+	getnstimeofday64(&now);
+
+	now = timespec64_sub(now, dccpw.tstart);
+
+	len = sprintf(tbuf, "%lu.%06lu ",
+		      (unsigned long) now.tv_sec,
+		      (unsigned long) now.tv_nsec / NSEC_PER_USEC);
+	len += vscnprintf(tbuf+len, sizeof(tbuf)-len, fmt, args);
+	va_end(args);
+
+	kfifo_in_locked(&dccpw.fifo, tbuf, len, &dccpw.lock);
+	wake_up(&dccpw.wait);
+}
+
+static int jdccp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)
+{
+	const struct inet_sock *inet = inet_sk(sk);
+	struct ccid3_hc_tx_sock *hc = NULL;
+
+	if (ccid_get_current_tx_ccid(dccp_sk(sk)) == DCCPC_CCID3)
+		hc = ccid3_hc_tx_sk(sk);
+
+	if (port == 0 || ntohs(inet->inet_dport) == port ||
+	    ntohs(inet->inet_sport) == port) {
+		if (hc)
+			printl("%pI4:%u %pI4:%u %d %d %d %d %u %llu %llu %d\n",
+			       &inet->inet_saddr, ntohs(inet->inet_sport),
+			       &inet->inet_daddr, ntohs(inet->inet_dport), size,
+			       hc->tx_s, hc->tx_rtt, hc->tx_p,
+			       hc->tx_x_calc, hc->tx_x_recv >> 6,
+			       hc->tx_x >> 6, hc->tx_t_ipi);
+		else
+			printl("%pI4:%u %pI4:%u %d\n",
+			       &inet->inet_saddr, ntohs(inet->inet_sport),
+			       &inet->inet_daddr, ntohs(inet->inet_dport),
+			       size);
+	}
+
+	jprobe_return();
+	return 0;
+}
+
+static struct jprobe dccp_send_probe = {
+	.kp	= {
+		.symbol_name = "dccp_sendmsg",
+	},
+	.entry	= jdccp_sendmsg,
+};
+
+static int dccpprobe_open(struct inode *inode, struct file *file)
+{
+	kfifo_reset(&dccpw.fifo);
+	getnstimeofday64(&dccpw.tstart);
+	return 0;
+}
+
+static ssize_t dccpprobe_read(struct file *file, char __user *buf,
+			      size_t len, loff_t *ppos)
+{
+	int error = 0, cnt = 0;
+	unsigned char *tbuf;
+
+	if (!buf)
+		return -EINVAL;
+
+	if (len == 0)
+		return 0;
+
+	tbuf = vmalloc(len);
+	if (!tbuf)
+		return -ENOMEM;
+
+	error = wait_event_interruptible(dccpw.wait,
+					 kfifo_len(&dccpw.fifo) != 0);
+	if (error)
+		goto out_free;
+
+	cnt = kfifo_out_locked(&dccpw.fifo, tbuf, len, &dccpw.lock);
+	error = copy_to_user(buf, tbuf, cnt) ? -EFAULT : 0;
+
+out_free:
+	vfree(tbuf);
+
+	return error ? error : cnt;
+}
+
+static const struct file_operations dccpprobe_fops = {
+	.owner	 = THIS_MODULE,
+	.open	 = dccpprobe_open,
+	.read    = dccpprobe_read,
+	.llseek  = noop_llseek,
+};
+
+static __init int dccpprobe_init(void)
+{
+	int ret = -ENOMEM;
+
+	init_waitqueue_head(&dccpw.wait);
+	spin_lock_init(&dccpw.lock);
+	if (kfifo_alloc(&dccpw.fifo, bufsize, GFP_KERNEL))
+		return ret;
+	if (!proc_create(procname, S_IRUSR, init_net.proc_net, &dccpprobe_fops))
+		goto err0;
+
+	ret = register_jprobe(&dccp_send_probe);
+	if (ret) {
+		ret = request_module("dccp");
+		if (!ret)
+			ret = register_jprobe(&dccp_send_probe);
+	}
+
+	if (ret)
+		goto err1;
+
+	pr_info("DCCP watch registered (port=%d)\n", port);
+	return 0;
+err1:
+	remove_proc_entry(procname, init_net.proc_net);
+err0:
+	kfifo_free(&dccpw.fifo);
+	return ret;
+}
+module_init(dccpprobe_init);
+
+static __exit void dccpprobe_exit(void)
+{
+	kfifo_free(&dccpw.fifo);
+	remove_proc_entry(procname, init_net.proc_net);
+	unregister_jprobe(&dccp_send_probe);
+
+}
+module_exit(dccpprobe_exit);
+
+MODULE_PARM_DESC(port, "Port to match (0=all)");
+module_param(port, int, 0);
+
+MODULE_PARM_DESC(bufsize, "Log buffer size (default 64k)");
+module_param(bufsize, int, 0);
+
+MODULE_AUTHOR("Ian McDonald <ian.mcdonald@jandi.co.nz>");
+MODULE_DESCRIPTION("DCCP snooper");
+MODULE_LICENSE("GPL");
diff --git a/net/dccp/proto.c b/net/dccp/proto.c
index fcc5c9d64f466..0187c90471551 100644
--- a/net/dccp/proto.c
+++ b/net/dccp/proto.c
@@ -34,10 +34,19 @@
 #include "ccid.h"
 #include "dccp.h"
 #include "feat.h"
+#include <linux/notifier.h>
+
+#include <net/mpdccp.h>
 
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+struct mpdccp_funcs      mpdccp_funcs = { .magic = 0, };
+EXPORT_SYMBOL (mpdccp_funcs);
+#endif
+
+
 DEFINE_SNMP_STAT(struct dccp_mib, dccp_statistics) __read_mostly;
 
 EXPORT_SYMBOL_GPL(dccp_statistics);
@@ -50,6 +59,11 @@ EXPORT_SYMBOL_GPL(dccp_hashinfo);
 
 /* the maximum queue length for tx in packets. 0 is no limit */
 int sysctl_dccp_tx_qlen __read_mostly = 5;
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+int sysctl_dccp_keepalive_enable __read_mostly = 0;
+int sysctl_dccp_keepalive_snd_intvl __read_mostly = DCCP_KEEPALIVE_SND_INTVL;
+int sysctl_dccp_keepalive_rcv_intvl __read_mostly = DCCP_KEEPALIVE_RCV_INTVL;
+#endif
 
 #ifdef CONFIG_IP_DCCP_DEBUG
 static const char *dccp_state_name(const int state)
@@ -75,6 +89,9 @@ static const char *dccp_state_name(const int state)
 }
 #endif
 
+static ATOMIC_NOTIFIER_HEAD(dccp_chain);
+
+
 void dccp_set_state(struct sock *sk, const int state)
 {
 	const int oldstate = sk->sk_state;
@@ -87,11 +104,21 @@ void dccp_set_state(struct sock *sk, const int state)
 	case DCCP_OPEN:
 		if (oldstate != DCCP_OPEN)
 			DCCP_INC_STATS(DCCP_MIB_CURRESTAB);
+		
 		/* Client retransmits all Confirm options until entering OPEN */
 		if (oldstate == DCCP_PARTOPEN)
 			dccp_feat_list_purge(&dccp_sk(sk)->dccps_featneg);
 		break;
-
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	case DCCP_PARTOPEN:
+		if (dccp_sk(sk)->dccps_keepalive_enable && sk->sk_prot->keepalive)
+		{
+			dccp_pr_debug("set keepalive");
+			sk->sk_prot->keepalive(sk, 1);
+			sock_set_flag(sk, SOCK_KEEPOPEN);
+		}
+		break;		
+#endif
 	case DCCP_CLOSED:
 		if (oldstate == DCCP_OPEN || oldstate == DCCP_ACTIVE_CLOSEREQ ||
 		    oldstate == DCCP_CLOSING)
@@ -111,11 +138,17 @@ void dccp_set_state(struct sock *sk, const int state)
 	 * socket sitting in hash tables.
 	 */
 	inet_sk_set_state(sk, state);
+	if (sk->sk_state_change)
+		sk->sk_state_change (sk);
+	
+	if(oldstate == DCCP_OPEN && state==DCCP_CLOSING)
+		atomic_notifier_call_chain(&dccp_chain,
+						DCCP_EVENT_CLOSE, sk);
 }
 
 EXPORT_SYMBOL_GPL(dccp_set_state);
 
-static void dccp_finish_passive_close(struct sock *sk)
+void dccp_finish_passive_close(struct sock *sk)
 {
 	switch (sk->sk_state) {
 	case DCCP_PASSIVE_CLOSE:
@@ -132,6 +165,7 @@ static void dccp_finish_passive_close(struct sock *sk)
 		dccp_set_state(sk, DCCP_CLOSING);
 	}
 }
+EXPORT_SYMBOL(dccp_finish_passive_close);
 
 void dccp_done(struct sock *sk)
 {
@@ -205,6 +239,16 @@ int dccp_init_sock(struct sock *sk, const __u8 ctl_sock_initialized)
 	dp->dccps_role		= DCCP_ROLE_UNDEFINED;
 	dp->dccps_service	= DCCP_SERVICE_CODE_IS_ABSENT;
 	dp->dccps_tx_qlen	= sysctl_dccp_tx_qlen;
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	dp->dccps_keepalive_snd_intvl = sysctl_dccp_keepalive_snd_intvl;
+	dp->dccps_keepalive_rcv_intvl = sysctl_dccp_keepalive_rcv_intvl;
+	dp->dccps_keepalive_enable = sysctl_dccp_keepalive_enable;
+#endif
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	dp->multipath_ver = MPDCCP_VERS_UNDEFINED;
+#endif
+	dp->dccps_hc_rx_ccid = NULL;
+	dp->dccps_hc_tx_ccid = NULL;
 
 	dccp_init_xmit_timers(sk);
 
@@ -226,6 +270,11 @@ void dccp_destroy_sock(struct sock *sk)
 		kfree_skb(sk->sk_send_head);
 		sk->sk_send_head = NULL;
 	}
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if (mpdccp_is_meta(sk)) {
+		mpdccp_destroy_sock (sk);
+	}
+#endif
 
 	/* Clean up a referenced DCCP bind bucket. */
 	if (inet_csk(sk)->icsk_bind_hash != NULL)
@@ -260,6 +309,12 @@ int dccp_disconnect(struct sock *sk, int flags)
 	struct dccp_sock *dp = dccp_sk(sk);
 	const int old_state = sk->sk_state;
 
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if (mpdccp_is_meta(sk)) {
+		/* not implemented yet */
+	}
+#endif
+
 	if (old_state != DCCP_CLOSED)
 		dccp_set_state(sk, DCCP_CLOSED);
 
@@ -512,6 +567,7 @@ static int do_dccp_setsockopt(struct sock *sk, int level, int optname,
 	struct dccp_sock *dp = dccp_sk(sk);
 	int val, err = 0;
 
+	dccp_pr_debug ("setsockopt: %d\n", optname);
 	switch (optname) {
 	case DCCP_SOCKOPT_PACKET_SIZE:
 		DCCP_WARN("sockopt(PACKET_SIZE) is deprecated: fix your app\n");
@@ -563,6 +619,19 @@ static int do_dccp_setsockopt(struct sock *sk, int level, int optname,
 		else
 			dp->dccps_tx_qlen = val;
 		break;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	case DCCP_SOCKOPT_MULTIPATH:
+		err = mpdccp_activate (sk, val);
+		break;
+#endif
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	case DCCP_SOCKOPT_KEEPALIVE:
+		dp->dccps_keepalive_snd_intvl = val;
+		break;
+	case DCCP_SOCKOPT_KA_TIMEOUT:
+		dp->dccps_keepalive_rcv_intvl = val;
+		break;
+#endif
 	default:
 		err = -ENOPROTOOPT;
 		break;
@@ -663,6 +732,19 @@ static int do_dccp_getsockopt(struct sock *sk, int level, int optname,
 	case DCCP_SOCKOPT_QPOLICY_TXQLEN:
 		val = dp->dccps_tx_qlen;
 		break;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	case DCCP_SOCKOPT_MULTIPATH:
+		val = mpdccp_isactive (sk);
+		break;
+#endif
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	case DCCP_SOCKOPT_KEEPALIVE:
+		val = dp->dccps_keepalive_snd_intvl;
+		break;
+	case DCCP_SOCKOPT_KA_TIMEOUT:
+		val = dp->dccps_keepalive_rcv_intvl;
+		break;
+#endif
 	case 128 ... 191:
 		return ccid_hc_rx_getsockopt(dp->dccps_hc_rx_ccid, sk, optname,
 					     len, (u32 __user *)optval, optlen);
@@ -809,6 +891,28 @@ int dccp_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
 
 EXPORT_SYMBOL_GPL(dccp_sendmsg);
 
+static inline struct sk_buff *skb_peek_safe(struct sk_buff_head *list)
+{
+	unsigned long flags;
+	struct sk_buff *skb;
+
+	spin_lock_irqsave(&list->lock, flags);
+	skb = skb_peek(list);
+	spin_unlock_irqrestore(&list->lock, flags);
+
+	return skb;
+}
+
+static inline void sk_eat_skb_safe(struct sock *sk, struct sk_buff *skb)
+{
+	unsigned long flags;
+	struct sk_buff_head *list = &sk->sk_receive_queue;
+
+	spin_lock_irqsave(&list->lock, flags);
+	sk_eat_skb(sk,skb);
+	spin_unlock_irqrestore(&list->lock, flags);
+}
+
 int dccp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int flags,
 		 int *addr_len)
 {
@@ -825,7 +929,14 @@ int dccp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int flags,
 	timeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);
 
 	do {
-		struct sk_buff *skb = skb_peek(&sk->sk_receive_queue);
+		/* For meta sockets this can race with mpdccp_forward_skb(), need atomic access to rx queue */
+		struct sk_buff *skb;
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+		if (mpdccp_is_meta(sk))
+			skb = skb_peek_safe(&sk->sk_receive_queue);
+		else
+#endif
+			skb = skb_peek(&sk->sk_receive_queue);
 
 		if (skb == NULL)
 			goto verify_sock_status;
@@ -850,7 +961,13 @@ int dccp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int flags,
 		default:
 			dccp_pr_debug("packet_type=%s\n",
 				      dccp_packet_name(dh->dccph_type));
-			sk_eat_skb(sk, skb);
+			/* For meta sockets this can race with mpdccp_forward_skb(), need atomic access to rx queue */
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+			if (mpdccp_is_meta(sk))
+				sk_eat_skb_safe(sk, skb);
+			else
+#endif
+				sk_eat_skb(sk, skb);
 		}
 verify_sock_status:
 		if (sock_flag(sk, SOCK_DONE)) {
@@ -906,8 +1023,15 @@ int dccp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int flags,
 		if (flags & MSG_TRUNC)
 			len = skb->len;
 	found_fin_ok:
-		if (!(flags & MSG_PEEK))
-			sk_eat_skb(sk, skb);
+		if (!(flags & MSG_PEEK)) {
+			/* For meta sockets this can race with mpdccp_forward_skb(), need atomic access to rx queue */
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+			if (mpdccp_is_meta(sk))
+				sk_eat_skb_safe(sk, skb);
+			else
+#endif
+				sk_eat_skb(sk, skb);
+		}
 		break;
 	} while (1);
 out:
@@ -942,6 +1066,17 @@ int inet_dccp_listen(struct socket *sock, int backlog)
 
 		dp->dccps_role = DCCP_ROLE_LISTEN;
 
+		/* Register MP_CAPABLE feature for multipath sockets */
+		if (is_mpdccp(sk) || try_mpdccp(sk) == 1) {
+			int ret;
+			ret = dccp_feat_register_sp(sk, DCCPF_MULTIPATH, 1,
+							mpdccp_supported_versions,
+							ARRAY_SIZE(mpdccp_supported_versions));
+			if (ret < 0 ) {
+				return -EPROTO;
+			}
+		}
+
 		/* do not start to listen if feature negotiation setup fails */
 		if (dccp_feat_finalise_settings(dp)) {
 			err = -EPROTO;
@@ -973,9 +1108,26 @@ static void dccp_terminate_connection(struct sock *sk)
 	case DCCP_PARTOPEN:
 		dccp_pr_debug("Stop PARTOPEN timer (%p)\n", sk);
 		inet_csk_clear_xmit_timer(sk, ICSK_TIME_DACK);
+
+		if (is_mpdccp(sk) && dccp_sk(sk)->dccps_role == DCCP_ROLE_CLIENT) {
+			/* Stop the ACK retry timer */
+			inet_csk_clear_xmit_timer(sk, ICSK_TIME_RETRANS);
+			if (sk->sk_send_head != NULL) {
+				kfree_skb(sk->sk_send_head);
+				sk->sk_send_head = NULL;
+			}
+		}
 		fallthrough;
 	case DCCP_OPEN:
-		dccp_send_close(sk, 1);
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+		/* Don't send close pkt on MP meta sockets*/
+		if (!mpdccp_is_meta(sk)) {
+			if (dccp_sk(sk)->is_fast_close)
+				dccp_send_reset(sk, DCCP_RESET_CODE_MPDCCP_ABORTED);
+			else
+				dccp_send_close(sk, 1);
+		}
+#endif
 
 		if (dccp_sk(sk)->dccps_role == DCCP_ROLE_SERVER &&
 		    !dccp_sk(sk)->dccps_server_timewait)
@@ -986,6 +1138,12 @@ static void dccp_terminate_connection(struct sock *sk)
 	default:
 		dccp_set_state(sk, next_state);
 	}
+
+#if IS_ENABLED(CONFIG_IP_MPDCCP)
+	if (mpdccp_is_meta(sk)) {
+		mpdccp_close_meta(sk);
+	}
+#endif
 }
 
 void dccp_close(struct sock *sk, long timeout)
@@ -994,7 +1152,7 @@ void dccp_close(struct sock *sk, long timeout)
 	struct sk_buff *skb;
 	u32 data_was_unread = 0;
 	int state;
-
+	dccp_pr_debug("enter dccp_close (%p), timeout %ld \n", sk, timeout);
 	lock_sock(sk);
 
 	sk->sk_shutdown = SHUTDOWN_MASK;
@@ -1021,8 +1179,12 @@ void dccp_close(struct sock *sk, long timeout)
 	}
 
 	/* If socket has been already reset kill it. */
-	if (sk->sk_state == DCCP_CLOSED)
+	if (sk->sk_state == DCCP_CLOSED){
+		if (is_mpdccp(sk) && dp->is_fast_close) {
+			dccp_send_reset(sk, DCCP_RESET_CODE_MPDCCP_ABORTED);
+		}
 		goto adjudge_to_death;
+	}
 
 	if (data_was_unread) {
 		/* Unread data was tossed, send an appropriate Reset Code */
@@ -1048,7 +1210,6 @@ void dccp_close(struct sock *sk, long timeout)
 	 * - normal termination but queue could not be flushed within time limit
 	 */
 	__skb_queue_purge(&sk->sk_write_queue);
-
 	sk_stream_wait_close(sk, timeout);
 
 adjudge_to_death:
@@ -1094,6 +1255,24 @@ void dccp_shutdown(struct sock *sk, int how)
 
 EXPORT_SYMBOL_GPL(dccp_shutdown);
 
+/*
+ * dccp notifier
+ */
+
+
+int register_dccp_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_register(&dccp_chain, nb);
+}
+EXPORT_SYMBOL(register_dccp_notifier);
+
+int unregister_dccp_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_unregister(&dccp_chain, nb);
+}
+EXPORT_SYMBOL(unregister_dccp_notifier);
+
+
 static inline int __init dccp_mib_init(void)
 {
 	dccp_statistics = alloc_percpu(struct dccp_mib);
diff --git a/net/dccp/qpolicy.c b/net/dccp/qpolicy.c
index 5ba204ec0aca4..cd17e0e1a035b 100644
--- a/net/dccp/qpolicy.c
+++ b/net/dccp/qpolicy.c
@@ -60,6 +60,29 @@ static bool qpolicy_prio_full(struct sock *sk)
 	return false;
 }
 
+static bool qpolicy_drop_oldest_full(struct sock *sk)
+{
+	if (qpolicy_simple_full(sk))
+		dccp_qpolicy_drop(sk, qpolicy_simple_top(sk));
+	return false;
+}
+
+static bool qpolicy_drop_newest_full(struct sock *sk)
+{
+	return false;
+}
+
+static void qpolicy_drop_newest_push(struct sock *sk, struct sk_buff *skb)
+{
+	if (qpolicy_simple_full(sk)) {
+		/* drop packet - do not use dccp_qpolicy_drop here - packet is not yet linked */
+		kfree_skb(skb);
+	} else {
+		qpolicy_simple_push(sk, skb);
+	}
+}
+
+
 /**
  * struct dccp_qpolicy_operations  -  TX Packet Dequeueing Interface
  * @push: add a new @skb to the write queue
@@ -87,6 +110,18 @@ static struct dccp_qpolicy_operations qpol_table[DCCPQ_POLICY_MAX] = {
 		.top    = qpolicy_prio_best_skb,
 		.params = DCCP_SCM_PRIORITY,
 	},
+	[DCCPQ_POLICY_DROP_OLDEST] = {
+		.push	= qpolicy_simple_push,
+		.full	= qpolicy_drop_oldest_full,
+		.top	= qpolicy_simple_top,
+		.params = 0,
+	},
+	[DCCPQ_POLICY_DROP_NEWEST] = {
+		.push	= qpolicy_drop_newest_push,
+		.full	= qpolicy_drop_newest_full,
+		.top	= qpolicy_simple_top,
+		.params = 0,
+	},
 };
 
 /*
@@ -96,11 +131,13 @@ void dccp_qpolicy_push(struct sock *sk, struct sk_buff *skb)
 {
 	qpol_table[dccp_sk(sk)->dccps_qpolicy].push(sk, skb);
 }
+EXPORT_SYMBOL_GPL(dccp_qpolicy_push);
 
 bool dccp_qpolicy_full(struct sock *sk)
 {
 	return qpol_table[dccp_sk(sk)->dccps_qpolicy].full(sk);
 }
+EXPORT_SYMBOL_GPL(dccp_qpolicy_full);
 
 void dccp_qpolicy_drop(struct sock *sk, struct sk_buff *skb)
 {
@@ -109,23 +146,31 @@ void dccp_qpolicy_drop(struct sock *sk, struct sk_buff *skb)
 		kfree_skb(skb);
 	}
 }
+EXPORT_SYMBOL_GPL(dccp_qpolicy_drop);
 
 struct sk_buff *dccp_qpolicy_top(struct sock *sk)
 {
 	return qpol_table[dccp_sk(sk)->dccps_qpolicy].top(sk);
 }
+EXPORT_SYMBOL_GPL(dccp_qpolicy_top);
 
 struct sk_buff *dccp_qpolicy_pop(struct sock *sk)
 {
 	struct sk_buff *skb = dccp_qpolicy_top(sk);
 
-	if (skb != NULL) {
-		/* Clear any skb fields that we used internally */
-		skb->priority = 0;
-		skb_unlink(skb, &sk->sk_write_queue);
-	}
+	dccp_qpolicy_unlink (sk, skb);
 	return skb;
 }
+EXPORT_SYMBOL_GPL(dccp_qpolicy_pop);
+
+void dccp_qpolicy_unlink (struct sock *sk, struct sk_buff *skb)
+{
+	if (!sk || !skb) return;
+	/* Clear any skb fields that we used internally */
+	skb->priority = 0;
+	skb_unlink(skb, &sk->sk_write_queue);
+}
+EXPORT_SYMBOL_GPL(dccp_qpolicy_unlink);
 
 bool dccp_qpolicy_param_ok(struct sock *sk, __be32 param)
 {
@@ -134,3 +179,6 @@ bool dccp_qpolicy_param_ok(struct sock *sk, __be32 param)
 		return false;
 	return (qpol_table[dccp_sk(sk)->dccps_qpolicy].params & param) == param;
 }
+EXPORT_SYMBOL_GPL(dccp_qpolicy_param_ok);
+
+
diff --git a/net/dccp/reordering/Kconfig b/net/dccp/reordering/Kconfig
new file mode 100644
index 0000000000000..c8173e3fabbbf
--- /dev/null
+++ b/net/dccp/reordering/Kconfig
@@ -0,0 +1,26 @@
+
+# Reordering
+menu  "MPDCCP advanced reordering control"
+
+endmenu
+
+# The following doesn't work, yet - tbd.
+#choice
+#	prompt "Default MPDCCP Reordering Engine"
+#	default DEFAULT
+#	help
+#	  Select the Reordering Engine of your choice
+#
+#	config DEFAULT_REORDER_DEFAULT
+#		bool "Default (Passive)"
+#		---help---
+#		  This is the default reordering engine.
+#
+#endchoice
+
+config DEFAULT_MPDCCP_REORDER
+	string
+	depends on IP_MPDCCP
+	default "default"
+
+
diff --git a/net/dccp/reordering/Makefile b/net/dccp/reordering/Makefile
new file mode 100644
index 0000000000000..b335318015bd5
--- /dev/null
+++ b/net/dccp/reordering/Makefile
@@ -0,0 +1,3 @@
+# SPDX-License-Identifier: GPL-2.0
+
+
diff --git a/net/dccp/reordering/ro_default.c b/net/dccp/reordering/ro_default.c
new file mode 100644
index 0000000000000..0cd127b4d4279
--- /dev/null
+++ b/net/dccp/reordering/ro_default.c
@@ -0,0 +1,92 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ * 
+ * Copyright (C) 2018 by Maximilian Schuengel, Deutsche Telekom AG
+ * Copyright (C) 2018 by Markus Amend, Deutsche Telekom AG
+ *
+ * MPDCCP - Generic reordering functions.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/module.h>
+#include <linux/hrtimer.h>
+#include <linux/time.h>
+#include <linux/ktime.h>
+
+#include "../mpdccp.h"
+#include "../mpdccp_reordering.h"
+
+
+/************************************************* 
+ *    Default Reordering
+ *************************************************/
+
+/**
+ * Initialize default reordering module.
+ */ 
+static void init_reorder_default (struct mpdccp_cb *mpcb)
+{
+	mpcb->mpdccp_reorder_cb = NULL;
+	ro_info("RO-INFO: default reordering module active [default module]\n"); 
+}
+
+/**
+ * Queue work item.
+ */
+static void do_reorder_default(struct rcv_buff *rb)
+{
+	int	ret;
+
+	//mpdccp_pr_debug ("do_reorder_default: called\n");
+	if (!rb) {
+		ro_err("RO-ERROR: w is NULL\n"); 
+		return;
+	}
+	ret = mpdccp_forward_skb(rb->skb, rb->mpcb);
+	if (ret < 0)
+		printk ("do_reorder_default: error in forward: %d\n", ret);
+	rb->mpcb->glob_lfor_seqno = (u64)rb->oall_seqno;
+	mpdccp_release_rcv_buff(&rb);
+	return; 
+}
+
+void do_update_pseq(struct my_sock *my_sk, struct sk_buff *skb){}
+
+/**
+ * Initialize active reordering operations.
+ */
+struct mpdccp_reorder_ops mpdccp_reorder_default = {
+	.init		= init_reorder_default,
+	.do_reorder	= do_reorder_default,
+	.update_pseq = do_update_pseq,
+	.name		= "default",
+	.owner		= THIS_MODULE,
+};
+
+
+int mpdccp_reorder_default_register(void)
+{
+	if (mpdccp_register_reordering(&mpdccp_reorder_default))
+		return FAIL_RO; 
+	return SUCCESS_RO;
+}
+
+/** 
+ * Exit: unregister the default reordering module.
+ */
+void mpdccp_reorder_default_unregister (void)
+{
+	mpdccp_unregister_reordering(&mpdccp_reorder_default);
+}
+
diff --git a/net/dccp/scheduler/Kconfig b/net/dccp/scheduler/Kconfig
new file mode 100644
index 0000000000000..8bb62f8e23ed3
--- /dev/null
+++ b/net/dccp/scheduler/Kconfig
@@ -0,0 +1,82 @@
+
+menu "MPDCCP scheduler selection"
+
+
+config MPDCCP_SCHED_SRTT
+	tristate "MPDCCP Smoothed RTT"
+	depends on IP_MPDCCP
+	help
+	  A SRTT (Smoothed Round-Trip Time) based scheduler. It will send on the flow 
+	  that has the lowest SRTT and is available (i.e., free Congestion Window).
+
+config MPDCCP_SCHED_ROUNDROBIN
+	tristate "MPDCCP Round-Robin"
+	depends on IP_MPDCCP
+	help
+	  This is a very simple round-robin scheduler. Probably has bad performance
+	  but might be interesting for researchers.
+
+config MPDCCP_SCHED_REDUNDANT
+	tristate "MPDCCP Redundant"
+	depends on IP_MPDCCP
+	help
+	  This is a redundant scheduler for latency reduction and higher reliability.
+	  Each packet is sent on all available flows.
+
+
+config MPDCCP_SCHED_OTIAS
+	tristate "MPDCCP OTIAS"
+	depends on IP_MPDCCP
+	help
+	  An OTIAS Scheduler. It tries to estimate link latencies and schedules 
+	  packets accordingly so that packets arrive in order.
+
+endmenu
+
+if IP_MPDCCP
+
+# The following doesn't work yet - tbd.
+#choice
+#	prompt "Default MPDCCP Scheduler"
+#	default DEFAULT
+#	help
+#	  Select the prefered default scheduler
+#
+#	config DEFAULT_SCHED_SRTT
+#		bool "Smoothed RTT" if MPDCCP_SCHED_SRTT
+#		---help---
+#		  This is the SRTT-based scheduler, tries to use the flow with
+#		  the lowest SRTT.
+#
+#	config DEFAULT_SCHED_ROUNDROBIN
+#		bool "Round-Robin" if MPDCCP_SCHED_ROUNDROBIN
+#		---help---
+#		  This is the round-robin scheduler, alternating between flows.
+#
+#	config DEFAULT_SCHED_REDUNDANT
+#		bool "Redundant" if MPDCCP_SCHED_REDUNDANT
+#		---help---
+#		  This is the redundant scheduler, which sends each packet on
+#		  all flows.
+#
+#	config DEFAULT_SCHED_OTIAS
+#		bool "OTIAS" if MPDCCP_SCHED_OTIAS
+#		---help---
+#		  This is the OTIAS scheduler, it tries to avoid
+#	  	  the need for a reordering entity.
+#
+#endchoice
+
+config DEFAULT_MPDCCP_SCHED
+	string
+	depends on IP_MPDCCP
+#	default "rr" if DEFAULT_SCHED_ROUNDROBIN
+#	default "redundant" if DEFAULT_SCHED_REDUNDANT
+#	default "srtt" if DEFAULT_SCHED_SRTT
+#	default "ratio" if DEFAULT_SCHED_RATIO
+#	default "otias" if DEFAULT_SCHED_OTIAS
+#	default "cpf" if DEFAULT_SCHED_CPF
+#	default "handover" if DEFAULT_SCHED_HANDOVER
+	default "default"
+
+endif # IP_MPDCCP
diff --git a/net/dccp/scheduler/Makefile b/net/dccp/scheduler/Makefile
new file mode 100644
index 0000000000000..fbd19a771bff4
--- /dev/null
+++ b/net/dccp/scheduler/Makefile
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0
+
+# MPDCCP scheduler modules
+obj-$(CONFIG_MPDCCP_SCHED_SRTT) += mpdccp_sched_srtt.o
+mpdccp_sched_srtt-y := sched_srtt.o
+
+obj-$(CONFIG_MPDCCP_SCHED_ROUNDROBIN) += mpdccp_sched_rr.o
+mpdccp_sched_rr-y := sched_rr.o
+
+obj-$(CONFIG_MPDCCP_SCHED_REDUNDANT) += mpdccp_sched_redundant.o
+mpdccp_sched_redundant-y := sched_redundant.o
+
+obj-$(CONFIG_MPDCCP_SCHED_OTIAS) += mpdccp_sched_otias.o
+mpdccp_sched_otias-y := sched_otias.o
diff --git a/net/dccp/scheduler/sched_default.c b/net/dccp/scheduler/sched_default.c
new file mode 100644
index 0000000000000..7778c68503eba
--- /dev/null
+++ b/net/dccp/scheduler/sched_default.c
@@ -0,0 +1,66 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2021 by Frank Reker, Deutsche Telekom AG
+ *
+ * MPDCCP - Default scheduler kernel module
+ *
+ * It returns the first available sk
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 3 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/module.h>
+#include <linux/rculist.h>
+
+#include <net/mpdccp_link.h>
+#include <net/mpdccp_link_info.h>
+#include "../mpdccp.h"
+#include "../mpdccp_scheduler.h"
+
+
+struct sock *sched_default (struct mpdccp_cb *mpcb)
+{
+    struct sock	*sk;
+
+    rcu_read_lock();
+    mpdccp_for_each_sk(mpcb, sk) {
+        if(!mpdccp_sk_can_send(sk) || !mpdccp_packet_fits_in_cwnd(sk)) continue;
+        rcu_read_unlock();
+        return sk;
+    }
+
+    rcu_read_unlock();
+    return NULL;
+}
+
+
+struct mpdccp_sched_ops sched_default_ops = {
+	.get_subflow	= sched_default,
+	.name		= "default",
+	.owner		= THIS_MODULE,
+};
+
+
+int mpdccp_sched_default_register (void)
+{
+	if (mpdccp_register_scheduler(&sched_default_ops))
+		return -1;
+	return 0;
+}
+
+void mpdccp_sched_default_unregister (void)
+{
+    mpdccp_unregister_scheduler(&sched_default_ops);
+}
+
diff --git a/net/dccp/scheduler/sched_otias.c b/net/dccp/scheduler/sched_otias.c
new file mode 100644
index 0000000000000..498ca9881c5b9
--- /dev/null
+++ b/net/dccp/scheduler/sched_otias.c
@@ -0,0 +1,170 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2018 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2018 by Markus Amend, Deutsche Telekom AG
+ *
+ * MPDCCP - OTIAS scheduler kernel module
+ *
+ * OTIAS - An Out-of-order Transmission for In-order Arrival Scheduler. 
+ * It tries to estimate link latencies and schedules packets accordingly 
+ * so that packets arrive in order.
+ * 
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 3 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/module.h>
+
+#include "../mpdccp.h"
+#include "../mpdccp_scheduler.h"
+
+/* NOTE: Currently, the scheduler will block if this is enabled and a flow is no longer available. */
+static bool overall_cwnd __read_mostly = 0;
+module_param(overall_cwnd, bool, 0644);
+MODULE_PARM_DESC(overall_cwnd, "if set to 1, the scheduler is limited by the smallest congestion window");
+
+static uint log_packet_length __read_mostly = 1228;
+module_param(log_packet_length, uint, 0644);
+MODULE_PARM_DESC(log_packet_length, "Only log packets with this length. Default 1228 (1200 byte payload + 28 byte header)");
+
+
+struct sock *mpdccp_otiassched(struct mpdccp_cb *mpcb)
+{
+	struct sock		*sk;
+	struct ccid2_hc_tx_sock	*hc;
+	u32			estd_arrival_time, rtt_to_wait, iperf_sequence_number;
+	s64			packet_can_be_sent, packets_must_wait;
+	struct sock		*meta_sk = mpcb ? mpcb->meta_sk : NULL;
+	struct sk_buff		*next_skb;
+	struct sock		*best_sk    = NULL;
+	
+	/* Initialise to arbitrarily high (max) value */
+	u32			min_arrival_time  = ~((u32)0);
+	
+	if(!meta_sk)
+		return NULL;
+
+	next_skb = dccp_qpolicy_top (meta_sk);
+	
+	if(next_skb->len == log_packet_length) {
+		memcpy(&iperf_sequence_number, next_skb->data+28, min(next_skb->len, (unsigned int)4));
+		iperf_sequence_number = ntohl(iperf_sequence_number);
+#if 0
+	} else {
+		printk(KERN_WARNING "mismatch size %u", next_skb->len);
+#endif
+	}
+	
+	/* if there is only 1 subflow, we bypass scheduling */
+	if(mpcb->cnt_subflows == 1) {
+		mpdccp_pr_debug("Only 1 socket available. Skipping selection.\n");
+		return mpdccp_return_single_flow(mpcb);
+	}
+
+#if 0	
+	/* Warning: Enabling this will stop the transmission if even one out of all
+	 * flows is currently unavailable (not established or cwnd full). This will 
+	 * halt your transmission. You have been warned. */
+	if(unlikely(overall_cwnd)) {
+		rcu_read_lock();
+		mpdccp_for_each_sk(mpcb, sk) {
+		    if(!mpdccp_sk_can_send(sk) || !mpdccp_packet_fits_in_cwnd(sk)) {
+		    	mpdccp_pr_debug("Overall congestion window enabled: Halt on full cwnd on sk %p.\n", sk);
+		    	return NULL;
+		    }
+		}
+		rcu_read_unlock();
+	}
+#endif
+	
+	rcu_read_lock();
+	mpdccp_for_each_sk(mpcb, sk) {
+		/* Overall cwnd guarantees that all flows are available before even starting
+		 * to send (see above). No need to check each socket twice. */
+		if(!unlikely(overall_cwnd)) {
+			/* Skip sockets that are still in handshake or where
+			 * the send queue is full */
+			
+			if(!mpdccp_sk_can_send(sk)) {
+				mpdccp_pr_debug("Flow %p not established. Continuing...\n", sk);
+				continue;
+			}
+			
+			if(dccp_qpolicy_full(sk)) {
+				mpdccp_pr_debug("Packet does not fit in send queue of %p. Continuing...\n", sk);
+				continue;
+			}
+		}
+		
+		hc = ccid2_hc_tx_sk(sk);
+		
+		/* Is there space in the cwnd? */
+		packet_can_be_sent  =   hc->tx_cwnd - hc->tx_pipe;
+		
+		/* How long do we have to wait to send? 
+		 * If we have more free cwnd than packets sitting in queue, we don't have to wait at all*/
+		packets_must_wait = sk->sk_write_queue.qlen - packet_can_be_sent;
+		
+		if(packets_must_wait < 0)
+			rtt_to_wait = 0;
+		else
+			rtt_to_wait = packets_must_wait / hc->tx_cwnd;
+		
+		/* Estimate packet arrival time, choose fastest socket */
+		estd_arrival_time   =   (rtt_to_wait * hc->tx_srtt) + (hc->tx_srtt / 2);
+		
+		if (estd_arrival_time < min_arrival_time) {
+			min_arrival_time = estd_arrival_time;
+			best_sk = sk;
+		}
+		if(next_skb->len == log_packet_length)
+			printk(KERN_WARNING "OTIAS iperf seq %u: sk %p, srtt %d, buffer %d, "
+		    		"cwnd %d, pipe %d, packets_must_wait %lld, estd_arrival_time %d", 
+					iperf_sequence_number, sk, hc->tx_srtt, sk->sk_write_queue.qlen,
+				hc->tx_cwnd, hc->tx_pipe, packets_must_wait, estd_arrival_time);
+	}
+	rcu_read_unlock();
+	
+	if(next_skb->len == log_packet_length)
+		printk(KERN_WARNING "OTIAS iperf seq %u: chosen sk %p", iperf_sequence_number, best_sk);
+	return best_sk;
+}
+
+
+struct mpdccp_sched_ops mpdccp_sched_otias = {
+	.get_subflow	= mpdccp_otiassched,
+	.name		= "otias",
+	.owner		= THIS_MODULE,
+};
+
+static int __init mpdccp_otiassched_register(void)
+{
+	if (mpdccp_register_scheduler(&mpdccp_sched_otias))
+		return -1;
+	return 0;
+}
+
+static void mpdccp_otiassched_unregister(void)
+{
+	mpdccp_unregister_scheduler(&mpdccp_sched_otias);
+}
+
+module_init(mpdccp_otiassched_register);
+module_exit(mpdccp_otiassched_unregister);
+
+MODULE_AUTHOR("Andreas Ph. Matz");
+MODULE_AUTHOR("Markus Amend");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Multipath DCCP OTIAS Scheduler");
+MODULE_VERSION(MPDCCP_VERSION);
+
diff --git a/net/dccp/scheduler/sched_redundant.c b/net/dccp/scheduler/sched_redundant.c
new file mode 100644
index 0000000000000..0f673d8d31ec0
--- /dev/null
+++ b/net/dccp/scheduler/sched_redundant.c
@@ -0,0 +1,141 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2018 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2018 by Markus Amend, Deutsche Telekom AG
+ *
+ * MPDCCP - Redundant scheduler kernel module
+ *
+ * A redundant scheduler. It will alternate between the available
+ * sockets and queue a packet on as many as possible, provided they
+ * have a free cwnd for both data in send queue and the new skb.
+ * 
+ * Note: 
+ * This scheduler currently is essentially a hack to get around API 
+ * limitations. We can only return one sk to send on, so the scheduler
+ * itself will send on all other flows. This will change as soon as 
+ * the API is updated.
+ * 
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 3 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/module.h>
+
+#include "../mpdccp.h"
+#include "../mpdccp_scheduler.h"
+
+
+/* One socket is returned by get_subflow operation. For all other flows, 
+ * get_subflow will call this function which mimics the behavior of the 
+ * original code path so that we can send packets on all flows without 
+ * making changes to the API. */
+static int redsched_transmit_on_flow(struct mpdccp_cb *mpcb, struct sock *sk)
+{
+	int		ret;
+	struct sock	*meta_sk;
+	struct sk_buff 	*skb, *skb2;
+	
+	if (!mpcb || !mpcb->meta_sk)
+		return -EINVAL;
+
+	meta_sk = mpcb->meta_sk;
+	skb2 = dccp_qpolicy_top (meta_sk);
+	//printk(KERN_INFO "inred skb2 %p", skb2);
+	skb = pskb_copy (skb2, GFP_KERNEL);
+	if (!skb) {
+		mpdccp_pr_debug ("cannot copy skb - dropping packet\n");
+		return -ENOMEM;
+	}
+	DCCP_SKB_CB(skb)->dccpd_mpseq = mpcb->mp_oall_seqno;
+	ret = mpdccp_xmit_to_sk (sk, skb);
+	if (ret < 0) {
+		mpdccp_pr_debug ("error in xmit: %d - dropping packet\n", ret);
+		kfree_skb(skb);
+		return ret;
+	}
+	return 0;
+}
+
+/* Iterate over all sockets, return one, and queue the packet
+ * on the rest. This gives us a redundant behavior without the
+ * need to overthrow the entire scheduler API. */
+struct sock *mpdccp_redsched(struct mpdccp_cb *mpcb)
+{
+	int		ret;
+	struct sock	*sk, *best_sk = NULL;
+	
+	/* if there is only 1 subflow, we bypass scheduling */
+	mpcb->do_incr_oallseq = false;
+	if(mpcb->cnt_subflows == 1) {
+		mpdccp_pr_debug("Only 1 socket available. Skipping selection.\n");
+		return mpdccp_return_single_flow(mpcb);
+	}
+	
+	rcu_read_lock();
+	mpdccp_for_each_sk(mpcb, sk) {
+		/* Skip sockets that are still in handshake or where
+		 * cwnd is full */
+		if(!mpdccp_sk_can_send(sk)) {
+			mpdccp_pr_debug("Flow %p not established. Continuing...\n", sk);
+			continue;
+		}
+		
+		if(!mpdccp_packet_fits_in_cwnd(sk)){ // && !dccp_ack_pending(sk)
+			mpdccp_pr_debug("Packet does not fit in cwnd of %p. Continuing...\n", sk);
+			continue;
+		}
+		
+		// save a single socket to return later
+		if (!best_sk){
+			best_sk = sk;
+			continue;
+		}
+
+		ret = redsched_transmit_on_flow(mpcb, sk);
+		if(ret){
+			mpdccp_pr_debug("Transmit failed on sk %p with error %d\n", sk, ret);
+		} 
+	}
+
+	rcu_read_unlock();
+	return best_sk;
+}
+
+
+struct mpdccp_sched_ops mpdccp_sched_red = {
+	.get_subflow	= mpdccp_redsched,
+	.name		= "redundant",
+	.owner		= THIS_MODULE,
+};
+
+static int __init mpdccp_redsched_register(void)
+{
+	if (mpdccp_register_scheduler(&mpdccp_sched_red))
+		return -1;
+	return 0;
+}
+
+static void mpdccp_redsched_unregister(void)
+{
+	mpdccp_unregister_scheduler(&mpdccp_sched_red);
+}
+
+module_init(mpdccp_redsched_register);
+module_exit(mpdccp_redsched_unregister);
+
+MODULE_AUTHOR("Andreas Ph. Matz");
+MODULE_AUTHOR("Markus Amend");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Multipath DCCP Redundant Scheduler");
+MODULE_VERSION(MPDCCP_VERSION);
+
diff --git a/net/dccp/scheduler/sched_rr.c b/net/dccp/scheduler/sched_rr.c
new file mode 100644
index 0000000000000..423f404612c33
--- /dev/null
+++ b/net/dccp/scheduler/sched_rr.c
@@ -0,0 +1,184 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2017 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2017 by Markus Amend, Deutsche Telekom AG
+ * Copyright (C) 2021 by Frank Reker, Deutsche Telekom AG
+ *
+ * MPDCCP - Round-Robin scheduler kernel module
+ *
+ * A round-robin style scheduler. It will alternate between the available
+ * sockets and return one after another that has a free cwnd for both data
+ * in send queue and the new skb.
+ * Heavily inspired by mpdccp_rr.c
+ * 
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 3 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+#include <linux/module.h>
+
+#include "../mpdccp.h"
+#include "../mpdccp_scheduler.h"
+
+// TODO: make this a sysctl variable so it can be changed during runtime
+static unsigned char num_segments __read_mostly = 1;
+module_param(num_segments, byte, 0644);
+MODULE_PARM_DESC(num_segments, "The number of consecutive segments that are part of a burst.\
+                                Must be a value between 1 and max(__u32)");
+// TODO: not yet implemented
+static bool overall_cwnd __read_mostly = 0;
+module_param(overall_cwnd, bool, 0644);
+MODULE_PARM_DESC(overall_cwnd, "if set to 1, the scheduler is limited by the smallest congestion window");
+
+// Private data, subflow-specific
+struct rrsched_priv {
+	__u32 quota;
+};
+
+static struct rrsched_priv *rrsched_get_priv(struct sock *sk)
+{
+	struct my_sock *my_sk = (struct my_sock *)sk->sk_user_data;
+	
+	return (struct rrsched_priv *)&my_sk->sched_priv[0];
+}
+
+static struct sock *mpdccp_rrsched (struct mpdccp_cb *mpcb)
+{
+	struct sock		*sk;
+	int			iter = 0, full_flows = 0;
+	struct sock		*best_sk = NULL;
+	struct rrsched_priv	*rr_priv;
+	
+	/* if there is only 1 subflow, we bypass scheduling */
+	if(mpcb->cnt_subflows == 1) {
+		mpdccp_pr_debug("Only 1 socket available. Skipping selection.\n");
+		return mpdccp_return_single_flow(mpcb);
+	}
+
+retry:
+	rcu_read_lock();
+	mpdccp_for_each_sk(mpcb, sk) {
+		/* Skip sockets that are still in handshake or where
+		 * cwnd is full */
+		if(!mpdccp_sk_can_send(sk)) {
+			mpdccp_pr_debug("Flow %p not established. Continuing...\n", sk);
+			continue;
+		}
+		
+		if(!mpdccp_packet_fits_in_cwnd(sk)){ //&& !dccp_ack_pending(sk)
+			mpdccp_pr_debug("Packet does not fit in cwnd of %p. Continuing...\n", sk);
+			continue;
+		}
+		
+		// Iter counts the sockets we considered for transmission
+		iter++;
+		rr_priv = rrsched_get_priv(sk);
+		
+		/* num_segments defines how many packets are sent on a flow before switching
+		 * to another one. There are three situations:
+		 * 1) We previously started to use this flow, but did not use up num_segments.
+		 * 2) We have not started to use this flow. Start sending away :-)
+		 * 3) This socket has a full quota. Use another one.
+		 */
+		if(rr_priv->quota > 0 && rr_priv->quota < num_segments) {
+			best_sk = sk;
+			break;
+		} else if(!rr_priv->quota) {
+			best_sk = sk;
+		} else if(rr_priv->quota >= num_segments) {
+			full_flows++;
+		}
+	}
+	rcu_read_unlock();
+	
+	if(iter && iter == full_flows) {
+		/* If we get here, all sockets have been used to their full quota.
+		 * Reset quota and retry. */
+		rcu_read_lock();
+		mpdccp_for_each_sk(mpcb, sk) {
+			rr_priv = rrsched_get_priv(sk);
+			rr_priv->quota = 0;
+		}
+		rcu_read_unlock();
+		
+		goto retry;
+	}
+	
+	if(best_sk) {
+		rr_priv = rrsched_get_priv(best_sk);
+		rr_priv->quota++;
+		
+		mpdccp_pr_debug("Round-Robin returned socket %p\n", best_sk);
+		return best_sk;
+	}
+	
+	return NULL;
+}
+
+
+static void rrsched_init_subflow (struct sock *sk)
+{
+	struct rrsched_priv *rr_priv;
+
+	if (!sk) return;
+	rr_priv = rrsched_get_priv(sk);
+	rr_priv->quota = 0;
+	return;
+}
+
+static void rrsched_init_conn(struct mpdccp_cb *mpcb)
+{
+	struct sock *sk;
+	
+	rcu_read_lock();
+	mpdccp_for_each_sk(mpcb, sk) {
+		rrsched_init_subflow (sk);
+	}
+	rcu_read_unlock();
+	
+	return;
+}
+
+
+struct mpdccp_sched_ops mpdccp_sched_rr = {
+	.get_subflow	= mpdccp_rrsched,
+	.init_subflow	= rrsched_init_subflow,
+	.init_conn	= rrsched_init_conn,
+	.name		= "rr",
+	.owner		= THIS_MODULE,
+};
+
+static int __init mpdccp_rrsched_register(void)
+{
+	BUILD_BUG_ON(sizeof(struct rrsched_priv) > MPDCCP_SCHED_SIZE);
+	
+	if (mpdccp_register_scheduler(&mpdccp_sched_rr))
+		return -1;
+	return 0;
+}
+
+static void mpdccp_rrsched_unregister(void)
+{
+	mpdccp_unregister_scheduler(&mpdccp_sched_rr);
+}
+
+module_init(mpdccp_rrsched_register);
+module_exit(mpdccp_rrsched_unregister);
+
+MODULE_AUTHOR("Andreas Ph. Matz");
+MODULE_AUTHOR("Markus Amend");
+MODULE_AUTHOR("Frank Reker");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Multipath DCCP Round-Robin Scheduler");
+MODULE_VERSION(MPDCCP_VERSION);
+
diff --git a/net/dccp/scheduler/sched_srtt.c b/net/dccp/scheduler/sched_srtt.c
new file mode 100644
index 0000000000000..0bcb6bb1d7540
--- /dev/null
+++ b/net/dccp/scheduler/sched_srtt.c
@@ -0,0 +1,215 @@
+/*  SPDX-License-Identifier: GNU General Public License v2 only (GPL-2.0-only)
+ *
+ * Copyright (C) 2018 by Andreas Philipp Matz, Deutsche Telekom AG
+ * Copyright (C) 2018 by Markus Amend, Deutsche Telekom AG
+ *
+ * MPDCCP - Smoothed-RTT based scheduler kernel module
+ *
+ * A SRTT based scheduler. It will return the flow with the
+ * lowest srtt that has a free cwnd for both the data in send 
+ * queue and the new skb.
+ * 
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 3 of the License, or
+ * (at your option) any later version.
+
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+*/
+
+
+#include <linux/module.h>
+#include <linux/timekeeping.h>
+
+#include "../mpdccp.h"
+#include "../mpdccp_scheduler.h"
+
+// Bandwidth and SRTT logging
+#define MPDCCP_SRTT_LOG_BW 1
+
+// Private data, connection-specific 
+struct srttsched_priv {
+	u8		socket_number;
+	u64		tx_bytepersecond;
+	time64_t	last_timestamp;
+};
+
+static struct srttsched_priv *srttsched_get_priv(struct sock *sk)
+{
+	struct my_sock *my_sk = (struct my_sock *)sk->sk_user_data;
+	
+	return (struct srttsched_priv *)&my_sk->sched_priv[0];
+}
+
+static struct sock *mpdccp_srttsched(struct mpdccp_cb *mpcb)
+{
+	struct ccid2_hc_tx_sock *hc;
+	struct sock		*sk;
+	struct sock		*best_sk = NULL;
+	
+	/* Initialise to arbitrarily high (max) value */
+	u32			min_srtt = ~((u32)0);
+
+#if defined MPDCCP_SRTT_LOG_BW && 0
+	struct srttsched_priv	*srtt_priv;
+	//struct timeval		tv;
+	time64_t tstamp
+	struct sk_buff		*next_skb;
+#endif
+
+	//* if there is only 1 subflow, we bypass scheduling */
+	if(mpcb->cnt_subflows == 1) {
+		dccp_pr_debug("Only 1 socket available. Skipping selection.\n");
+		return mpdccp_return_single_flow(mpcb);
+	}
+	
+	rcu_read_lock();
+	mpdccp_for_each_sk(mpcb, sk) {
+		/* Skip sockets that are still in handshake or where
+		 * cwnd is full */
+		if (!mpdccp_sk_can_send(sk)) {
+			mpdccp_pr_debug("Flow %p not established. Continuing...\n", sk);
+			continue;
+		}
+		
+		if (!mpdccp_packet_fits_in_cwnd(sk)){ // && !dccp_ack_pending(sk)
+			mpdccp_pr_debug("Packet does not fit in cwnd of %p. Continuing...\n", sk);
+			continue;
+		}
+		
+		hc = ccid2_hc_tx_sk(sk);
+		//printk(KERN_ERR "SRTT of flow #%d is %d", i, hc->tx_srtt);
+		if (hc->tx_srtt < min_srtt) {
+			min_srtt = hc->tx_srtt;
+			best_sk  = sk;
+#if 0
+		} else {
+			/*
+			 * This is the loosing socket. If we do not get any traffic, how
+			 * should we ever know that our path properties changed? 
+			 *
+			 * After one RTO period without any packets sent, we should give
+			 * that socket a chance.
+			 */
+			if (ccid2_time_stamp > (hc->tx_lsndtime + hc->tx_rto))
+				return sk;
+#endif
+		}
+	}
+	rcu_read_unlock();
+
+#if defined MPDCCP_SRTT_LOG_BW && 0
+	// No best_sk = no socket available
+	if(best_sk) {
+		next_skb = mpcb && mpcbp->meta_sk ? dccp_qpolicy_top (mpcb->meta_sk) : NULL;
+		//do_gettimeofday(&tv);
+		tstamp = ktime_get_seconds();
+
+		
+		// For each socket, we log time stamp, bandwidth and srtt
+		rcu_read_lock();
+		mpdccp_for_each_sk(mpcb, sk) {
+			srtt_priv = srttsched_get_priv(sk);
+			
+			if (tstamp == srtt_priv->last_timestamp) {
+				if (!next_skb)
+					return NULL;
+				
+				// Count the packet towards the socket it will be sent on
+				if (sk == best_sk)
+					srtt_priv->tx_bytepersecond += next_skb->len;
+			} else {
+				printk(KERN_INFO "time: %lld sk: %d bw: %lld srtt: %u", 
+					(long long)srtt_priv->last_timestamp, srtt_priv->socket_number, 
+					srtt_priv->tx_bytepersecond, ccid2_hc_tx_sk(sk)->tx_srtt);
+				
+				// Reset byte counter and count the packet towards the socket it will be sent on
+				if(sk == best_sk)
+					srtt_priv->tx_bytepersecond    = next_skb->len;
+				else
+					srtt_priv->tx_bytepersecond    = 0;
+				
+				srtt_priv->last_timestamp      = tstamp;
+			}
+		}
+		rcu_read_unlock();
+	}
+#endif
+
+	mpdccp_pr_debug("SRTT scheduler returned socket %p\n", best_sk);
+	return best_sk;
+}
+
+
+
+static void srttsched_init_conn(struct mpdccp_cb *mpcb)
+{
+#ifdef MPDCCP_SRTT_LOG_BW
+	struct sock		*sk;
+	time64_t tstamp;
+	struct srttsched_priv	*srtt_priv;
+	int			i = 0;
+
+	mpdccp_pr_debug ("NOTE: Bandwidth logging enabled. See ring buffer for measurement.\n");
+	
+	rcu_read_lock();
+	mpdccp_for_each_sk(mpcb, sk) {
+		srtt_priv = srttsched_get_priv(sk);
+		
+		srtt_priv->socket_number = i;
+		i++;
+		
+		srtt_priv->tx_bytepersecond             = 0;
+		
+		//do_gettimeofday(&tv);
+		tstamp = ktime_get_seconds();
+		srttsched_get_priv(sk)->last_timestamp  = tstamp;
+	}
+	rcu_read_unlock();
+#endif
+	return;
+}
+
+static void srttsched_init_subflow (struct sock *sk)
+{
+	struct my_sock *my_sk = (struct my_sock *)sk->sk_user_data;
+	srttsched_init_conn (my_sk->mpcb);
+}
+
+struct mpdccp_sched_ops mpdccp_sched_srtt = {
+	.get_subflow	= mpdccp_srttsched,
+	.init_subflow	= srttsched_init_subflow,
+	.init_conn	= srttsched_init_conn,
+	.name		= "srtt",
+	.owner		= THIS_MODULE,
+};
+
+static int __init mpdccp_srttsched_register(void)
+{
+	BUILD_BUG_ON(sizeof(struct srttsched_priv) > MPDCCP_SCHED_SIZE);
+	
+	if (mpdccp_register_scheduler(&mpdccp_sched_srtt))
+		return -1;
+	return 0;
+}
+
+static void mpdccp_srttsched_unregister(void)
+{
+    mpdccp_unregister_scheduler(&mpdccp_sched_srtt);
+}
+
+module_init(mpdccp_srttsched_register);
+module_exit(mpdccp_srttsched_unregister);
+
+MODULE_AUTHOR("Andreas Ph. Matz");
+MODULE_AUTHOR("Markus Amend");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Multipath DCCP SRTT Scheduler");
+MODULE_VERSION(MPDCCP_VERSION);
+
diff --git a/net/dccp/sysctl.c b/net/dccp/sysctl.c
index ee8d4f5afa722..6aa47f321d02c 100644
--- a/net/dccp/sysctl.c
+++ b/net/dccp/sysctl.c
@@ -83,6 +83,29 @@ static struct ctl_table dccp_default_table[] = {
 		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= SYSCTL_ZERO,
 	},
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	{
+		.procname	= "dccp_keepalive_enable",
+		.data		= &sysctl_dccp_keepalive_enable,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "dccp_keepalive_snd_intvl",
+		.data		= &sysctl_dccp_keepalive_snd_intvl,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_jiffies,
+	},
+	{
+		.procname	= "dccp_keepalive_rcv_intvl",
+		.data		= &sysctl_dccp_keepalive_rcv_intvl,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_jiffies,
+	},
+#endif
 	{
 		.procname	= "sync_ratelimit",
 		.data		= &sysctl_dccp_sync_ratelimit,
diff --git a/net/dccp/timer.c b/net/dccp/timer.c
index b3255e87cc7e1..1a4c71d1d0468 100644
--- a/net/dccp/timer.c
+++ b/net/dccp/timer.c
@@ -158,11 +158,47 @@ static void dccp_write_timer(struct timer_list *t)
 	sock_put(sk);
 }
 
+
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+void dccp_set_keepalive(struct sock *sk, int val)
+{
+	if ((1 << sk->sk_state) & (DCCPF_CLOSED | DCCPF_LISTEN))
+		return;
+	dccp_pr_debug("set keepalive sk %p", sk);
+
+	if (val && !sock_flag(sk, SOCK_KEEPOPEN))
+	{
+		inet_csk_reset_keepalive_timer(sk, dccp_keepalive_snd_when(dccp_sk(sk)));
+		sk_reset_timer(sk, &dccp_sk(sk)->dccps_rcv_timer, jiffies + dccp_keepalive_rcv_when(dccp_sk(sk)));
+		dccp_pr_debug("after reset sendt %d, rcvt %d",  dccp_keepalive_snd_when(dccp_sk(sk)), dccp_keepalive_rcv_when(dccp_sk(sk)));
+	}
+	else if (!val)
+		inet_csk_delete_keepalive_timer(sk);
+}
+EXPORT_SYMBOL_GPL(dccp_set_keepalive);
+#endif
 static void dccp_keepalive_timer(struct timer_list *t)
 {
 	struct sock *sk = from_timer(sk, t, sk_timer);
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	struct dccp_sock *dp = dccp_sk(sk);
+	u32 elapsed;
 
-	pr_err("dccp should not use a keepalive timer !\n");
+	bh_lock_sock(sk);
+	if (sock_owned_by_user(sk)){
+		/* Try again later. */
+		inet_csk_reset_keepalive_timer (sk, HZ/20);
+		goto out;
+	}
+	dccp_pr_debug("enter dccp_keepalive_timer sk %p", sk);
+	//pr_err("dccp should not use a keepalive timer !\n");
+	elapsed = dccp_keepalive_snd_elapsed(dp);
+	if (elapsed > dccp_keepalive_snd_when(dp))
+		dccp_send_keepalive(sk);
+	inet_csk_reset_keepalive_timer(sk, dccp_keepalive_snd_when(dp));
+out:
+	bh_unlock_sock(sk);
+#endif
 	sock_put(sk);
 }
 
@@ -176,6 +212,7 @@ static void dccp_delack_timer(struct timer_list *t)
 	bh_lock_sock(sk);
 	if (sock_owned_by_user(sk)) {
 		/* Try again later. */
+printk ("delack blocked\n");
 		__NET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOCKED);
 		sk_reset_timer(sk, &icsk->icsk_delack_timer,
 			       jiffies + TCP_DELACK_MIN);
@@ -240,12 +277,52 @@ static void dccp_write_xmit_timer(struct timer_list *t)
 	dccp_write_xmitlet(&dp->dccps_xmitlet);
 }
 
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+static void dccp_rcv_timer(struct timer_list *t)
+{
+	//struct sock *sk = (struct sock *)data;
+	struct dccp_sock *dp = from_timer(dp, t, dccps_xmit_timer);
+	struct sock *sk = &dp->dccps_inet_connection.icsk_inet.sk;
+	u32 elapsed;
+	//struct dccp_sock *dp = dccp_sk(sk);
+	dccp_pr_debug("enter rcv timer sk %p", sk);
+	if (sk->sk_state == DCCP_CLOSED || sk->sk_state == DCCP_CLOSING) {
+		dccp_pr_debug ("socket %p already closed/closing\n", sk);
+		return;
+	}
+	if (sock_owned_by_user(sk))
+		dccp_pr_debug("sock owned by user");
+	
+	bh_lock_sock(sk);
+	if (sk->sk_state == DCCP_OPEN || sk->sk_state == DCCP_PARTOPEN) {
+		elapsed = dccp_keepalive_rcv_elapsed(dp);
+		if (elapsed > dccp_keepalive_rcv_when(dp))
+		{
+			dccp_pr_debug("no data received sk %p elapsed %u", sk, elapsed);
+			bh_unlock_sock(sk);
+			sock_put(sk);
+			dccp_close(sk, 0);
+			return;
+		}
+	}
+	sk_reset_timer(sk, &dccp_sk(sk)->dccps_rcv_timer, jiffies + dccp_keepalive_rcv_when(dp));
+	bh_unlock_sock(sk);
+	sock_put(sk);
+
+}
+#endif
+
 void dccp_init_xmit_timers(struct sock *sk)
 {
 	struct dccp_sock *dp = dccp_sk(sk);
 
 	tasklet_setup(&dp->dccps_xmitlet, dccp_write_xmitlet);
 	timer_setup(&dp->dccps_xmit_timer, dccp_write_xmit_timer, 0);
+#if IS_ENABLED(CONFIG_DCCP_KEEPALIVE)
+	//setup_timer(&dp->dccps_rcv_timer, dccp_rcv_timer,
+	//						     (unsigned long)sk);
+	timer_setup(&dp->dccps_rcv_timer, dccp_rcv_timer, 0);
+#endif
 	inet_csk_init_xmit_timers(sk, &dccp_write_timer, &dccp_delack_timer,
 				  &dccp_keepalive_timer);
 }
diff --git a/net/ipv6/addrconf.c b/net/ipv6/addrconf.c
index 9dfbda164e8c1..5d4668ac12be6 100644
--- a/net/ipv6/addrconf.c
+++ b/net/ipv6/addrconf.c
@@ -1002,6 +1002,7 @@ void inet6_ifa_finish_destroy(struct inet6_ifaddr *ifp)
 
 	kfree_rcu(ifp, rcu);
 }
+EXPORT_SYMBOL(inet6_ifa_finish_destroy);
 
 static void
 ipv6_link_dev_addr(struct inet6_dev *idev, struct inet6_ifaddr *ifp)
-- 
2.43.0

